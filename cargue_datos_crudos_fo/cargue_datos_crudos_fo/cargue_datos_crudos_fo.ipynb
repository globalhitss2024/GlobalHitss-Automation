{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n***************************************************************************************\\n* CLARO  HITSS - EMPRESAS Y NEGOCIOS                                                  *\\n* OBJETIVO: Extración de fuentes crudas de fibra optica                               * \\n*           y cargue a base de datos de forma automatica                              *\\n*           Comunicacion Celular S.A.- Comcel S.A\\\\Wilmer Camargo Ochoa - Data_PCC     *\\n* TABLA DE INGESTA POSTGRESQL: tb_datos_crudos_fibra_optica                           *\\n* FECHA CREACION: 27 de Mayo de 2024                                                  *\\n* ELABORADO POR: LAURA GAITAN                                                         *\\n* *************************************************************************************\\n* MODIFICACIONES\\n* NOMBRE                   FECHA      VERSION            DESCRIPCION\\n* \\n*\\n***************************************************************************************\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "***************************************************************************************\n",
    "* CLARO  HITSS - EMPRESAS Y NEGOCIOS                                                  *\n",
    "* OBJETIVO: Extración de fuentes crudas de fibra optica                               * \n",
    "*           y cargue a base de datos de forma automatica                              *\n",
    "*           Comunicacion Celular S.A.- Comcel S.A\\Wilmer Camargo Ochoa - Data_PCC     *\n",
    "* TABLA DE INGESTA POSTGRESQL: tb_datos_crudos_fibra_optica                           *\n",
    "* FECHA CREACION: 27 de Mayo de 2024                                                  *\n",
    "* ELABORADO POR: LAURA GAITAN                                                         *\n",
    "* *************************************************************************************\n",
    "* MODIFICACIONES\n",
    "* NOMBRE                   FECHA      VERSION            DESCRIPCION\n",
    "* \n",
    "*\n",
    "***************************************************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "from datetime import datetime\n",
    "import pyodbc\n",
    "import sys\n",
    "#sys.path.append('C:/Users/46122499/Documents/ambiente_desarrollo/dev-empresas-negocios-env/desarrollo_notebook')\n",
    "sys.path.append('C:/ambiente_desarrollo/dev-empresas-negocios-env/desarrollo_notebook')\n",
    "import parametros_desarrollo as par\n",
    "import uuid\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import os\n",
    "import psycopg2\n",
    "import logging\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#VARIABLES GLOBALES\n",
    "fecha_inicio = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "fecha_actual = datetime.today().date()\n",
    "duracion = []\n",
    "fuentes = []\n",
    "cantidad_registros = []\n",
    "destino = ['tb_datos_crudos_fibra_optica']\n",
    "estado = []\n",
    "funcion_error = []\n",
    "descripcion_error = []\n",
    "id_ejecucion_en_curso = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargueResumen(id_ejecucion, fecha_inicio_date,fuentes,cantidad_registros,destino,estado):\n",
    "    \"\"\"\n",
    "    Función que se encarga de cargar estadisticas de los datos que estan siendo procesados\n",
    "    \n",
    "    Argumentos:\n",
    "        id_ejecucion: Contiene un numero alfanumerico para creación de llaves primarias y foraneas de la base de datos\n",
    "        fecha_inicio_date: Fecha de inicio del procesamiento\n",
    "        fecha_fin_date: Fecha de fin del procesamiento\n",
    "        duracion_proceso: Duración del procesamiento \n",
    "        fuentes: Fuentes de donde provienen los datos\n",
    "        cantidad_registros: Cantidad de registros procesados\n",
    "        destino: Tabla donde se ingestan los datos\n",
    "        estado: Indica el estado del proceso de acuerdo a lo definido en la base de datos en la tabla control_procesamiento.estados_cargue \n",
    "        \n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        \n",
    "        df_resumen_cargue = pd.DataFrame({\n",
    "            'id_ejecucion': id_ejecucion,\n",
    "            'fecha_inicio_procesamiento': fecha_inicio_date,\n",
    "            'fuentes': fuentes,\n",
    "            'cantidad_registros': cantidad_registros,\n",
    "            'destino': [destino],\n",
    "            'id_estado': [estado],\n",
    "        })\n",
    "\n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'control_procesamiento'\n",
    "        nombre_tabla = 'tb_resumen_cargue'\n",
    "        \n",
    "        df_resumen_cargue.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueResumen.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertarErroresDB():\n",
    "    \"\"\"\n",
    "    Metodo para insertar a POSTGRESQL los errores capturados durante la ejecución\n",
    "    Argumentos Globales:\n",
    "        fecha_inicio: Captura la fecha en que inicio la ejecución\n",
    "        fecha_fin: Captura la fecha en que finalizo la ejecución\n",
    "        duracion: Duración del procesamiento\n",
    "        fuente: Indica la fuente de donde provienen los datos\n",
    "        cantidad_registros: Cantidad de registros por fuente\n",
    "        destino: Indica la tabla a donde se estan ingestando los datos\n",
    "        id_estado: Indica el estado del proceso definidos en la base de datos \n",
    "        funcion_error: Indica la función donde se esta presentando una falla\n",
    "        descripcion_error: Descripción del error generado\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convertir las cadenas de texto a objetos datetime\n",
    "        fecha_inicio_tr = datetime.strptime(fecha_inicio, \"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin_tr = datetime.strptime(fecha_fin, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        duracion_proceso_timedelta = fecha_fin_tr - fecha_inicio_tr\n",
    "        duracion_proceso_seconds = duracion_proceso_timedelta.total_seconds()\n",
    "        \n",
    "        errores = pd.DataFrame({\n",
    "            'fecha_inicio': fecha_inicio,\n",
    "            'fecha_fin': fecha_fin,\n",
    "            'duracion': duracion_proceso_seconds,\n",
    "            'fuente': fuentes,\n",
    "            'cantidad_registros': cantidad_registros,\n",
    "            'destino': destino,\n",
    "            'id_estado': estado,\n",
    "            'funcion_error': funcion_error,\n",
    "            'descripcion_error': descripcion_error\n",
    "        })\n",
    "        \n",
    "        conexion_errores = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'control_procesamiento'\n",
    "        nombre_tabla = 'tb_errores_cargue'\n",
    "        errores.to_sql(nombre_tabla, con=conexion_errores, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        cargueResumen(id_ejecucion_en_curso, fecha_inicio_tr,'REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP',0,par.destino_macrofo,2) \n",
    "        salidaLogMonitoreo()\n",
    "\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(insertarErroresDB.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def salidaLogMonitoreo():\n",
    "    \n",
    "    Este metodo captura la informacion que se desea imprimir en el Log\n",
    "    para monitoreo y funcionamiento del desarrollo\n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        None\n",
    "    \n",
    "    Fecha_fin = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    print(f\"Fecha_inicio: {fecha_inicio}\")\n",
    "    print(f\"Fecha_fin: {Fecha_fin}\")\n",
    "    print(f\"Duracion: {duracion}\")\n",
    "    print(f\"Fuentes: {fuentes}\")\n",
    "    print(f\"Cantidad_registros: {cantidad_registros}\")\n",
    "    print(f\"Destino: {destino}\")\n",
    "    print(f\"Estado: {estado}\")\n",
    "    print(\"Lugar errores: \", ' | '.join(map(str, funcion_error)))\n",
    "    print(\"Descripción errores: \", ' | '.join(map(str, descripcion_error)))\n",
    "    if estado[0] == 1 :\n",
    "        print(\"Ejecución exitosa\")\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "\"\"\"\n",
    "def salidaLogMonitoreo():\n",
    "    \"\"\"\n",
    "    Este método captura la información que se desea imprimir en el Log\n",
    "    para monitoreo y funcionamiento del desarrollo.\n",
    "    \"\"\"\n",
    "    Fecha_fin = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    logging.info(f\"Fecha_inicio: {fecha_inicio}\")\n",
    "    logging.info(f\"Fecha_fin: {Fecha_fin}\")\n",
    "    logging.info(f\"Duracion: {duracion}\")\n",
    "    logging.info(f\"Fuentes: {fuentes}\")\n",
    "    logging.info(f\"Cantidad_registros: {cantidad_registros}\")\n",
    "    logging.info(f\"Destino: {destino}\")\n",
    "    logging.info(f\"Estado: {estado}\")\n",
    "    logging.info(\"Lugar errores: \" + ' | '.join(map(str, funcion_error)))\n",
    "    logging.info(\"Descripción errores: \" + ' | '.join(map(str, descripcion_error)))\n",
    "    if estado[0] == 1:\n",
    "        logging.info(\"Ejecución exitosa\")\n",
    "    logging.info(\"------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conectarsqlServer():\n",
    "\n",
    "    \"\"\"\n",
    "    Función de conexion a sql server, \n",
    "\n",
    "    Argumentos:\n",
    "        id_ejecucion: Contiene un numero alfanumerico para creación de llaves primarias y foraneas de la base de datos\n",
    "        fecha_inicio_date: Fecha de inicio del procesamiento\n",
    "        fecha_fin_date: Fecha de fin del procesamiento\n",
    "        duracion_proceso: Duración del procesamiento \n",
    "        \n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Establecer la conexión utilizando pyodbc\n",
    "        cadena_conexion = f'DRIVER={par.driversql1};SERVER={par.hostsql};DATABASE={par.bdsql};UID={par.usuariosql};PWD={par.contrasenasql}'\n",
    "         # Establecer conexión\n",
    "        conn = pyodbc.connect(cadena_conexion)\n",
    "        \n",
    "        return conn\n",
    "    \n",
    "    except pyodbc.Error as e:\n",
    "            if 1 in estado:\n",
    "                estado.remove(1)\n",
    "            if 2 not in estado:\n",
    "                estado.append(2)\n",
    "            funcion_error.append(conectarsqlServer.__name__)\n",
    "            descripcion_error.append(str(e)[:100])\n",
    "            insertarErroresDB()\n",
    "            salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conectarsqlServerTemp():\n",
    "    bdsqltemp = 'db_col_dwh01_temp'\n",
    "    try:\n",
    "        # Establecer la conexión utilizando pyodbc\n",
    "        cadena_conexion = f'DRIVER={par.driversql1};SERVER={par.hostsql};DATABASE={bdsqltemp};UID={par.usuariosql};PWD={par.contrasenasql}'\n",
    "        conn = pyodbc.connect(cadena_conexion)\n",
    "        return conn\n",
    "    except pyodbc.Error as e:\n",
    "            if 1 in estado:\n",
    "                estado.remove(1)\n",
    "            if 2 not in estado:\n",
    "                estado.append(2)\n",
    "            funcion_error.append(conectarsqlServerTemp.__name__)\n",
    "            descripcion_error.append(str(e)[:100])\n",
    "            insertarErroresDB()\n",
    "            salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conexion_BD():\n",
    "    \"\"\"\n",
    "    Función que genera la conexión hacia la base de datos por medio de la libreria psycopg2\n",
    "    \n",
    "    Argumentos:\n",
    "        id_ejecucion: id del proceso ejecutado\n",
    "        fecha_fin_date: Fecha fin de procesamiento\n",
    "        duracion_proceso_seg: Duración en segundos del procesamiento\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        conn = psycopg2.connect(\n",
    "            host=par.host,\n",
    "            database=par.bd_inteligencia_comercial,\n",
    "            user=par.usuario,\n",
    "            password=par.contrasena\n",
    "        )\n",
    "        return conn\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(conexion_BD.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conexionDenodoOdbc():\n",
    "    \"\"\"\n",
    "    Función que genera la conexión hacia la base de datos por medio de la libreria psycopg2\n",
    "    \n",
    "    Argumentos:\n",
    "        id_ejecucion: id del proceso ejecutado\n",
    "        fecha_fin_date: Fecha fin de procesamiento\n",
    "        duracion_proceso_seg: Duración en segundos del procesamiento\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = pyodbc.connect('DSN=DenodoODBC;UID=46028569;PWD=Evento.15*')\n",
    "        return conn\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(conexion_BD.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutarConsultaOdbc():\n",
    "    \"\"\"\n",
    "    Método para ejecutar una consulta y devolver los resultados en un DataFrame.\n",
    "    \"\"\"\n",
    "    conn = conexionDenodoOdbc()\n",
    "    \n",
    "    try:\n",
    "        #cur = conn.cursor()\n",
    "        query = \"SELECT * FROM vw_tbl_inteligencia_com_ventas_pymes_total_report\"\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        rows = cursor.fetchall()\n",
    "        df = pd.DataFrame.from_records(rows, columns=[desc[0] for desc in cursor.description])\n",
    "        print (f'cantidad de registros descargados para macro fo1: {df.shape[0]}')\n",
    "        df_resultado=ordenColumnas(df,1)\n",
    "        return df_resultado\n",
    "    \n",
    "    except pyodbc.Error as e:\n",
    "        if 1 in estado:\n",
    "            estado.remove(1)\n",
    "        if 2 not in estado:\n",
    "            estado.append(2)\n",
    "        cantidad_registros.append(0)\n",
    "        funcion_error.append(ejecutar_consulta.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutarConsultaOdbc_1():\n",
    "    \"\"\"\n",
    "    Método para ejecutar una consulta y devolver los resultados en un DataFrame.\n",
    "    \"\"\"\n",
    "    conn = conexionDenodoOdbc()\n",
    "    \n",
    "    try:\n",
    "        #cur = conn.cursor()\n",
    "        query = \"SELECT * FROM vw_tbl_inteligencia_com_ventas_pymes_total_corp_report\"\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        rows = cursor.fetchall()\n",
    "        df = pd.DataFrame.from_records(rows, columns=[desc[0] for desc in cursor.description])\n",
    "        df.to_csv('data_fo2-2.csv',encoding='utf-8',index=False,mode='w')\n",
    "        #print (f'cantidad de registros descargados para macro fo2: {df.shape[0]}')\n",
    "        df_resultado=ordenColumnas(df,2)\n",
    "        return df_resultado\n",
    "    except pyodbc.Error as e:\n",
    "        if 1 in estado:\n",
    "            estado.remove(1)\n",
    "        if 2 not in estado:\n",
    "            estado.append(2)\n",
    "        cantidad_registros.append(0)\n",
    "        funcion_error.append(ejecutar_consulta.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordenColumnas(datos,num):\n",
    "    df=pd.DataFrame()\n",
    "    if num ==1:\n",
    "        df['ID']=datos['id']\n",
    "        df['ID_ORDEN_TRABAJO_PADRE']=datos['id_orden_trabajo_padre']\n",
    "        df['ESTADO_ORDEN_TRABAJO']=datos['estado_orden_trabajo']\n",
    "        df['ESTADO_VENTA']=datos['estado_venta']\n",
    "        df['FECHA_CREACION']=datos['fecha_creacion']\n",
    "        df['MES_CREACION']=datos['mes_creacion']\n",
    "        df['FECHA_INICIO_FACTURACION']=datos['fecha_inicio_facturacion']\n",
    "        df['FECHA_ESTADO']=datos['fecha_estado']\n",
    "        df['MES_ESTADO']=datos['mes_estado']\n",
    "        df['NIT']=datos['nit']\n",
    "        df['RAZON_SOCIAL']=datos['cliente']\n",
    "        df['ID_CLIENTE']=datos['id_cliente']\n",
    "        df['CONSULTOR_BASE']=datos['consultor_base']\n",
    "        df['USUARIO_GRUPO_CONSULTOR']=datos['usuario_grupo_consultor']\n",
    "        df['PRODUCTO']=datos['producto']\n",
    "        df['LINEA']=datos['linea']\n",
    "        df['TIPO_LINEA']=datos['tipo_linea']\n",
    "        df['VELOCIDAD']=datos['velocidad']\n",
    "        df['CIUDAD_INCIDENTE']=datos['ciudad_incidente']\n",
    "        df['PROCESO_TIPO_VENTA']=datos['proceso_tipo_venta']\n",
    "        df['NUM_SERVICIOS']=datos['num_servicios']\n",
    "        df['MONTO_MONEDA_LOCAL_ACTIVACION']=datos['monto_moneda_local_activacion']\n",
    "        df['MONTO_MONEDA_LOCAL_CARGO_MENSUAL']=datos['monto_moneda_local_cargo_mensual']\n",
    "        df['SOPORTE_PC']=datos['soporte_pc']\n",
    "        df['MONTO_MONEDA_LOCAL_ARRIENDO']=datos['monto_moneda_local_arriendo']\n",
    "        df['DURACION_CONTRATO']=datos['duracion_contrato']\n",
    "        df['TRM_CREACION']=datos['trm_creacion']\n",
    "        df['TRM_CAMBIO_ESTADO']=datos['trm_cambio_estado']\n",
    "        df['MONTO_MONEDA_LOCAL_OTROS']=datos['monto_moneda_local_otros'].fillna(0)\n",
    "        df['VARIACION_MONTO_MONEDA_LOCAL_MENSUAL']=datos['variacion_monto_moneda_local_mensual'].fillna(0)\n",
    "        df['VARIACION_MONTO_MONEDA_LOCAL_ARRIENDO']=datos['variacion_monto_moneda_local_arriendo'].fillna(0)\n",
    "        df['VARIACION_CARGO_ARRIENDO']=datos['variacion_cargo_arriendo'].fillna(0)\n",
    "        datos['variacion_total_moneda_local'] = pd.to_numeric(datos['variacion_total_moneda_local'], errors='coerce').fillna(0).astype(int)\n",
    "        df['VARIACION_TOTAL_MONEDA_LOCAL'] = datos['variacion_total_moneda_local']\n",
    "        datos['variacion_total'] = pd.to_numeric(datos['variacion_total'], errors='coerce').fillna(0).astype(int)\n",
    "        df['VARIACION_TOTAL'] = datos['variacion_total']\n",
    "        df['NRO_CONTRATO']=datos['nro_contrato']\n",
    "        df['ID_PROCESO_TIPO_VENTA']=datos['id_proceso_tipo_venta']\n",
    "        df['CIUDAD_DESTINO']=datos['ciudad_destino']\n",
    "        df['TIPO_VENTA']=datos['tipo_venta']\n",
    "        df['SEGMENTO']=datos['segmento']\n",
    "        df['SEGMENTO_MERCADO']=datos['segmento_mercado']\n",
    "        df['NODO']=datos['nodo']\n",
    "        df['DESCRIPCION']=datos['descripcion']\n",
    "        df['ID_ENLACE']=datos['id_enlace']\n",
    "        df['ID_TIPO']=datos['id_tipo']\n",
    "        df['TIPO_ORDEN_TRABAJO']=datos['tipo_orden_trabajo']\n",
    "        df['RESOLUCION1']=datos['resolucion1']\n",
    "        df['RESOLUCION2']=datos['resolucion2']\n",
    "        df['RESOLUCION3']=datos['resolucion3']\n",
    "        df['RESOLUCION4']=datos['resolucion4']\n",
    "        df['RESOLUCION_VENTA']=datos['resolucion_venta']\n",
    "        df['FAMILIA']=datos['familia']\n",
    "        df['CIUDAD_ORIGEN']=datos['ciudad_incidente']\n",
    "        print (f'cantidad de registros reorganizados: {df.shape[0]}')\n",
    "        \n",
    "    if num ==2:\n",
    "        df['ID']=datos['id']\n",
    "        df['ID_ORDEN_TRABAJO_PADRE']=datos['ot']\n",
    "        df['OTC_DIFERIDA']=datos['otc diferida']\n",
    "        df['ESTADO_ORDEN_TRABAJO']=datos['estado_orden_trabajo']\n",
    "        df['ESTADO_VENTA']=datos['estado_venta']\n",
    "        df['FECHA_CREACION']=datos['fecha_creacion']\n",
    "        df['MES_CREACION']=datos['mes_creacion']\n",
    "        df['FECHA_INICIO_FACTURACION']=datos['fecha_inicio_facturacion']\n",
    "        df['FECHA_ESTADO']=datos['fecha_estado']\n",
    "        df['MES_ESTADO']=datos['mes_estado']\n",
    "        df['NIT']=datos['nit']\n",
    "        df['RAZON_SOCIAL']=datos['razon social']\n",
    "        df['ID_CLIENTE']=datos['id_cliente']\n",
    "        df['CONSULTOR_BASE']=datos['consultor_base']\n",
    "        df['USUARIO_GRUPO_CONSULTOR']=datos['usuario_grupo_consultor']\n",
    "        df['PRODUCTO']=datos['producto (servicio)']\n",
    "        df['LINEA']=datos['linea']\n",
    "        df['TIPO_LINEA']=datos['tipo_linea']\n",
    "        df['VELOCIDAD']=datos['velocidad']\n",
    "        df['CIUDAD_INCIDENTE']=datos['ciudad_incidente']\n",
    "        df['PROCESO_TIPO_VENTA']=datos['proceso_tipo_venta (tipo venta)']\n",
    "        df['NUM_SERVICIOS']=datos['num_servicios']\n",
    "        df['MONTO_MONEDA_LOCAL_ACTIVACION']=datos['monto_moneda_local_activacion']\n",
    "        df['MONTO_MONEDA_LOCAL_CARGO_MENSUAL']=datos['monto_moneda_local_cargo_mensual']\n",
    "        df['SOPORTE_PC']=datos['soporte_pc']\n",
    "        df['MONTO_MONEDA_LOCAL_ARRIENDO']=datos['monto_moneda_local_arriendo']\n",
    "        df['DURACION_CONTRATO']=datos['duracion_contrato']\n",
    "        df['TRM_CREACION']=datos['trm_creacion']\n",
    "        df['TRM_CAMBIO_ESTADO']=datos['trm_cambio_estado']\n",
    "        df['MONTO_MONEDA_LOCAL_OTROS']=datos['monto_moneda_local_otros'].fillna(0)\n",
    "        df['VARIACION_MONTO_MONEDA_LOCAL_MENSUAL']=datos['variacion_monto_moneda_local_mensual'].fillna(0)\n",
    "        df['VARIACION_MONTO_MONEDA_LOCAL_ARRIENDO']=datos['variacion_monto_moneda_local_arriendo'].fillna(0)\n",
    "        df['VARIACION_CARGO_ARRIENDO']=datos['variacion_cargo_arriendo'].fillna(0)\n",
    "        df['VARIACION_TOTAL_MONEDA_LOCAL']=datos['variacion_total_moneda_local'].fillna(0)\n",
    "        df['VARIACION_TOTAL']=datos['variacion_total'].fillna(0)\n",
    "        df['NRO_CONTRATO']=datos['nro_contrato']\n",
    "        df['CIUDAD_DESTINO']=datos['ciudad_destino']\n",
    "        df['TIPO_VENTA']=datos['tipo_venta']\n",
    "        df['SEGMENTO']=datos['segmento']\n",
    "        df['SEGMENTO_MERCADO']=datos['segmento_mercado (division)']\n",
    "        df['NODO']=datos['nodo']\n",
    "        df['DESCRIPCION']=datos['descripcion (observaciones)']\n",
    "        df['ID_ENLACE']=datos['id_enlace']\n",
    "        df['ID_TIPO']=datos['id_tipo']\n",
    "        df['TIPO_ORDEN_TRABAJO']=datos['tipo_orden_trabajo']\n",
    "        df['RESOLUCION1']=datos['resolucion1']\n",
    "        df['RESOLUCION2']=datos['resolucion2']\n",
    "        df['RESOLUCION3']=datos['resolucion3']\n",
    "        df['RESOLUCION4']=datos['resolucion4']\n",
    "        df['RESOLUCION_VENTA']=datos['resolucion_venta']\n",
    "        df['FAMILIA']=datos['familia']\n",
    "        df['ID_HECHOS_ORDEN_TRABAJO_INSTALACION']='0'\n",
    "        df['CIUDAD_ORIGEN']=datos['ciudad_incidente']\n",
    "        df['GRUPO_OBJETIVO']=datos['grupo_objetivo']\n",
    "        df['COD_PROYECTO']='0'\n",
    "        print (f'cantidad de registros reorganizados: {df.shape[0]}')\n",
    "\n",
    " \n",
    "    return df\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_consulta():\n",
    "    \"\"\"\n",
    "    Método para ejecutar una consulta y devolver los resultados en un DataFrame.\n",
    "    \"\"\"\n",
    "    conn = conectarsqlServer()\n",
    "    \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT\n",
    "                FO1.ID, \n",
    "                FO1.ID_ORDEN_TRABAJO_PADRE,\n",
    "                FO2.OTC_DIFERIDA,\n",
    "                FO2.ESTADO_ORDEN_TRABAJO,\n",
    "                FO2.ESTADO_VENTA,\n",
    "                LEFT(CONVERT(VARCHAR, FO2.FECHA_CREACION, 120), 10) AS FECHA_CREACION,\n",
    "                FO2.MES_CREACION,\n",
    "                LEFT(CONVERT(VARCHAR, FO2.FECHA_INICIO_FACTURACION, 120), 10) AS FECHA_INICIO_FACTURACION,\n",
    "                LEFT(CONVERT(VARCHAR, FO2.FECHA_ESTADO, 120), 10) AS FECHA_ESTADO,\n",
    "                FO2.MES_ESTADO,\n",
    "                FO2.NIT,\n",
    "                FO2.CLIENTE AS RAZON_SOCIAL,\n",
    "                FO2.ID_CLIENTE,\n",
    "                FO2.CONSULTOR_BASE,\n",
    "                FO2.USUARIO_GRUPO_CONSULTOR,\n",
    "                FO2.PRODUCTO,\n",
    "                FO2.LINEA,\n",
    "                FO2.TIPO_LINEA,\n",
    "                FO2.VELOCIDAD,\n",
    "                FO2.CIUDAD_INCIDENTE,\n",
    "                FO2.PROCESO_TIPO_VENTA,\n",
    "                FO2.NUM_SERVICIOS,\n",
    "                FO2.MONTO_MONEDA_LOCAL_ACTIVACION,\n",
    "                FO2.MONTO_MONEDA_LOCAL_CARGO_MENSUAL,\n",
    "                FO2.SOPORTE_PC,\n",
    "                FO2.MONTO_MONEDA_LOCAL_ARRIENDO,\n",
    "                FO2.DURACION_CONTRATO,\n",
    "                FO2.TRM_CREACION,\n",
    "                FO2.TRM_CAMBIO_ESTADO,\n",
    "                FO2.MONTO_MONEDA_LOCAL_OTROS,\n",
    "                FO2.VARIACION_MONTO_MONEDA_LOCAL_MENSUAL,\n",
    "                FO2.VARIACION_MONTO_MONEDA_LOCAL_ARRIENDO,\n",
    "                FO2.VARIACION_CARGO_ARRIENDO,\n",
    "                FO2.VARIACION_TOTAL_MONEDA_LOCAL,\n",
    "                FO2.VARIACION_TOTAL,\n",
    "                FO2.NRO_CONTRATO,\n",
    "                FO1.ID_PROCESO_TIPO_VENTA,\n",
    "                FO2.CIUDAD_DESTINO,\n",
    "                FO2.TIPO_VENTA,\n",
    "                FO2.SEGMENTO,\n",
    "                FO2.SEGMENTO_MERCADO,\n",
    "                FO2.NODO,\n",
    "                FO2.DESCRIPCION,\n",
    "                FO2.ID_ENLACE,\n",
    "                FO2.ID_TIPO,\n",
    "                FO2.TIPO_ORDEN_TRABAJO,\n",
    "                FO2.RESOLUCION1,\n",
    "                FO2.RESOLUCION2,\n",
    "                FO2.RESOLUCION3,\n",
    "                FO2.RESOLUCION4,\n",
    "                FO2.RESOLUCION_VENTA,\n",
    "                FO2.FAMILIA,\n",
    "                FO2.CIUDAD_INCIDENTE AS CIUDAD_ORIGEN,\n",
    "                FO2.GRUPO_OBJETIVO,\n",
    "                FO2.COD_PROYECTO\n",
    "            FROM \n",
    "                REPORTE_VENTAS_PYMES_TOTAL FO1\n",
    "            INNER JOIN \n",
    "                REPORTE_VENTAS_PYMES_TOTAL_CORP FO2\n",
    "            ON \n",
    "                FO1.ID_ORDEN_TRABAJO_PADRE = FO2.ID_ORDEN_TRABAJO_PADRE;\n",
    "        \"\"\"\n",
    "        cur.execute(query)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [column[0] for column in cur.description]\n",
    "\n",
    "        df_sql = pd.DataFrame.from_records(rows, columns=columns)\n",
    "        return df_sql\n",
    "\n",
    "    except pyodbc.Error as e:\n",
    "        if 1 in estado:\n",
    "            estado.remove(1)\n",
    "        if 2 not in estado:\n",
    "            estado.append(2)\n",
    "        cantidad_registros.append(0)\n",
    "        funcion_error.append(ejecutar_consulta.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "        return None\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejecutar_consulta_tablafo1():\n",
    "    \"\"\"\n",
    "    Método para ejecutar una consulta y devolver los resultados en un DataFrame.\n",
    "    \"\"\"\n",
    "    conn = conectarsqlServer()\n",
    "    \n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        query = \"\"\"\n",
    "            SELECT\n",
    "                FO1.ID, \n",
    "                FO1.ID_ORDEN_TRABAJO_PADRE,\n",
    "                FO1.ESTADO_ORDEN_TRABAJO,\n",
    "                FO1.ESTADO_VENTA,\n",
    "                LEFT(CONVERT(VARCHAR, FO1.FECHA_CREACION, 120), 10) AS FECHA_CREACION,\n",
    "                FO1.MES_CREACION,\n",
    "                LEFT(CONVERT(VARCHAR, FO1.FECHA_INICIO_FACTURACION, 120), 10) AS FECHA_INICIO_FACTURACION,\n",
    "                LEFT(CONVERT(VARCHAR, FO1.FECHA_ESTADO, 120), 10) AS FECHA_ESTADO,\n",
    "                FO1.MES_ESTADO,\n",
    "                FO1.NIT,\n",
    "                FO1.CLIENTE AS RAZON_SOCIAL,\n",
    "                FO1.ID_CLIENTE,\n",
    "                FO1.CONSULTOR_BASE,\n",
    "                FO1.USUARIO_GRUPO_CONSULTOR,\n",
    "                FO1.PRODUCTO,\n",
    "                FO1.LINEA,\n",
    "                FO1.TIPO_LINEA,\n",
    "                FO1.VELOCIDAD,\n",
    "                FO1.CIUDAD_INCIDENTE,\n",
    "                FO1.PROCESO_TIPO_VENTA,\n",
    "                FO1.NUM_SERVICIOS,\n",
    "                FO1.MONTO_MONEDA_LOCAL_ACTIVACION,\n",
    "                FO1.MONTO_MONEDA_LOCAL_CARGO_MENSUAL,\n",
    "                FO1.SOPORTE_PC,\n",
    "                FO1.MONTO_MONEDA_LOCAL_ARRIENDO,\n",
    "                FO1.DURACION_CONTRATO,\n",
    "                FO1.TRM_CREACION,\n",
    "                FO1.TRM_CAMBIO_ESTADO,\n",
    "                FO1.MONTO_MONEDA_LOCAL_OTROS,\n",
    "                FO1.VARIACION_MONTO_MONEDA_LOCAL_MENSUAL,\n",
    "                FO1.VARIACION_MONTO_MONEDA_LOCAL_ARRIENDO,\n",
    "                FO1.VARIACION_CARGO_ARRIENDO,\n",
    "                FO1.VARIACION_TOTAL_MONEDA_LOCAL,\n",
    "                FO1.VARIACION_TOTAL,\n",
    "                FO1.NRO_CONTRATO,\n",
    "                FO1.ID_PROCESO_TIPO_VENTA,\n",
    "                FO1.CIUDAD_DESTINO,\n",
    "                FO1.TIPO_VENTA,\n",
    "                FO1.SEGMENTO,\n",
    "                FO1.SEGMENTO_MERCADO,\n",
    "                FO1.NODO,\n",
    "                FO1.DESCRIPCION,\n",
    "                FO1.ID_ENLACE,\n",
    "                FO1.ID_TIPO,\n",
    "                FO1.TIPO_ORDEN_TRABAJO,\n",
    "                FO1.RESOLUCION1,\n",
    "                FO1.RESOLUCION2,\n",
    "                FO1.RESOLUCION3,\n",
    "                FO1.RESOLUCION4,\n",
    "                FO1.RESOLUCION_VENTA,\n",
    "                FO1.FAMILIA,\n",
    "                FO1.CIUDAD_INCIDENTE AS CIUDAD_ORIGEN\n",
    "            FROM [db_col_dwh01].[dbo].[REPORTE_VENTAS_PYMES_TOTAL_CORP] FO1\n",
    "            WHERE ID_ORDEN_TRABAJO_PADRE IS NULL OR ID_ORDEN_TRABAJO_PADRE=0;\n",
    "        \"\"\"\n",
    "        cur.execute(query)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [column[0] for column in cur.description]\n",
    "\n",
    "        df_sql = pd.DataFrame.from_records(rows, columns=columns)\n",
    "        return df_sql\n",
    "\n",
    "    except pyodbc.Error as e:\n",
    "        if 1 in estado:\n",
    "            estado.remove(1)\n",
    "        if 2 not in estado:\n",
    "            estado.append(2)\n",
    "        cantidad_registros.append(0)\n",
    "        funcion_error.append(ejecutar_consulta_tablafo1.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "        return None\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraccionDatosOnyx():\n",
    "    \"\"\"\n",
    "    Este método carga los datos de Onyx para la extracción de Nombre y cédula.\n",
    "    Argumentos:\n",
    "        chFieldName 204\n",
    "        chFieldName 190\n",
    "        chFieldName 170 - Clase_producto\n",
    "        chFieldName 171 - Servicio_componente\n",
    "        chFieldName 147 - tipo_contrato\n",
    "        chFieldName 189 - direccion_comercial\n",
    "        chFieldName 198 - proyecto\n",
    "    Retorna: \n",
    "        DataFrame de Fibra Óptica.\n",
    "    Excepciones manejadas: \n",
    "        None\n",
    "    \"\"\"\n",
    "    conn_server = conectarsqlServerTemp()\n",
    "    if conn_server is None:\n",
    "        return None  # Manejar la falta de conexión adecuadamente\n",
    "\n",
    "    try:\n",
    "        import datetime\n",
    "        fecha_actual = datetime.datetime.now()\n",
    "        primer_dia_mes_actual = fecha_actual.replace(day=1)\n",
    "        fecha_formateada = primer_dia_mes_actual.strftime(\"%Y-%m-%d 00:00:00.000\")\n",
    "        \n",
    "        consulta_sql = f\"\"\" \n",
    "        SELECT \n",
    "            chFieldName,\n",
    "            iSystemId as id_orden_trabajo_padre,\n",
    "            vchDataValue as nombre_consultor,\n",
    "            dtUpdateDate\n",
    "            FROM [db_col_dwh01_temp].[dbo].[TEMP_EXPANSION_DATA]\n",
    "            WHERE (chFieldName LIKE '%190%')\n",
    "            --AND dtUpdateDate >= '{fecha_formateada}'\n",
    "            --AND DATALENGTH(vchDataValue) > 3;\n",
    "        \"\"\"\n",
    "\n",
    "        df_onyx = pd.read_sql(consulta_sql, conn_server)\n",
    "        \n",
    "\n",
    "        # Ejecutar la consulta y guardar los resultados en un DataFrame\n",
    "        if not df_onyx.empty:     \n",
    "            if 1 not in estado:\n",
    "                estado.append(1)\n",
    "        #print(f'cantidad de registros de Onix: {df_onyx.shape[0]}')\n",
    "        return df_onyx  # Aquí devolvemos el DataFrame\n",
    "\n",
    "    except Exception as e:\n",
    "        cantidad_registros.append(0)\n",
    "        if 1 in estado:\n",
    "            estado.remove(1)\n",
    "        if 2 not in estado:\n",
    "            estado.append(2)\n",
    "        funcion_error.append(extraccionDatosOnyx.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "\n",
    "    finally:\n",
    "        conn_server.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraccionDatosOnyxcolumnasextra():\n",
    "    \"\"\"\n",
    "    Este método carga los datos de Onyx para la extracción de Nombre y cédula.\n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        DataFrame de Fibra Óptica.\n",
    "    Excepciones manejadas: \n",
    "        None\n",
    "    \"\"\"\n",
    "    conn_server = conectarsqlServerTemp()\n",
    "    if conn_server is None:\n",
    "        return None  # Manejar la falta de conexión adecuadamente\n",
    "\n",
    "    try:\n",
    "        consulta_sql = \"\"\"\n",
    "        SELECT \n",
    "            a.[ITABLEID],\n",
    "            a.[CHFIELDNAME],\n",
    "            a.[ISYSTEMID] as id_orden_trabajo_padre,\n",
    "            a.[ISITEID] AS a_ISITEID,\n",
    "            b.[ISITEID] AS b_ISITEID,\n",
    "            a.[VCHDATAVALUE],\n",
    "            c.[VCHFIELDCAPTION] as nombre_columna,\n",
    "            b.[vchParameterDesc] as dato_columna,\n",
    "            a.[CHINSERTBY],\n",
    "            a.[DTINSERTDATE],\n",
    "            a.[CHUPDATEBY],\n",
    "            a.[DTUPDATEDATE]\n",
    "        FROM \n",
    "            [db_col_dwh01_temp].[dbo].[TEMP_EXPANSION_DATA] a\n",
    "        INNER JOIN \n",
    "            [db_col_dwh01_temp].[dbo].[TEMP_REFERENCE_PARAMETERS] b\n",
    "            ON CASE \n",
    "                WHEN ISNUMERIC(a.[VCHDATAVALUE]) = 1 AND \n",
    "                        a.[VCHDATAVALUE] NOT LIKE '%[^0-9]%' AND\n",
    "                        LEN(a.[VCHDATAVALUE]) <= 19 \n",
    "                THEN CAST(a.[VCHDATAVALUE] AS BIGINT)\n",
    "                ELSE NULL\n",
    "            END = b.[iParameterId]\n",
    "        INNER JOIN \n",
    "            [db_col_dwh01_temp].[dbo].[TEMP_REFERENCE_FIELDS] c\n",
    "            ON a.[CHFIELDNAME] = c.[CHFIELDNAME]\n",
    "        WHERE (a.[CHFIELDNAME] LIKE '%198%' OR a.[CHFIELDNAME] LIKE '%189%' OR a.[CHFIELDNAME] LIKE '%169%' OR a.[CHFIELDNAME] LIKE '%170%' OR a.[CHFIELDNAME] LIKE '%171%' \n",
    "            OR a.[CHFIELDNAME] LIKE '%147%')\n",
    "        ORDER BY \n",
    "            a.[DTUPDATEDATE] DESC;\n",
    "        \"\"\"\n",
    "\n",
    "        df_onyx = pd.read_sql(consulta_sql, conn_server)\n",
    "        \n",
    "        if df_onyx.empty:\n",
    "            return df_onyx  # Retornar el DataFrame vacío si no hay datos\n",
    "        \n",
    "        # Pivotar el DataFrame para obtener las columnas adicionales\n",
    "        df_pivoted = df_onyx.pivot_table(\n",
    "            index=['id_orden_trabajo_padre'], \n",
    "            columns='nombre_columna', \n",
    "            values='dato_columna', \n",
    "            aggfunc='first'\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Renombrar columnas para coincidir con la tabla destino\n",
    "        column_mapping = {\n",
    "            'Proyecto': 'proyecto',\n",
    "            'Dirección Comercial': 'direccion_comercial',\n",
    "            'Familia': 'familia_1',\n",
    "            'Clases Producto': 'clase_producto',\n",
    "            'Servicio/Componente': 'servicio_componente',\n",
    "            'Tipo de Contrato': 'tipo_contrato'\n",
    "        }\n",
    "        df_pivoted.rename(columns=column_mapping, inplace=True)\n",
    "        \n",
    "        # Añadir columnas adicionales que sean necesarias con valores nulos\n",
    "        for col in ['proyecto', 'direccion_comercial', 'familia_1', 'clase_producto', 'servicio_componente','tipo_contrato']:\n",
    "            if col not in df_pivoted.columns:\n",
    "                df_pivoted[col] = None\n",
    "        \n",
    "        return df_pivoted\n",
    "\n",
    "    except Exception as e:\n",
    "        cantidad_registros.append(0)\n",
    "        if 1 in estado:\n",
    "            estado.remove(1)\n",
    "        if 2 not in estado:\n",
    "            estado.append(2)\n",
    "        funcion_error.append(extraccionDatosOnyxcolumnasextra.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "\n",
    "    finally:\n",
    "        conn_server.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_id_orden_trabajo_padre():\n",
    "    try:\n",
    "        # Obtener los DataFrames de las funciones\n",
    "        df_onyx = extraccionDatosOnyx()\n",
    "        df_onyx_columnas = extraccionDatosOnyxcolumnasextra()\n",
    "        df_sql = ejecutarConsultaOdbc()\n",
    "        #df_sql = ejecutar_consulta()\n",
    "\n",
    "        \n",
    "        # Verificar si ambos DataFrames fueron obtenidos correctamente\n",
    "        if df_onyx is None or df_sql is None or df_onyx_columnas is None:\n",
    "            print(\"No se pudo obtener uno o ambos DataFrames de Onyx o la consulta SQL.\")\n",
    "            return None\n",
    "        \n",
    "        # Convertir los valores de id_orden_trabajo_padre a conjunto para la comparación\n",
    "        onyx_ids = set(df_onyx['id_orden_trabajo_padre'])\n",
    "        sql_ids = set(df_sql['ID_ORDEN_TRABAJO_PADRE'])\n",
    "        \n",
    "        # Encontrar los ids que están en ambos conjuntos (intersección)\n",
    "        ids_comunes = onyx_ids.intersection(sql_ids)\n",
    "\n",
    "\n",
    "        \n",
    "        # Crear DataFrame con los registros de df_onyx que tienen ids_comunes\n",
    "        df_ids_comunes = df_onyx[df_onyx['id_orden_trabajo_padre'].isin(ids_comunes)]\n",
    "\n",
    "        \n",
    "        # Integrar los datos adicionales de Onyx a df_ids_comunes usando mergees\n",
    "        df_ids_comunes = pd.merge(df_ids_comunes, df_onyx_columnas, on='id_orden_trabajo_padre', how='left')\n",
    "        \n",
    "        return df_ids_comunes\n",
    "    \n",
    "    except Exception as e:\n",
    "        cantidad_registros.append(0)\n",
    "        if 1 in estado:\n",
    "            estado.remove(1)\n",
    "        if 2 not in estado:\n",
    "            estado.append(2)\n",
    "        funcion_error.append(comparar_id_orden_trabajo_padre.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizarFechaFinProcesamiento(id_ejecucion, fecha_fin_date, duracion_proceso_seg):\n",
    "\n",
    "    \"\"\"\n",
    "    Función que actualiza la fecha fin de procesamiento y duración para el proceso que se ejecuto.\n",
    "    Utilizando cursores\n",
    "    \n",
    "    Argumentos:\n",
    "        id_ejecucion: id del proceso ejecutado\n",
    "        fecha_fin_date: Fecha fin de procesamiento\n",
    "        duracion_proceso_seg: Duración en segundos del procesamiento\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        conn = conexion_BD()\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        update_query = \"\"\"\n",
    "            UPDATE control_procesamiento.tb_resumen_cargue \n",
    "            SET fecha_fin_procesamiento = %s,\n",
    "            duracion_segundos = %s\n",
    "            WHERE id_ejecucion = %s\n",
    "        \"\"\"\n",
    "        cur.execute(update_query, (fecha_fin_date, duracion_proceso_seg, id_ejecucion))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(actualizarFechaFinProcesamiento.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uuid():\n",
    "    \"\"\"\n",
    "    Función que genera un numero alfanumerico para creación de llaves primarias y foraneas\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        return str(uuid.uuid4())\n",
    "    \n",
    "    except Exception as e:\n",
    "        fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(generate_uuid.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurarLogging():\n",
    "    \"\"\"\n",
    "    Configura el logging para escribir en un archivo y en la salida estándar\n",
    "    Utiliza la ruta definida en par.ruta_log para el directorio de logs.\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        None\n",
    "    \"\"\"\n",
    "    # Configuración del logging\n",
    "    log_directory = par.ruta_log  # Usa la ruta definida en config.py\n",
    "    log_file = os.path.join(log_directory, \"cargue_datos_crudos_fibra_optica.log\")\n",
    "\n",
    "    # Crear el directorio si no existe\n",
    "    if not os.path.exists(log_directory):\n",
    "        os.makedirs(log_directory)\n",
    "\n",
    "    # Configurar el logger\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file, mode='a'),  # 'a' para modo append\n",
    "            #logging.StreamHandler()  # Para imprimir en pantalla\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargueDatosBD(df_final):\n",
    "    \"\"\"\n",
    "    Función que se encarga de cargar los dataframes procesados hacia la base de datos\n",
    "    \n",
    "    Argumentos:\n",
    "        df_final: Contiene el dataframe que se requiere cargar a la BD\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'fuentes_cruda'\n",
    "        nombre_tabla = 'tb_datos_crudos_fibra_optica'\n",
    "        #print('cargando datos en base de datos principal')\n",
    "        #df_final.to_csv('data_base.csv',encoding='utf-8',index=False,mode='w')\n",
    "        df_final.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "       \n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "        cantidad_registros.append(0)\n",
    "        if 1 in estado:\n",
    "            estado.remove(1)\n",
    "        if 2 not in estado:\n",
    "            estado.append(2)\n",
    "        funcion_error.append(cargueDatosBD.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_estado_orden_trabajo():\n",
    "    \"\"\"\n",
    "    Consulta en la tabla de Postgress el id del estado orden trabajo\n",
    "    \n",
    "    Argumentos:\n",
    "        df: DataFrame que contiene la columna ESTADO_ORDEN_TRABAJO.\n",
    "    \n",
    "    Retorna: \n",
    "        df: df_estado de la columna\n",
    "    \"\"\"\n",
    "    conn = conexion_BD()\n",
    "    if conn:\n",
    "        try:\n",
    "            query = \"SELECT id_fo_estado, estado_orden_trabajo FROM fuentes_cruda.tb_fo_estado_orden_trabajo\"\n",
    "            df_estado = pd.read_sql(query, conn)\n",
    "            return df_estado\n",
    "        except SQLAlchemyError as e:\n",
    "            fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "            cantidad_registros.append(0)\n",
    "            estado.append(2)\n",
    "            funcion_error.append(obtener_estado_orden_trabajo.__name__)\n",
    "            descripcion_error.append(str(e)[:100])\n",
    "            insertarErroresDB()\n",
    "            salidaLogMonitoreo()\n",
    "        finally:\n",
    "            conn.close()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_estado_venta():\n",
    "    \"\"\"\n",
    "    Consulta en la tabla de Postgress el id del estado venta\n",
    "    \n",
    "    Argumentos:\n",
    "        df: DataFrame que contiene la columna ESTADO_VENTA.\n",
    "    \n",
    "    Retorna: \n",
    "        df: df_estado de la columna\n",
    "    \"\"\"\n",
    "    conn = conexion_BD()\n",
    "    if conn:\n",
    "        try:\n",
    "            query = \"SELECT id_fo_estado_venta, estado_venta, fecha_creacion, fecha_modificacion, id_estado_registro FROM fuentes_cruda.tb_fo_estado_venta\"\n",
    "            df_estado = pd.read_sql(query, conn)\n",
    "            return df_estado\n",
    "        except SQLAlchemyError as e:\n",
    "            fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "            cantidad_registros.append(0)\n",
    "            estado.append(2)\n",
    "            funcion_error.append(obtener_estado_venta.__name__)\n",
    "            descripcion_error.append(str(e)[:100])\n",
    "            insertarErroresDB()\n",
    "            salidaLogMonitoreo()\n",
    "        finally:\n",
    "            conn.close()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asignar_id_estados(df):\n",
    "    \"\"\"\n",
    "    Asigna el ID correspondiente de la tabla tb_fo_estado_orden_trabajo a cada estado en la columna ESTADO_ORDEN_TRABAJO.\n",
    "    \n",
    "    Argumentos:\n",
    "        df: DataFrame que contiene la columna ESTADO_ORDEN_TRABAJO.\n",
    "    \n",
    "    Retorna: \n",
    "        df: DataFrame con la columna id_estado_orden_trabajo actualizada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener el DataFrame de estados desde la tabla ESTADOS_ORDEN_TRABAJO\n",
    "        df_estados = obtener_estado_orden_trabajo()\n",
    "        \n",
    "        # Crear un diccionario de mapeo de estados (estado -> id_fo_estado)\n",
    "        estado_dict = dict(zip(df_estados['estado_orden_trabajo'], df_estados['id_fo_estado']))\n",
    "        \n",
    "        # Convertir todos los valores en la columna ESTADO_ORDEN_TRABAJO a mayúsculas\n",
    "        df['ESTADO_ORDEN_TRABAJO'] = df['ESTADO_ORDEN_TRABAJO'].str.upper()\n",
    "        \n",
    "        # Asignar el ID correspondiente a cada estado en la columna ESTADO_ORDEN_TRABAJO\n",
    "        df['id_estado_orden_trabajo'] = df['ESTADO_ORDEN_TRABAJO'].map(estado_dict)\n",
    "       \n",
    "        return df['id_estado_orden_trabajo']\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(asignar_id_estados.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asignar_id_estados_venta(df):\n",
    "    \"\"\"\n",
    "    Asigna el ID correspondiente de la tabla tb_fo_estado_venta a cada estado en la columna ESTADO_VENTA.\n",
    "    \n",
    "    Argumentos:\n",
    "        df: DataFrame que contiene la columna ESTADO_VENTA.\n",
    "    \n",
    "    Retorna: \n",
    "        df: DataFrame con la columna id_estado_venta actualizada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener el DataFrame de estados desde la tabla ESTADOS_VENTA\n",
    "        df_estados = obtener_estado_venta()\n",
    "        \n",
    "        # Crear un diccionario de mapeo de estados (estado_venta -> id_fo_estado_venta)\n",
    "        estado_dict = dict(zip(df_estados['estado_venta'], df_estados['id_fo_estado_venta']))\n",
    "        \n",
    "        # Convertir todos los valores en la columna ESTADOS_VENTA a mayúsculas\n",
    "        df['ESTADO_VENTA'] = df['ESTADO_VENTA'].str.upper()\n",
    "        \n",
    "        # Asignar el ID correspondiente a cada estado en la columna ESTADOS_VENTA\n",
    "        df['id_estado_venta'] = df['ESTADO_VENTA'].map(estado_dict)\n",
    "       \n",
    "        return df['id_estado_venta']\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(asignar_id_estados_venta.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seleccionCamposfibraoptica(df, df_onyx, df_resultado_fo1, fecha_inicio_date, id_ejecucion):\n",
    "    \"\"\"\n",
    "    Función que se encarga de seleccionar los datos correspondientes de la base de fibra óptica\n",
    "    y crea campos adicionales necesarios para el control de los datos.\n",
    "\n",
    "    *** se realiza el llenado del id nulo o vacio con el numero cero (0) debido a la restriccion de no nulo de la tabla de crudos ***\n",
    "    \n",
    "    Argumentos:\n",
    "        df: DataFrame importado previamente y que contiene los datos a procesar.\n",
    "        df_onyx: DataFrame con la información de Onyx.\n",
    "        df_resultado_fo1: DataFrame con la información adicional de FO1.\n",
    "        fecha_inicio_date: Fecha de inicio del procesamiento.\n",
    "        id_ejecucion: Contiene un número alfanumérico para creación de llaves primarias y foráneas de la base de datos.\n",
    "    \n",
    "    Retorna: \n",
    "        df_selected: DataFrame con los datos seleccionados y agregados.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_base = df.copy()\n",
    "        df_fo1 = df_resultado_fo1.copy()\n",
    "\n",
    "        # Combinación de los datos de df y df_fo1\n",
    "        df_base = pd.concat([df_base, df_fo1], ignore_index=True)\n",
    "\n",
    "        df_base['id_macrofo'] = [generate_uuid().upper() for _ in range(len(df_base))]\n",
    "        df_base['id_ejecucion'] = id_ejecucion\n",
    "        df_base['id_estado'] = 1\n",
    "        df_base['id'] = df_base['ID'].fillna(0)\n",
    "        df_base['id_orden_trabajo_padre'] = df_base['ID_ORDEN_TRABAJO_PADRE'] \n",
    "        df_base['otc_diferida'] = df_base['OTC_DIFERIDA'] \n",
    "        df_base['estado_orden_trabajo'] = df_base['ESTADO_ORDEN_TRABAJO'].str.upper() \n",
    "        df_base['estado_venta'] = df_base['ESTADO_VENTA'].str.upper() \n",
    "        df_base['fecha_creacion'] = df_base['FECHA_CREACION'] \n",
    "        df_base['mes_creacion'] = df_base['MES_CREACION'] \n",
    "        df_base['fecha_inicio_facturacion'] = df_base['FECHA_INICIO_FACTURACION'] \n",
    "        df_base['fecha_estado'] = df_base['FECHA_ESTADO'] \n",
    "        df_base['mes_estado'] = df_base['MES_ESTADO'] \n",
    "        df_base['nit'] = df_base['NIT'] \n",
    "        df_base['razon_social'] = df_base['RAZON_SOCIAL'] \n",
    "        df_base['id_cliente'] = df_base['ID_CLIENTE'] \n",
    "        df_base['consultor_base'] = df_base['CONSULTOR_BASE'] \n",
    "        df_base['usuario_grupo_consultor'] = df_base['USUARIO_GRUPO_CONSULTOR'] \n",
    "        df_base['producto'] = df_base['PRODUCTO'] \n",
    "        df_base['linea'] = df_base['LINEA'] \n",
    "        df_base['tipo_linea'] = df_base['TIPO_LINEA'] \n",
    "        df_base['velocidad'] = df_base['VELOCIDAD'] \n",
    "        df_base['ciudad_incidente'] = df_base['CIUDAD_INCIDENTE'].str.upper() \n",
    "        df_base['proceso_tipo_venta'] = df_base['PROCESO_TIPO_VENTA'] \n",
    "        df_base['num_servicios'] = df_base['NUM_SERVICIOS'] \n",
    "        df_base['monto_moneda_local_activacion'] = df_base['MONTO_MONEDA_LOCAL_ACTIVACION'].fillna(0)\n",
    "        df_base['monto_moneda_local_cargo_mensual'] = df_base['MONTO_MONEDA_LOCAL_CARGO_MENSUAL'].fillna(0)\n",
    "        df_base['soporte_pc'] = df_base['SOPORTE_PC'] \n",
    "        df_base['monto_moneda_local_arriendo'] = df_base['MONTO_MONEDA_LOCAL_ARRIENDO'] \n",
    "        df_base['duracion_contrato'] = df_base['DURACION_CONTRATO'] \n",
    "        df_base['trm_creacion'] = df_base['TRM_CREACION'] \n",
    "        df_base['trm_cambio_estado'] = df_base['TRM_CAMBIO_ESTADO'] \n",
    "        df_base['monto_moneda_local_otros'] = df_base['MONTO_MONEDA_LOCAL_OTROS'].fillna(0)\n",
    "        df_base['variacion_monto_moneda_local_mensual'] = df_base['VARIACION_MONTO_MONEDA_LOCAL_MENSUAL'].fillna(0)\n",
    "        df_base['variacion_monto_moneda_local_arriendo'] = df_base['VARIACION_MONTO_MONEDA_LOCAL_ARRIENDO'].fillna(0)\n",
    "        df_base['variacion_cargo_arriendo'] = df_base['VARIACION_CARGO_ARRIENDO'].fillna(0)\n",
    "        df_base['VARIACION_TOTAL_MONEDA_LOCAL'] = pd.to_numeric(df_base['VARIACION_TOTAL_MONEDA_LOCAL'], errors='coerce').fillna(0).astype(int)\n",
    "        df_base['variacion_total_moneda_local'] = df_base['VARIACION_TOTAL_MONEDA_LOCAL'].fillna(0)\n",
    "        df_base['VARIACION_TOTAL'] = pd.to_numeric(df_base['VARIACION_TOTAL'], errors='coerce').fillna(0).astype(int)\n",
    "        df_base['variacion_total'] = df_base['VARIACION_TOTAL']\n",
    "        df_base['nro_contrato'] = df_base['NRO_CONTRATO'] \n",
    "        df_base['id_proceso_tipo_venta'] = df_base['ID_PROCESO_TIPO_VENTA'].str.upper() \n",
    "        df_base['ciudad_destino'] = df_base['CIUDAD_DESTINO'].str.upper() \n",
    "        df_base['tipo_venta'] = df_base['TIPO_VENTA'].str.upper() \n",
    "        df_base['segmento'] = df_base['SEGMENTO'] \n",
    "        df_base['segmento_mercado'] = df_base['SEGMENTO_MERCADO'].str.upper() \n",
    "        df_base['nodo'] = df_base['NODO'] \n",
    "        df_base['descripcion'] = df_base['DESCRIPCION'].str.upper() \n",
    "        df_base['id_enlace'] = df_base['ID_ENLACE'] \n",
    "        df_base['id_tipo'] = df_base['ID_TIPO'].str.lstrip('$')\n",
    "        df_base['tipo_orden_trabajo'] = df_base['TIPO_ORDEN_TRABAJO'].str.lstrip('$')\n",
    "        df_base['resolucion1'] = df_base['RESOLUCION1'].str.upper() \n",
    "        df_base['resolucion2'] = df_base['RESOLUCION2'].str.upper() \n",
    "        df_base['resolucion3'] = df_base['RESOLUCION3'].str.upper() \n",
    "        df_base['resolucion4'] = df_base['RESOLUCION4'].str.upper() \n",
    "        df_base['resolucion_venta'] = df_base['RESOLUCION_VENTA'].str.upper() \n",
    "        df_base['familia'] = df_base['FAMILIA'].str.upper() \n",
    "        df_base['ciudad_origen'] = df_base['CIUDAD_ORIGEN'].str.upper() \n",
    "        df_base['grupo_objetivo'] = df_base['GRUPO_OBJETIVO'].str.upper()\n",
    "        df_base['cod_proyecto'] = df_base['COD_PROYECTO'].str.upper()\n",
    "        \n",
    "        # Comprobar si las columnas existen en df_onyx antes de realizar el merge\n",
    "        columnas_necesarias = ['id_orden_trabajo_padre', 'nombre_consultor', 'clase_producto', 'servicio_componente', 'direccion_comercial', 'proyecto', 'tipo_contrato', 'familia_1']\n",
    "        for col in columnas_necesarias:\n",
    "            if col not in df_onyx.columns:\n",
    "                df_onyx[col] = ''\n",
    "\n",
    "        # Convertir las columnas necesarias a mayúsculas\n",
    "        df_onyx[columnas_necesarias] = df_onyx[columnas_necesarias].apply(lambda x: x.str.upper() if x.dtype == \"object\" else x)\n",
    "\n",
    "        df_onyx['nombre_consultor'] = df_onyx['nombre_consultor'].replace('0', None)\n",
    "        df_base['cod_proyecto'] = df_base['cod_proyecto'].apply(lambda x: None if x in ['0', '.', 'NA', 'N/A', 'NO', 'n/a', '00', 'N','..','N.A','0.'] else x)\n",
    "\n",
    "        # Realizar el merge para agregar columnas de df_onyx\n",
    "        df_base = pd.merge(df_base, df_onyx[columnas_necesarias], on='id_orden_trabajo_padre', how='left')\n",
    "\n",
    "        # Obtener el mapeo de estados de orden de trabajo\n",
    "        df_base['id_estado_orden_trabajo'] = asignar_id_estados(df_base)\n",
    "        df_base['id_estado_venta'] = asignar_id_estados_venta(df_base)\n",
    "\n",
    "        # la columna 'id_estado_orden_trabajo' existe\n",
    "        if 'id_estado_orden_trabajo' not in df_base.columns:\n",
    "            df_base['id_estado_orden_trabajo'] = None\n",
    "\n",
    "        # Asignar el valor predeterminado 13 si 'id_estado_orden_trabajo' es nulo\n",
    "        df_base['id_estado_orden_trabajo'] = df_base['id_estado_orden_trabajo'].fillna(13)\n",
    "\n",
    "        df_base['id_estado_venta'] = df_base['id_estado_venta'].fillna(6)\n",
    "\n",
    "        # Selección de columnas\n",
    "        columnas_ordenadas = [\n",
    "            'id_macrofo', 'id_ejecucion', 'id', 'id_orden_trabajo_padre', 'otc_diferida', \n",
    "            'id_estado_orden_trabajo', 'id_estado_venta', 'fecha_creacion', 'mes_creacion', 'fecha_inicio_facturacion', 'fecha_estado', \n",
    "            'mes_estado', 'nit', 'razon_social', 'id_cliente', 'consultor_base', \n",
    "            'usuario_grupo_consultor', 'producto', 'linea', 'tipo_linea', 'velocidad', \n",
    "            'ciudad_incidente', 'proceso_tipo_venta', 'num_servicios', 'monto_moneda_local_activacion', \n",
    "            'monto_moneda_local_cargo_mensual', 'soporte_pc', 'monto_moneda_local_arriendo', \n",
    "            'duracion_contrato', 'trm_creacion', 'trm_cambio_estado', 'monto_moneda_local_otros', \n",
    "            'variacion_monto_moneda_local_mensual', 'variacion_monto_moneda_local_arriendo', \n",
    "            'variacion_cargo_arriendo', 'variacion_total_moneda_local', 'variacion_total', \n",
    "            'nro_contrato', 'id_proceso_tipo_venta', 'ciudad_destino', 'tipo_venta', \n",
    "            'segmento', 'segmento_mercado', 'nodo', 'descripcion', 'id_enlace', 'id_tipo', \n",
    "            'tipo_orden_trabajo', 'resolucion1', 'resolucion2', 'resolucion3', \n",
    "            'resolucion4', 'resolucion_venta', 'familia', 'ciudad_origen', 'grupo_objetivo', \n",
    "            'nombre_consultor', 'cod_proyecto', 'clase_producto', 'tipo_contrato', 'servicio_componente',\n",
    "            'direccion_comercial', 'proyecto', 'familia_1', 'id_estado'\n",
    "        ]\n",
    "\n",
    "        # Filtrar columnas existentes\n",
    "        columnas_seleccionadas = [col for col in columnas_ordenadas if col in df_base.columns]\n",
    "        df_selected = df_base[columnas_seleccionadas]\n",
    "        df_selected['fecha_inicio_procesamiento'] = pd.to_datetime(fecha_inicio_date)\n",
    "        \n",
    "        return df_selected\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(seleccionCamposfibraoptica.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "        raise e  # Re-raise the exception to handle it outside the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Configuración del logging, generación del UUID de ejecución y consulta a la base de datos\n",
    "        configurarLogging()\n",
    "        id_ejecucion = generate_uuid().upper()\n",
    "        fecha_inicio = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_inicio_date = datetime.strptime(fecha_inicio, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        df_resultado_denodo=ejecutarConsultaOdbc_1()\n",
    "        #df_resultado_denodo.to_csv('data_fo2.csv',encoding='utf-8',index=False,mode='w') todo mayuscula lore\n",
    "        df_resultado_fo1=ejecutarConsultaOdbc()\n",
    "        #df_resultado_fo1.to_csv('data_fo1.csv',encoding='utf-8',index=False,mode='w') todo mayuscula lore\n",
    "        df_onyx = comparar_id_orden_trabajo_padre()\n",
    "        \n",
    "        if df_resultado_denodo is not None:\n",
    "            registros = len(df_resultado_denodo)\n",
    "        \n",
    "            if registros > 0:\n",
    "                # Realiza la selección de campos y agrega información adicional\n",
    "                df_base = seleccionCamposfibraoptica(df_resultado_denodo, df_onyx, df_resultado_fo1, fecha_inicio_date, id_ejecucion)\n",
    "                if df_base is not None:\n",
    "                    #print('cargando de resumen de datos en base de datos principal')\n",
    "                    df_resumen = cargueResumen(id_ejecucion, fecha_inicio_date, 'REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP', registros, 'tb_datos_crudos_fibra_optica', 1)\n",
    "                    cargueDatosBD(df_base)\n",
    "                    \n",
    "                \n",
    "        fecha_fin = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin_date = datetime.strptime(fecha_fin, \"%Y-%m-%d %H:%M:%S\")\n",
    "        duracion_proceso = fecha_fin_date - fecha_inicio_date\n",
    "        duracion_proceso_seg = int(duracion_proceso.total_seconds())\n",
    "        actualizarFechaFinProcesamiento(id_ejecucion, fecha_fin_date, duracion_proceso_seg)\n",
    "        duracion.append(str(duracion_proceso))\n",
    "        estado.append(1)\n",
    "        salidaLogMonitoreo()\n",
    "    \n",
    "    except Exception as e:\n",
    "        fuentes.append('REPORTE_VENTAS_PYMES_TOTAL, REPORTE_VENTAS_PYMES_TOTAL_CORP')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(\"__main__\")\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
