{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl as opn\n",
    "import psycopg2\n",
    "from psycopg2 import sql, Error, OperationalError\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil  # Para mover el archivo descargado\n",
    "import logging\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import sys\n",
    "import ast\n",
    "sys.path.append('C:\\\\ambiente_desarrollo\\\\dev-empresas-negocios-env\\\\desarrollo_produccion')\n",
    "import parametros_produccion as par\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Ruta al archivo Excel\n",
    "ruta_excel = r'C:\\\\ambiente_desarrollo\\\\dev-empresas-negocios-env\\\\fuentes\\\\base_marca_blanca\\\\BASE ASIGNACION EMPRESAS.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES GLOBALES\n",
    "fecha_actual = datetime.today().date()\n",
    "duracion = []\n",
    "fuentes = []\n",
    "cantidad_registros = []\n",
    "estado = []\n",
    "fecha_fin_procesamiento =[]\n",
    "funcion_error = []\n",
    "descripcion_error = []\n",
    "id_ejecucion = str(uuid.uuid4())  # Generar UUID de ejecución\n",
    "destino = 'Marca Blanca'\n",
    "id_estado = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def salidaLogMonitoreo():\n",
    "    \n",
    "    Este metodo captura la informacion que se desea imprimir en el Log\n",
    "    para monitoreo y funcionamiento del desarrollo\n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        None\n",
    "    \n",
    "    Fecha_fin = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    print(f\"Fecha_inicio: {fecha_inicio}\")\n",
    "    print(f\"Fecha_fin: {Fecha_fin}\")\n",
    "    print(f\"Duracion: {duracion}\")\n",
    "    print(f\"Fuentes: {fuentes}\")\n",
    "    print(f\"Cantidad_registros: {cantidad_registros}\")\n",
    "    print(f\"Destino: {destino}\")\n",
    "    print(f\"Estado: {estado}\")\n",
    "    print(\"Lugar errores: \", ' | '.join(map(str, funcion_error)))\n",
    "    print(\"Descripción errores: \", ' | '.join(map(str, descripcion_error)))\n",
    "    if estado[0] == 1 :\n",
    "        print(\"Ejecución exitosa\")\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "\"\"\"\n",
    "def salidaLogMonitoreo():\n",
    "    \"\"\"\n",
    "    Este método captura la información que se desea imprimir en el Log\n",
    "    para monitoreo y funcionamiento del desarrollo.\n",
    "    \"\"\"\n",
    "    Fecha_fin = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    logging.info(f\"Fecha_inicio: {fecha_inicio}\")\n",
    "    logging.info(f\"Fecha_fin: {Fecha_fin}\")\n",
    "    logging.info(f\"Duracion: {duracion}\")\n",
    "    logging.info(f\"Fuentes: {fuentes}\")\n",
    "    logging.info(f\"Cantidad_registros: {cantidad_registros}\")\n",
    "    logging.info(f\"Destino: {destino}\")\n",
    "    logging.info(f\"Estado: {estado}\")\n",
    "    logging.info(\"Lugar errores: \" + ' | '.join(map(str, funcion_error)))\n",
    "    logging.info(\"Descripción errores: \" + ' | '.join(map(str, descripcion_error)))\n",
    "    if estado[0] == 1:\n",
    "        logging.info(\"Ejecución exitosa\")\n",
    "    logging.info(\"------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar resumen de datos en la BD\n",
    "def cargueResumen(id_ejecucion, fecha_inicio_date,fecha_fin_procesamiento, duracion,fuentes, cantidad_registros, destino, id_estado):\n",
    "    try:\n",
    "        df_resumen_cargue = pd.DataFrame({\n",
    "        'id_ejecucion': [id_ejecucion],  # Envolver en una lista\n",
    "        'fecha_inicio_procesamiento': [fecha_inicio_date],\n",
    "        'fecha_fin_procesamiento': [fecha_fin_procesamiento], \n",
    "        'duracion_segundos': [duracion],\n",
    "        'fuentes': [fuentes],\n",
    "        'cantidad_registros': [cantidad_registros],\n",
    "        'destino': [destino],\n",
    "        'id_estado': [id_estado],\n",
    "    })\n",
    "        Usuario_pro = 'postgres'\n",
    "        contraseña_pro = '1Nt3l163nC14_C0m3rc14L'\n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'control_procesamiento'\n",
    "        nombre_tabla = 'tb_resumen_cargue'\n",
    "        \n",
    "        df_resumen_cargue.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Marca Blanca')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueResumen.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargueDatosBD(df_final):\n",
    "    \"\"\"\n",
    "    Función que se encarga de cargar los dataframes procesados hacia la base de datos\n",
    "    \n",
    "    Argumentos:\n",
    "        df_final: Contiene el dataframe que se requiere cargar a la BD\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'fuentes_cruda'\n",
    "        nombre_tabla = 'tb_datos_crudos_marca_blanca'\n",
    "        \n",
    "        df_final.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Marca Blanca')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueDatosBD.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertarErroresDB():\n",
    "    \"\"\"\n",
    "    Metodo para insertar a POSTGRESQL los errores capturados durante la ejecución\n",
    "    Argumentos Globales:\n",
    "        fecha_inicio: Captura la fecha en que inicio la ejecución\n",
    "        fecha_fin: Captura la fecha en que finalizo la ejecución\n",
    "        duracion: Duración del procesamiento\n",
    "        fuente: Indica la fuente de donde provienen los datos\n",
    "        cantidad_registros: Cantidad de registros por fuente\n",
    "        destino: Indica la tabla a donde se estan ingestando los datos\n",
    "        id_estado: Indica el estado del proceso definidos en la base de datos \n",
    "        funcion_error: Indica la función donde se esta presentando una falla\n",
    "        descripcion_error: Descripción del error generado\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convertir las cadenas de texto a objetos datetime\n",
    "        fecha_inicio_tr = datetime.strptime(fecha_inicio, \"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin_tr = datetime.strptime(fecha_fin, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        duracion_proceso_timedelta = fecha_fin_tr - fecha_inicio_tr\n",
    "        duracion_proceso_seconds = duracion_proceso_timedelta.total_seconds()\n",
    "        \n",
    "        errores = pd.DataFrame({\n",
    "            'fecha_inicio': fecha_inicio,\n",
    "            'fecha_fin': fecha_fin,\n",
    "            'duracion': duracion_proceso_seconds,\n",
    "            'fuente': fuentes,\n",
    "            'cantidad_registros': cantidad_registros,\n",
    "            'destino': destino,\n",
    "            'id_estado': estado,\n",
    "            'funcion_error': funcion_error,\n",
    "            'descripcion_error': descripcion_error\n",
    "        })\n",
    "        \n",
    "        conexion_errores = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'control_procesamiento'\n",
    "        nombre_tabla = 'tb_errores_cargue'\n",
    "        errores.to_sql(nombre_tabla, con=conexion_errores, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        cargueResumen(id_ejecucion_en_curso, fecha_inicio_tr,2) \n",
    "        salidaLogMonitoreo()\n",
    "\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Marca Blanca')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(insertarErroresDB.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conexion_BD():\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            dbname=\"dwh_db\",    \n",
    "            user=\"45110947\",            \n",
    "            password=\"Mmilu28()*\",    \n",
    "            host=\"100.123.59.140\",          \n",
    "            port=\"5432\"                \n",
    "        )\n",
    "        print(\"Conexión a la base de datos Yellowbrick exitosa.\")\n",
    "        return connection\n",
    "    except Error as e:\n",
    "        print(f\"Error al conectar a la base de datos: {e}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultarHistoricoMb():\n",
    "    \"\"\"\n",
    "    Función que consulta los datos historicos existentes en la base de datos de la tabla de tb_datos_crudos_legalizadas\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        df_historico_mb : Retorna el historico de los datos cargados en la BD\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "\n",
    "        #engine = conexion_BD()\n",
    "        sql_consulta = \"Select * \\\n",
    "                    from fuentes_cruda.tb_datos_crudos_marca_blanca\"\n",
    "        df_historico_mb = pd.read_sql(sql_consulta, engine)\n",
    "    \n",
    "    \n",
    "        return df_historico_mb\n",
    "        \n",
    "    except Exception as e:\n",
    "        fuentes.append('Marca Blanca')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consultarHistoricoMb.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurarLogging():\n",
    "    \"\"\"\n",
    "    Configura el logging para escribir en un archivo y en la salida estándar\n",
    "    Utiliza la ruta definida en par.ruta_log para el directorio de logs.\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        None\n",
    "    \"\"\"\n",
    "    # Configuración del logging\n",
    "    log_directory = par.ruta_log_produccion  # Usa la ruta definida en config.py\n",
    "    log_file = os.path.join(log_directory, \"cargue_marcablanca.log\")\n",
    "\n",
    "    # Crear el directorio si no existe\n",
    "    if not os.path.exists(log_directory):\n",
    "        os.makedirs(log_directory)\n",
    "\n",
    "    # Configurar el logger\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file, mode='a'),  # 'a' para modo append\n",
    "            #logging.StreamHandler()  # Para imprimir en pantalla\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_mb(engine):\n",
    "    try:\n",
    "        # Consulta tabla TBL_CLIEN_SOLUCIONES_CORP\n",
    "        consulta = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM dwh_db.CLIENTES.TBL_CLIEN_SOLUCIONES_CORP\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor = engine.cursor()\n",
    "        cursor.execute(consulta)\n",
    "        resultados = cursor.fetchall()\n",
    "\n",
    "        # Obtener los nombres de las columnas\n",
    "        columnas = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        # Crea un DataFrame con los resultados y los nombres de columnas\n",
    "        df_resultados = pd.DataFrame(resultados, columns=columnas)\n",
    "\n",
    "        return df_resultados\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Marca Blanca')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consulta_mb.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar los datos\n",
    "def limpiar_data_mb(df):\n",
    "    try:\n",
    "        # Verificar si las columnas 'valor_paq' y 'valor_cfm' existen\n",
    "        if 'valor_paq' not in df.columns or 'valor_cfm' not in df.columns:\n",
    "            raise ValueError(\"Una o ambas columnas ('valor_paq', 'valor_cfm') no existen en el DataFrame.\")\n",
    "        \n",
    "        # Asegurarse de que las columnas 'valor_paq' y 'valor_cfm' estén en formato numérico\n",
    "        df['valor_paq'] = pd.to_numeric(df['valor_paq'], errors='coerce')\n",
    "        df['valor_cfm'] = pd.to_numeric(df['valor_cfm'], errors='coerce')\n",
    "        \n",
    "        # Elimina los decimales (mantiene solo la parte entera) y convierte a int\n",
    "        df['valor_paq'] = df['valor_paq'].fillna(0).astype(int) \n",
    "        df['valor_cfm'] = df['valor_cfm'].fillna(0).astype(int)  \n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Marca Blanca')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(limpiar_data_mb.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "\n",
    "# obtener los datos y manejo de la limpieza\n",
    "def ejecutar_proceso(engine):\n",
    "    try:\n",
    "        # Obteniene los resultados de la consulta\n",
    "        resultado = consulta_mb(engine)\n",
    "        \n",
    "        if resultado is None:\n",
    "            print(\"No se obtuvieron resultados de la consulta.\")\n",
    "            return\n",
    "        \n",
    "        # Aplicar la función de limpieza\n",
    "        resultado_limpio = limpiar_data_mb(resultado)\n",
    "        \n",
    "        if resultado_limpio is None:\n",
    "            print(\"No se pudo realizar la limpieza de datos.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Limpieza realizada.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        fuentes.append('Marca Blanca')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(ejecutar_proceso.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_asignacion_empresas(ruta_excel):\n",
    "    try:\n",
    "        # Leer el archivo Excel de asignación de empresas\n",
    "        df_asignacion_bajas = pd.read_excel(ruta_excel)\n",
    "        \n",
    "        # Verifica si la columna 'NIT' existe en el Excel\n",
    "        if 'NIT' not in df_asignacion_bajas.columns:\n",
    "            print(\"La columna 'NIT' no se encuentra en el archivo Excel.\")\n",
    "            return None\n",
    "        \n",
    "        return df_asignacion_bajas\n",
    "    \n",
    "    except Exception as e:\n",
    "        fuentes.append('Marca blanca')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargar_asignacion_empresas.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultarDatosParametrosDesarrollo():\n",
    "    try:\n",
    "\n",
    "  \n",
    "        engine = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "\n",
    "        #engine = conexion_BD()\n",
    "        sql_consulta = \"Select * \\\n",
    "                    from control_procesamiento.reglas_negocio\"\n",
    "        df_reglas = pd.read_sql(sql_consulta, engine)\n",
    "    \n",
    "    \n",
    "        return df_reglas\n",
    "    \n",
    "    except Exception as e:\n",
    "        fuentes.append('Marca blanca')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consultarDatosParametrosDesarrollo.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los datos de bajas con los filtros especificos\n",
    "def extraer_datos_bajas(dfa, df_ParametrosDesarrollo):\n",
    "    try:\n",
    "\n",
    "        #Creacion Variable de parametros para los filtros constantes\n",
    "        df_filtro_par = df_ParametrosDesarrollo[df_ParametrosDesarrollo['nombre'] == 'valor_paq_mb']\n",
    "        val_paq = int(df_filtro_par['valor'].values[0])\n",
    "        df_filtro_par2 = df_ParametrosDesarrollo[df_ParametrosDesarrollo['nombre'] == 'excluidos_mb']\n",
    "        excluidos_texto = df_filtro_par2['valor'].values[0]  \n",
    "        excluidos = ast.literal_eval(excluidos_texto)\n",
    "        \n",
    "        # Filtra por 'fecha_desact_paq' que NO esté vacía (excluir NaN)\n",
    "        df_filtrado_ba = df_filtrado = dfa[~dfa['fecha_desact_paq'].isna()]\n",
    "        \n",
    "        #df_filtro_bajas = df_filtrado_ba[df_filtrado_ba['fecha_desact_paq'].str.contains('2024-11', na=False)]\n",
    "        df_filtro_bajas = df_filtrado_ba[df_filtrado_ba['fecha_desact_paq'].dt.strftime('%Y-%m') == '2024-12']\n",
    "\n",
    "        # Filtra por 'anio_mes_act_paq' que no sea igual a '2024-11'\n",
    "        df_filtro_bajas = df_filtro_bajas[df_filtro_bajas['anio_mes_act_paq'] != '2024-12']\n",
    "        # Filtra por 'valor_paq' mayor que 0\n",
    "        df_filtro_bajas = df_filtro_bajas[df_filtro_bajas['valor_paq'] > val_paq]\n",
    "        \n",
    "\n",
    "        #excluidos = ['Claro Directo', 'CLARO DIRECTO', 'Servicio Datos', 'Servicio Telefonia GSM']\n",
    "        df_filtro_bajas = df_filtro_bajas[~df_filtro_bajas['desc_serv_instalado'].isin(excluidos)]\n",
    "        \n",
    "        # Ordenar el DataFrame por las columnas 'co_id', 'tel_num', 'cod_paq_instalado' de forma ascendente\n",
    "        df_filtro_bajas = df_filtro_bajas.sort_values(by=['co_id', 'tel_num', 'cod_paq_instalado'], ascending=True)\n",
    "        # Establecer la fecha base de Excel (1 de enero de 1900)\n",
    "        fecha_base_excel = pd.to_datetime('1900-01-01')\n",
    "        # Crear una nueva columna con el número de serie de Excel para la fecha\n",
    "        df_filtro_bajas['fecha_act_paq_excel'] = (df_filtro_bajas['fecha_act_paq'] - fecha_base_excel).dt.days + 2  # Sumamos 2 por el ajuste de Excel\n",
    "        df_filtro_bajas['fecha_desac_paq_excel'] = (df_filtro_bajas['fecha_desact_paq'] - fecha_base_excel).dt.days + 2  # Sumamos 2 por el ajuste de Excel\n",
    "        # Crear la columna de concatenación\n",
    "        df_filtro_bajas['concat_duplicados'] = df_filtro_bajas['co_id'].astype(str) + \\\n",
    "            df_filtro_bajas['tel_num'].astype(str) + \\\n",
    "            df_filtro_bajas['cod_paq_instalado'].apply(lambda x: str(int(x)) if pd.notna(x) else '') + \\\n",
    "            df_filtro_bajas['fecha_desac_paq_excel'].astype(str) + \\\n",
    "            df_filtro_bajas['valor_paq'].astype(str)\n",
    "        # Crear la columna de concatenación por servicio\n",
    "        df_filtro_bajas['concat_duplicados_por_servicio'] = df_filtro_bajas['co_id'].astype(str) + \\\n",
    "            df_filtro_bajas['fecha_desac_paq_excel'].astype(str) + \\\n",
    "            df_filtro_bajas['valor_paq'].astype(str) + \\\n",
    "            df_filtro_bajas['desc_serv_instalado'].astype(str)\n",
    "        \n",
    "        df_filtro_bajas['concat_com_bajas_altas'] = df_filtro_bajas['nit'].astype(str) + \\\n",
    "            df_filtro_bajas['tel_num'].astype(str) + \\\n",
    "            df_filtro_bajas['cod_paq_instalado'].apply(lambda x: str(int(x)) if pd.notna(x) else '') + \\\n",
    "            df_filtro_bajas['valor_paq'].astype(str)\n",
    "        # Verifica si el DataFrame filtrado tiene datos\n",
    "        # Identificar duplicados en 'concat_duplicados_por_servicio'\n",
    "        df_filtro_bajas['duplicado_por_servicio'] = df_filtro_bajas['concat_duplicados_por_servicio'].duplicated(keep=False)  # `keep=False` marca todos los duplicados\n",
    "        df_filtro_bajas['numero_duplicado'] = df_filtro_bajas.groupby('concat_duplicados_por_servicio').cumcount() + 1\n",
    "        # Filtrar solo aquellos donde el número de duplicado es 1 (es decir, el primero)\n",
    "        df_bajas = df_filtro_bajas[df_filtro_bajas['numero_duplicado'] == 1]\n",
    "        df_bajas['Segmento'] = 'NEGOCIOS'\n",
    "        df_bajas['cruce_bajas_altas'] = 'No cruza'\n",
    "        df_bajas['Tipo'] = 'BAJAS'\n",
    "\n",
    "        df_asignacion_bajas = cargar_asignacion_empresas(ruta_excel)\n",
    "        if df_asignacion_bajas is not None:\n",
    "            # Comparar el NIT entre df_altas y el archivo Excel, y asignar 'EMPRESAS' si coincide\n",
    "            df_bajas.loc[df_bajas['nit'].isin(df_asignacion_bajas['NIT']), 'Segmento'] = 'EMPRESAS'\n",
    "        df_bajas.loc[df_bajas['nit'].isin(df_asignacion_bajas['NIT']), 'Segmento'] = 'EMPRESAS'\n",
    "        \n",
    "         # Filtra por 'fecha_desact_paq' que esté vacía\n",
    "        df_filtrado_al_a = dfa[dfa['fecha_desact_paq'].isna()]\n",
    "        # Filtra por 'anio_mes_act_paq' distinto de '2024-11'\n",
    "        df_filtrado_altas_a = df_filtrado_al_a[df_filtrado_al_a['anio_mes_act_paq'] == '2024-12']\n",
    "        # Filtra por 'valor_paq' mayor que 0\n",
    "        df_filtrado_altas_a = df_filtrado_altas_a[df_filtrado_altas_a['valor_paq'] > 0]\n",
    "        # Ordenar el DataFrame por las columnas 'co_id', 'tel_num', 'cod_paq_instalado' de forma ascendente\n",
    "        df_filtrado_altas_a = df_filtrado_altas_a.sort_values(by=['co_id', 'tel_num', 'cod_paq_instalado'], ascending=True)\n",
    "        \n",
    "        df_filtrado_altas_a['concat_com_bajas_altas'] = df_filtrado_altas_a['nit'].astype(str) + \\\n",
    "              df_filtrado_altas_a['tel_num'].astype(str) + \\\n",
    "              df_filtrado_altas_a['cod_paq_instalado'].apply(lambda x: str(int(x)) if pd.notna(x) else '') + \\\n",
    "              df_filtrado_altas_a['valor_paq'].astype(str)\n",
    "        # Crear una columna 'Segmento' con valor por defecto 'NEGOCIOS'\n",
    "        df_bajas.loc[df_bajas['concat_com_bajas_altas'].isin(df_filtrado_altas_a['concat_com_bajas_altas']),'cruce_bajas_altas'] = 'Cruza'\n",
    "\n",
    "   \n",
    "        return df_bajas\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Marca Blanca')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(extraer_datos_bajas.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_Datos_Altas(df,df_ParametrosDesarrollo):\n",
    "    try:\n",
    "\n",
    "\n",
    "            #Creacion Variable de parametros para los filtros constantes\n",
    "        df_filtro_par = df_ParametrosDesarrollo[df_ParametrosDesarrollo['nombre'] == 'valor_paq_mb']\n",
    "        val_paq = int(df_filtro_par['valor'].values[0])\n",
    "        df_filtro_par2 = df_ParametrosDesarrollo[df_ParametrosDesarrollo['nombre'] == 'excluidos_mb']\n",
    "        excluidos_texto = df_filtro_par2['valor'].values[0]  # Obtiene el texto de la \"lista\"\n",
    "        excluidos = ast.literal_eval(excluidos_texto)\n",
    "\n",
    "        # Filtra por 'fecha_desact_paq' que esté vacía\n",
    "        df_filtrado_al = df[df['fecha_desact_paq'].isna()]\n",
    "        # Filtra por 'anio_mes_act_paq' distinto de '2024-11'\n",
    "        df_filtrado_altas = df_filtrado_al[df_filtrado_al['anio_mes_act_paq'] == '2024-12']\n",
    "        # Filtra por 'valor_paq' mayor que 0\n",
    "        df_filtrado_altas = df_filtrado_altas[df_filtrado_altas['valor_paq'] > val_paq]\n",
    "        # Elimina filas donde 'desc_serv_instalado' tenga ciertos valores específicos\n",
    "        #excluidos = ['Claro Directo', 'CLARO DIRECTO', 'Servicio Datos', 'Servicio Telefonia GSM']\n",
    "        df_filtrado_altas = df_filtrado_altas[~df_filtrado_altas['desc_serv_instalado'].isin(excluidos)]\n",
    "        # Ordenar el DataFrame por las columnas 'co_id', 'tel_num', 'cod_paq_instalado' de forma ascendente\n",
    "        df_filtrado_altas = df_filtrado_altas.sort_values(by=['co_id', 'tel_num', 'cod_paq_instalado'], ascending=True)\n",
    "        # Establecer la fecha base de Excel (1 de enero de 1900)\n",
    "        fecha_base_excel = pd.to_datetime('1900-01-01')\n",
    "        # Crear una nueva columna con el número de serie de Excel para la fecha\n",
    "        df_filtrado_altas['fecha_act_paq_excel'] = (df_filtrado_altas['fecha_act_paq'] - fecha_base_excel).dt.days + 2  # Sumamos 2 por el ajuste de Excel\n",
    "        # Crear la columna de concatenación\n",
    "        df_filtrado_altas['concat_duplicados'] = df_filtrado_altas['co_id'].astype(str) + \\\n",
    "            df_filtrado_altas['tel_num'].astype(str) + \\\n",
    "            df_filtrado_altas['cod_paq_instalado'].apply(lambda x: str(int(x)) if pd.notna(x) else '') + \\\n",
    "            df_filtrado_altas['fecha_act_paq_excel'].astype(str) + \\\n",
    "            df_filtrado_altas['valor_paq'].astype(str)\n",
    "        # Crear la columna de concatenación por servicio\n",
    "        df_filtrado_altas['concat_duplicados_por_servicio'] = df_filtrado_altas['co_id'].astype(str) + \\\n",
    "            df_filtrado_altas['fecha_act_paq_excel'].astype(str) + \\\n",
    "            df_filtrado_altas['valor_paq'].astype(str) + \\\n",
    "            df_filtrado_altas['desc_serv_instalado'].astype(str)\n",
    "        # Crea columna para identificar cruce contra bajas\n",
    "        df_filtrado_altas['concat_com_bajas_altas'] = df_filtrado_altas['nit'].astype(str) + \\\n",
    "        df_filtrado_altas['tel_num'].astype(str) + \\\n",
    "        df_filtrado_altas['cod_paq_instalado'].apply(lambda x: str(int(x)) if pd.notna(x) else '') + \\\n",
    "        df_filtrado_altas['valor_paq'].astype(str)\n",
    "        # Identificar duplicados en 'concat_duplicados_por_servicio'\n",
    "        df_filtrado_altas['duplicado_por_servicio'] = df_filtrado_altas['concat_duplicados_por_servicio'].duplicated(keep=False)  # `keep=False` marca todos los duplicados\n",
    "        # Asignar un número de secuencia a cada grupo de duplicados basado en la columna 'concat_duplicados_por_servicio'\n",
    "        df_filtrado_altas['numero_duplicado'] = df_filtrado_altas.groupby('concat_duplicados_por_servicio').cumcount() + 1\n",
    "        # Filtrar solo aquellos donde el número de duplicado es 1 (es decir, el primero)\n",
    "        df_altas = df_filtrado_altas[df_filtrado_altas['numero_duplicado'] == 1]\n",
    "        # Crear una columna 'Segmento' con valor por defecto 'NEGOCIOS'\n",
    "        df_altas['Segmento'] = 'NEGOCIOS'\n",
    "        df_altas['cruce_bajas_altas'] = 'No cruza'\n",
    "        df_altas['Tipo'] = 'ALTAS'\n",
    "        #Leer el archivo Excel de asignación de empresas\n",
    "           #Leer el archivo Excel de asignación de empresas\n",
    "        df_asignacion_bajas = cargar_asignacion_empresas(ruta_excel)\n",
    "        if df_asignacion_bajas is not None:\n",
    "            # Comparar el NIT entre df_altas y el archivo Excel, y asignar 'EMPRESAS' si coincide\n",
    "            df_altas.loc[df_altas['nit'].isin(df_asignacion_bajas['NIT']), 'Segmento'] = 'EMPRESAS'\n",
    "        df_altas.loc[df_altas['nit'].isin(df_asignacion_bajas['NIT']), 'Segmento'] = 'EMPRESAS'\n",
    "\n",
    "\n",
    "            # Filtrado para las bajas\n",
    "        df_filtrado_ba_a = df[~df['fecha_desact_paq'].isna()]  # Filtramos solo las filas con fecha_desact_paq no nula\n",
    "        # Aseguramos que 'fecha_desact_paq' sea string antes de aplicar .str.contains()\n",
    "        df_filtrado_ba_a['fecha_desact_paq'] = df_filtrado_ba_a['fecha_desact_paq'].fillna('').astype(str)\n",
    "        # Filtra por 'anio_mes_act_paq' distinto de '2024-11' (no es necesario modificar esta parte)\n",
    "        df_filtrado_bajas_a = df_filtrado_ba_a[df_filtrado_ba_a['fecha_desact_paq'].str.contains('2024-12', na=False)]\n",
    "        # Filtra por 'anio_mes_act_paq' que no sea igual a '2024-11'\n",
    "        df_filtrado_bajas_a = df_filtrado_bajas_a[df_filtrado_bajas_a['anio_mes_act_paq'] != '2024-12']\n",
    "        # Filtra por 'valor_paq' mayor que 0\n",
    "        df_filtrado_bajas_a = df_filtrado_bajas_a[df_filtrado_bajas_a['valor_paq'] > 0]\n",
    "        # Identificar duplicados en 'concat_duplicados_por_servicio'\n",
    "        df_filtrado_bajas_a['concat_com_bajas_altas'] = df_filtrado_bajas_a['nit'].astype(str) + \\\n",
    "        df_filtrado_bajas_a['tel_num'].astype(str) + \\\n",
    "        df_filtrado_bajas_a['cod_paq_instalado'].apply(lambda x: str(int(x)) if pd.notna(x) else '') + \\\n",
    "        df_filtrado_bajas_a['valor_paq'].astype(str)\n",
    "        # Ahora, el cruce con el DataFrame de Altas\n",
    "        df_altas.loc[df_altas['concat_com_bajas_altas'].isin(df_filtrado_bajas_a['concat_com_bajas_altas']),'cruce_bajas_altas'] = 'Cruza'\n",
    "        \n",
    "                \n",
    "\n",
    "        \n",
    "        return df_altas \n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Marca Blanca')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(extraer_Datos_Altas.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión a la base de datos Yellowbrick exitosa.\n",
      "Limpieza realizada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMD_INTCOM\\AppData\\Local\\Temp\\ipykernel_21500\\3688477697.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bajas['Segmento'] = 'NEGOCIOS'\n",
      "C:\\Users\\AMD_INTCOM\\AppData\\Local\\Temp\\ipykernel_21500\\3688477697.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bajas['cruce_bajas_altas'] = 'No cruza'\n",
      "C:\\Users\\AMD_INTCOM\\AppData\\Local\\Temp\\ipykernel_21500\\3688477697.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_bajas['Tipo'] = 'BAJAS'\n",
      "C:\\Users\\AMD_INTCOM\\AppData\\Local\\Temp\\ipykernel_21500\\1501425238.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_altas['Segmento'] = 'NEGOCIOS'\n",
      "C:\\Users\\AMD_INTCOM\\AppData\\Local\\Temp\\ipykernel_21500\\1501425238.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_altas['cruce_bajas_altas'] = 'No cruza'\n",
      "C:\\Users\\AMD_INTCOM\\AppData\\Local\\Temp\\ipykernel_21500\\1501425238.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_altas['Tipo'] = 'ALTAS'\n",
      "C:\\Users\\AMD_INTCOM\\AppData\\Local\\Temp\\ipykernel_21500\\1501425238.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtrado_ba_a['fecha_desact_paq'] = df_filtrado_ba_a['fecha_desact_paq'].fillna('').astype(str)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        configurarLogging()\n",
    "        #Variables constantes dentro del codigo para funciones\n",
    "        \n",
    "\n",
    "        id_ejecucion = str(uuid.uuid4()).upper()  # Generar ID de ejecución\n",
    "        fecha_inicio = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_inicio_tr = datetime.strptime(fecha_inicio, \"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin_tr = datetime.strptime(fecha_fin, \"%Y-%m-%d %H:%M:%S\")\n",
    "        id_estado = 1\n",
    "        estado = 1  # O el valor adecuado para el estado\n",
    "        duracion_proceso_timedelta = fecha_fin_tr - fecha_inicio_tr\n",
    "        duracion_proceso_seconds = duracion_proceso_timedelta.total_seconds()\n",
    "        \n",
    "        df_ParametrosDesarrollo = consultarDatosParametrosDesarrollo()   \n",
    "        engine = conexion_BD()\n",
    "        resultado = consulta_mb(engine)\n",
    "        ejecutar_proceso(engine)\n",
    "        resultado_limpio = limpiar_data_mb(resultado) \n",
    "\n",
    "        mb_bajas = extraer_datos_bajas(resultado_limpio,df_ParametrosDesarrollo)\n",
    "        mb_Altas = extraer_Datos_Altas(resultado_limpio,df_ParametrosDesarrollo)\n",
    "        mb_historico = consultarHistoricoMb()\n",
    "\n",
    "\n",
    "        # Concatenar los DataFrames\n",
    "        df_mb = pd.concat([mb_bajas, mb_Altas], axis=0, ignore_index=True)\n",
    "\n",
    "        # Filtrar por 'cruce_bajas_altas' == 'No cruza'\n",
    "        df_mb = df_mb[df_mb['cruce_bajas_altas'] == 'No cruza']\n",
    "        df_mb['id_ejecucion'] = id_ejecucion  # Agregar la columna id_ejecucion con el mismo UUID en todas las fila\n",
    "        df_mb['id'] = [str(uuid.uuid4()) for _ in range(len(df_mb))]  # Agregar la columna id con un UUID único por cada fila\n",
    "        df_mb['fecha_procesamiento'] = fecha_inicio  # Agregar la columna fecha_procesamiento con la fecha y hora actual\n",
    "        df_mb['id_estado'] = 1  # Crear el estado del registro como entero\n",
    "        df_mb['id_estado_registro'] = 1\n",
    "        df_mb.columns = [col.lower() for col in df_mb.columns]  # Convertir los nombres de las columnas a minúsculas\n",
    "        df_mb.drop(columns=['duplicado_por_servicio', 'cruce_bajas_altas', 'numero_duplicado'], inplace=True)\n",
    "        df_mb['mes_mb'] = df_mb.apply(\n",
    "        lambda row: row['anio_mes_act_paq'] if row['tipo'] == 'ALTAS' \n",
    "        else (str(row['fecha_desact_paq'])[:7] if pd.notnull(row['fecha_desact_paq']) else None) \n",
    "        if row['tipo'] == 'BAJAS' \n",
    "        else None, axis=1)\n",
    "\n",
    "        df_marca_blanca_a = pd.merge(df_mb, mb_historico[['concat_duplicados_por_servicio', 'mes_mb']], \n",
    "                    on=['concat_duplicados_por_servicio', 'mes_mb'], \n",
    "                    how='left', \n",
    "                    indicator=True)\n",
    "        \n",
    "        df_marca_blanca = df_marca_blanca_a[df_marca_blanca_a['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        \n",
    "        registros = len(df_marca_blanca)\n",
    "        cantidad_registros.append(registros)\n",
    "\n",
    "        # Ejecucion cargue de datos ETL, se carga la funcion de CargueDatosBD, insercion a BD\n",
    "        if registros > 0:\n",
    "           df_resumen = cargueResumen(\n",
    "        id_ejecucion, fecha_inicio_tr, fecha_fin_tr, duracion_proceso_seconds,\n",
    "        'Marca Blanca', registros, 'tb_datos_crudos_marca_blanca', id_estado\n",
    "        )\n",
    "        cargueDatosBD(df_marca_blanca)\n",
    "  \n",
    "    except Exception as e:\n",
    "        fuentes.append('Marca Blanca')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(\"__main__\")\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_marca_blanca.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
