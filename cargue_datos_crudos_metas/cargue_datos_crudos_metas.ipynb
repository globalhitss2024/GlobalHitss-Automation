{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\INTCOM\\AppData\\Local\\Temp\\2\\ipykernel_14464\\1565411016.py:1: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n***************************************************************************************\\n* CLARO  HITSS - EMPRESAS Y NEGOCIOS                                                  *\\n* OBJETIVO: Extración de fuentes crudas de Metas                                      * \\n*           y cargue a base de datos de forma automatica                              *\\n*           Comunicacion Celular S.A.- Comcel S.A\\\\Wilmer Camargo Ochoa - Data_PCC     *\\n* TABLA DE INGESTA POSTGRESQL: tb_datos_crudos_metas                                  *\\n* FECHA CREACION: 15 de Julio de 2024                                                 *\\n* ELABORADO POR: JEFFERSON ROZO                                                       *\\n* *************************************************************************************\\n* MODIFICACIONES\\n* NOMBRE                   FECHA      VERSION            DESCRIPCION\\n* \\n*\\n***************************************************************************************\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "***************************************************************************************\n",
    "* CLARO  HITSS - EMPRESAS Y NEGOCIOS                                                  *\n",
    "* OBJETIVO: Extración de fuentes crudas de Metas                                      * \n",
    "*           y cargue a base de datos de forma automatica                              *\n",
    "*           Comunicacion Celular S.A.- Comcel S.A\\Wilmer Camargo Ochoa - Data_PCC     *\n",
    "* TABLA DE INGESTA POSTGRESQL: tb_datos_crudos_metas                                  *\n",
    "* FECHA CREACION: 15 de Julio de 2024                                                 *\n",
    "* ELABORADO POR: JEFFERSON ROZO                                                       *\n",
    "* *************************************************************************************\n",
    "* MODIFICACIONES\n",
    "* NOMBRE                   FECHA      VERSION            DESCRIPCION\n",
    "* \n",
    "*\n",
    "***************************************************************************************\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.append('C:/ambiente_desarrollo/dev-empresas-negocios-env/desarrollo_produccion')\n",
    "import parametros_produccion as par\n",
    "import uuid\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import os\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import logging\n",
    "import openpyxl\n",
    "import sys \n",
    "from openpyxl import load_workbook\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES GLOBALES\n",
    "fecha_inicio = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "fecha_actual = datetime.today().date()\n",
    "duracion = []\n",
    "fuentes = []\n",
    "cantidad_registros = []\n",
    "destino = [par.destino_metas]\n",
    "estado = []\n",
    "funcion_error = []\n",
    "descripcion_error = []\n",
    "id_ejecucion_en_curso = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef salidaLogMonitoreo():\\n    Este método captura la información que se desea imprimir en el Log\\n    para monitoreo y funcionamiento del desarrollo.\\n    Fecha_fin = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\\n    logging.info(f\"Fecha_inicio: {fecha_inicio}\")\\n    logging.info(f\"Fecha_fin: {Fecha_fin}\")\\n    logging.info(f\"Duracion: {duracion}\")\\n    logging.info(f\"Fuentes: {fuentes}\")\\n    logging.info(f\"Cantidad_registros: {cantidad_registros}\")\\n    logging.info(f\"Destino: {destino}\")\\n    logging.info(f\"Estado: {estado}\")\\n    logging.info(\"Lugar errores: \" + \\' | \\'.join(map(str, funcion_error)))\\n    logging.info(\"Descripción errores: \" + \\' | \\'.join(map(str, descripcion_error)))\\n    if estado[0] == 1:\\n        logging.info(\"Ejecución exitosa\")\\n    logging.info(\"------------------------------------------------------------------\")\\n '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def salidaLogMonitoreo():\n",
    "    \"\"\"\n",
    "    Este metodo captura la informacion que se desea imprimir en el Log\n",
    "    para monitoreo y funcionamiento del desarrollo\n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        None\n",
    "    \"\"\"\n",
    "    Fecha_fin = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    print(f\"Fecha_inicio: {fecha_inicio}\")\n",
    "    print(f\"Fecha_fin: {Fecha_fin}\")\n",
    "    print(f\"Duracion: {duracion}\")\n",
    "    print(f\"Fuentes: {fuentes}\")\n",
    "    print(f\"Cantidad_registros: {cantidad_registros}\")\n",
    "    print(f\"Destino: {destino}\")\n",
    "    print(f\"Estado: {estado}\")\n",
    "    print(\"Lugar errores: \", ' | '.join(map(str, funcion_error)))\n",
    "    print(\"Descripción errores: \", ' | '.join(map(str, descripcion_error)))\n",
    "    if estado[0] == 1 :\n",
    "        print(\"Ejecución exitosa\")\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "\"\"\"\n",
    "def salidaLogMonitoreo():\n",
    "    Este método captura la información que se desea imprimir en el Log\n",
    "    para monitoreo y funcionamiento del desarrollo.\n",
    "    Fecha_fin = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    logging.info(f\"Fecha_inicio: {fecha_inicio}\")\n",
    "    logging.info(f\"Fecha_fin: {Fecha_fin}\")\n",
    "    logging.info(f\"Duracion: {duracion}\")\n",
    "    logging.info(f\"Fuentes: {fuentes}\")\n",
    "    logging.info(f\"Cantidad_registros: {cantidad_registros}\")\n",
    "    logging.info(f\"Destino: {destino}\")\n",
    "    logging.info(f\"Estado: {estado}\")\n",
    "    logging.info(\"Lugar errores: \" + ' | '.join(map(str, funcion_error)))\n",
    "    logging.info(\"Descripción errores: \" + ' | '.join(map(str, descripcion_error)))\n",
    "    if estado[0] == 1:\n",
    "        logging.info(\"Ejecución exitosa\")\n",
    "    logging.info(\"------------------------------------------------------------------\")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conexion_BD():\n",
    "    \"\"\"\n",
    "    Función que genera la conexión hacia la base de datos por medio de la libreria psycopg2\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        conn: Conexion con la base de datos\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host= par.host,\n",
    "            port= par.port,\n",
    "            dbname= par.bd_inteligencia_comercial,\n",
    "            user= par.usuario,\n",
    "            password= par.contrasena\n",
    "        )\n",
    "        return conn\n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append(par.nombre_archivo_metas)\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(conexion_BD.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertarErroresDB():\n",
    "    \"\"\"\n",
    "    Metodo para insertar a POSTGRESQL los errores capturados durante la ejecución\n",
    "    Argumentos Globales:\n",
    "        fecha_inicio: Captura la fecha en que inicio la ejecución\n",
    "        fecha_fin: Captura la fecha en que finalizo la ejecución\n",
    "        duracion: Duración del procesamiento\n",
    "        fuente: Indica la fuente de donde provienen los datos\n",
    "        cantidad_registros: Cantidad de registros por fuente\n",
    "        destino: Indica la tabla a donde se estan ingestando los datos\n",
    "        id_estado: Indica el estado del proceso definidos en la base de datos \n",
    "        funcion_error: Indica la función donde se esta presentando una falla\n",
    "        descripcion_error: Descripción del error generado\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convertir las cadenas de texto a objetos datetime\n",
    "        fecha_inicio_tr = datetime.strptime(fecha_inicio, \"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin_tr = datetime.strptime(fecha_fin, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        duracion_proceso_timedelta = fecha_fin_tr - fecha_inicio_tr\n",
    "        duracion_proceso_seconds = duracion_proceso_timedelta.total_seconds()\n",
    "        \n",
    "        errores = pd.DataFrame({\n",
    "            'fecha_inicio': fecha_inicio,\n",
    "            'fecha_fin': fecha_fin,\n",
    "            'duracion': duracion_proceso_seconds,\n",
    "            'fuente': fuentes,\n",
    "            'cantidad_registros': cantidad_registros,\n",
    "            'destino': destino,\n",
    "            'id_estado': estado,\n",
    "            'funcion_error': funcion_error,\n",
    "            'descripcion_error': descripcion_error\n",
    "        })\n",
    "        \n",
    "        conexion_errores = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'control_procesamiento'\n",
    "        nombre_tabla = 'tb_errores_cargue'\n",
    "        errores.to_sql(nombre_tabla, con=conexion_errores, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        cargueResumen(id_ejecucion_en_curso, fecha_inicio_tr,par.nombre_archivo_metas,0,par.destino_metas,2) \n",
    "        salidaLogMonitoreo()\n",
    "\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append(par.ruta_fuente_metas)\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(insertarErroresDB.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizarFechaFinProcesamiento(id_ejecucion, fecha_fin_date, duracion_proceso_seg):\n",
    "    \"\"\"\n",
    "    Función que actualiza la fecha fin de procesamiento y duración para el proceso que se ejecuto.\n",
    "    Utilizando cursores\n",
    "    \n",
    "    Argumentos:\n",
    "        id_ejecucion: id del proceso ejecutado\n",
    "        fecha_fin_date: Fecha fin de procesamiento\n",
    "        duracion_proceso_seg: Duración en segundos del procesamiento\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        \n",
    "        conn = conexion_BD()\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        update_query = \"\"\"\n",
    "            UPDATE control_procesamiento.tb_resumen_cargue \n",
    "            SET fecha_fin_procesamiento = %s,\n",
    "            duracion_segundos = %s\n",
    "            WHERE id_ejecucion = %s\n",
    "        \"\"\"\n",
    "        cur.execute(update_query, (fecha_fin_date, duracion_proceso_seg, id_ejecucion))\n",
    "        conn.commit()\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        fuentes.append(par.nombre_archivo_metas)\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(actualizarFechaFinProcesamiento.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uuid():\n",
    "    \"\"\"\n",
    "    Función que genera un numero alfanumerico para creación de llaves primarias y foraneas\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        return str(uuid.uuid4())\n",
    "    \n",
    "    except Exception as e:\n",
    "        fuentes.append(par.nombre_archivo_metas)\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(generate_uuid.__name__)\n",
    "        insertarErroresDB()\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultarTablaHistorico(tabla_consulta):\n",
    "    \"\"\"\n",
    "    Función que consulta los datos historicos existentes en la base de datos de las tablas de domiminio\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "    \n",
    "        \n",
    "        engine = conexion_BD()\n",
    "        fecha_actual = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        sql_consulta = f\"\"\"\n",
    "            SELECT fecha_procesamiento\n",
    "            FROM fuentes_cruda.{tabla_consulta} \n",
    "            WHERE DATE(fecha_procesamiento) = '{fecha_actual}'\n",
    "        \"\"\"\n",
    "\n",
    "        df_tabla_bd = pd.read_sql(sql_consulta, engine)\n",
    "\n",
    "        return df_tabla_bd\n",
    "     \n",
    "       \n",
    "    except Exception as e:\n",
    "        fuentes.append(par.nombre_archivo_planta)\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consultarTablaHistorico.__name__)\n",
    "        descripcion_error.append(str(e))\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def archivo_modificado_hoy(ruta_archivo):\n",
    "    \"\"\"\n",
    "    Función que indica la fecha de actualización de las fuentes\n",
    "    \n",
    "    Argumentos:\n",
    "        ruta_archivo: Contiene la ruta donde se encuentran los archivos fuente\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        fecha_modificacion = datetime.fromtimestamp(os.path.getmtime(ruta_archivo)).date()\n",
    "\n",
    "        return fecha_modificacion == fecha_actual\n",
    "    \n",
    "    except Exception as e:\n",
    "        fuentes.append(par.nombre_archivo_metas)\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(archivo_modificado_hoy.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importarMetas(ruta, hoja_calculo_canal_fijo, hoja_calculo_canal_movil, hoja_calculo_red, hoja_calculo_bajas_fijo):\n",
    "    \"\"\"\n",
    "    Este metodo realiza la importacion de las metas de la fuente cruda\n",
    "    Argumentos:\n",
    "        ruta: ruta donde se encuentra el archivo\n",
    "        nombre_archivo: nombre del archivo\n",
    "        hoja_calculo_canal_fijo: nombre de la hoja de calculo del canal fijo\n",
    "        hoja_calculo_canal_movil: nombre de la hoja de calculo del canal movil\n",
    "        hoja_calculo_red: nombre de la hoja de calculo de la red\n",
    "        hoja_calculo_bajas_fijo: nombre de la hoja de calculo de las bajas fijo\n",
    "    Retorna: \n",
    "        df: dataframe con los datos de la fuente cruda\n",
    "    Excepciones manejadas:\n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        files = os.listdir(ruta)\n",
    "\n",
    "        # Unir la ruta con el nombre del archivo para obtener la ruta completa\n",
    "        full_paths = [os.path.join(ruta, file) for file in files]\n",
    "\n",
    "        # Obtener el archivo más reciente de la ruta\n",
    "        newest_file = max(full_paths, key=os.path.getctime)\n",
    "    \n",
    "        # Leer las hojas de cálculo especificadas\n",
    "        base_excel_canal_fijo = pd.read_excel(newest_file, sheet_name=hoja_calculo_canal_fijo, engine='openpyxl')\n",
    "        base_excel_canal_movil = pd.read_excel(newest_file, sheet_name=hoja_calculo_canal_movil, engine='openpyxl')\n",
    "        base_excel_red = pd.read_excel(newest_file, sheet_name=hoja_calculo_red, engine='openpyxl')\n",
    "        base_excel_bajas_fijo = pd.read_excel(newest_file, sheet_name=hoja_calculo_bajas_fijo, engine='openpyxl')\n",
    "        \n",
    "        return base_excel_canal_fijo, base_excel_canal_movil, base_excel_red, base_excel_bajas_fijo\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Asegúrate de definir estas listas o variables en algún lugar antes de utilizarlas.\n",
    "        fuentes.append(newest_file)  # Si no se define 'par', uso 'newest_file'\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(importarMetas.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        \n",
    "        # Las funciones 'insertarErroresDB' y 'salidaLogMonitoreo' deben estar implementadas\n",
    "        insertarErroresDB()  # Esta función debe estar definida\n",
    "        salidaLogMonitoreo()  # Esta función debe estar definida también\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combinarMetas(df_canal_fijo, df_canal_movil, df_red, df_bajas_fijo):\n",
    "    \"\"\"\n",
    "    Este metodo realiza la union de los dataframes de las metas\n",
    "    Argumentos:\n",
    "        df_canal: dataframe del canal\n",
    "        df_red: dataframe de la red\n",
    "        df_tipo: dataframe del tipo\n",
    "    Retorna: \n",
    "        df: dataframe con los datos de la fuente cruda\n",
    "    Excepciones manejadas:\n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try :\n",
    "        # Se crea una columna para identificar el tipo de fuente\n",
    "        df_canal_fijo['Hoja_Fuente'] = 'META-'+ par.nombre_hoja_canal_fijo\n",
    "        df_canal_movil['Hoja_Fuente'] = 'META-'+ par.nombre_hoja_canal_movil\n",
    "        df_red['Hoja_Fuente'] = 'META-'+ par.nombre_hoja_red\n",
    "        df_bajas_fijo['Hoja_Fuente'] = 'META-'+ par. nombre_hoja_bajas_fijo\n",
    "\n",
    "        #Se Crea una columna detalle para todas los Dataframe donde contiene el nombre de la columna del detalle\n",
    "        df_canal_fijo['Detalle'] = 'Canal'\n",
    "        df_canal_movil['Detalle'] = 'Canal'\n",
    "        df_red['Detalle'] = 'Red'\n",
    "        df_bajas_fijo['Detalle'] = 'Crecimiento'\n",
    "\n",
    "        # Se combinan los dataframes de Canal, Red y Tipo\n",
    "        df_combinado = pd.concat([df_canal_fijo, df_canal_movil, df_red, df_bajas_fijo], ignore_index=True)\n",
    "        \n",
    "        #Se Crea la columna Tipo, donde se contienen los datos de las hojas\n",
    "        #df_combinado['Tipo'] = df_combinado['Canal'].fillna('') + df_combinado['Red'].fillna('') + df_combinado['Crecimiento'].fillna('')\n",
    "        #df_combinado.drop(columns=['Canal', 'Red', 'Crecimiento'], inplace=True)\n",
    "        df_combinado = df_combinado.rename(columns={'Red': 'Tipo'})\n",
    "        df_combinado['Tipo'] = df_combinado['Tipo'].fillna('')\n",
    "\n",
    "        #Cambiar mes de nombre a número\n",
    "        df_combinado['Mes'] = df_combinado['Mes'].map({'Enero': 1, 'Febrero': 2, 'Marzo': 3, 'Abril': 4, 'Mayo': 5, 'Junio': 6, 'Julio': 7, 'Agosto': 8, 'Septiembre': 9, 'Octubre': 10, 'Noviembre': 11, 'Diciembre': 12})\n",
    "\n",
    "        df_combinado['CC'] = df_combinado['CC'].fillna(0)\n",
    "        df_combinado['CC'] = df_combinado['CC'].astype(int)\n",
    "\n",
    "        # Se reordenan las columnas\n",
    "        df_combinado = df_combinado[['CC', 'Nombre', 'Mes', 'Hoja_Fuente', 'Detalle', 'Tipo', 'Meta']]\n",
    "\n",
    "        return df_combinado\n",
    "    \n",
    "    except Exception as e:\n",
    "        fuentes.append(par.nombre_archivo_metas)\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(combinarMetas.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargueDatosBD(df_final):\n",
    "    \"\"\"\n",
    "    Este metodo realiza el cargue de los datos a la base de datos\n",
    "    Argumentos:\n",
    "        df_final: dataframe con los datos de la fuente cruda\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas:\n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial}')\n",
    "    \n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'fuentes_cruda'\n",
    "        nombre_tabla = 'tb_datos_crudos_metas'\n",
    "\n",
    "        df_final.to_sql(nombre_tabla, con = conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append(par.nombre_archivo_metas)\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueDatosBD.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargueResumen(id_ejecucion, fecha_inicio_date,fuentes,cantidad_registros,destino,estado):\n",
    "    \"\"\"\n",
    "    Función que se encarga de cargar estadisticas de los datos que estan siendo procesados\n",
    "    \n",
    "    Argumentos:\n",
    "        id_ejecucion: Contiene un numero alfanumerico para creación de llaves primarias y foraneas de la base de datos\n",
    "        fecha_inicio_date: Fecha de inicio del procesamiento\n",
    "        fecha_fin_date: Fecha de fin del procesamiento\n",
    "        duracion_proceso: Duración del procesamiento \n",
    "        fuentes: Fuentes de donde provienen los datos\n",
    "        cantidad_registros: Cantidad de registros procesados\n",
    "        destino: Tabla donde se ingestan los datos\n",
    "        estado: Indica el estado del proceso de acuerdo a lo definido en la base de datos en la tabla control_procesamiento.estados_cargue \n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        \n",
    "        df_resumen_cargue = pd.DataFrame({\n",
    "            'id_ejecucion': id_ejecucion,\n",
    "            'fecha_inicio_procesamiento': fecha_inicio_date,\n",
    "            'fuentes': fuentes,\n",
    "            'cantidad_registros': cantidad_registros,\n",
    "            'destino': [destino],\n",
    "            'id_estado': [estado],\n",
    "        })\n",
    "\n",
    "        #errores de conexion se ponen a mano\n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'control_procesamiento'\n",
    "        nombre_tabla = 'tb_resumen_cargue'\n",
    "        \n",
    "        df_resumen_cargue.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append(par.nombre_archivo_metas)\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueResumen.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seleccionCamposMetas(df_combinado,fecha_inicio_date,id_ejecucion):\n",
    "    \"\"\"\n",
    "    Función que se encarga de añadir campos necesarios o faltantes para el cargue a la base de datos\n",
    "\n",
    "    Argumentos:\n",
    "        df_combinado: Contiene el dataframe que se requiere para añadir los campos\n",
    "        fecha_inicio_date: Fecha de inicio de procesamiento\n",
    "        id_ejecucion: ID de ejecucion\n",
    "    Retorna: \n",
    "        df_base: Retorna el dataframe con los campos faltantes\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        df_base = df_combinado.copy()\n",
    "\n",
    "        df_base['anio'] = pd.to_datetime(fecha_inicio_date).year\n",
    "        df_base['id'] = [generate_uuid().upper() for _ in range(len(df_base))]\n",
    "        df_base['id_ejecucion'] = id_ejecucion\n",
    "        df_base['fecha_procesamiento'] = fecha_inicio_date\n",
    "        df_base['fuente'] = par.nombre_archivo_metas\n",
    "        #df_base['id_estado'] = 1\n",
    "        df_base['id_estado_registro'] = 1\n",
    "\n",
    "        df_base = df_base.rename(columns={\n",
    "            'CC': 'identificacion',\n",
    "            'Nombre': 'nombre',\n",
    "            'Mes': 'mes',\n",
    "            'Hoja_Fuente': 'hoja_fuente',\n",
    "            'Tipo': 'tipo',\n",
    "            'Meta': 'couta_mes',\n",
    "            'Detalle': 'detalle'\n",
    "        })\n",
    "        \n",
    "        df_base = df_base[['id', 'id_ejecucion', 'identificacion', 'nombre', 'anio', 'mes', 'hoja_fuente', 'tipo', 'couta_mes', 'fecha_procesamiento', 'fuente', 'id_estado_registro', 'detalle']]\n",
    "        df_base['identificacion'] = df_base['identificacion'].astype('int64')\n",
    "        return df_base\n",
    "    \n",
    "    except Exception as e:\n",
    "        fuentes.append(par.nombre_archivo_metas)\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(seleccionCamposMetas.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cambioDeEstado():\n",
    "    \"\"\"\n",
    "    Este metodo realiza el cambio de id_estado de los cargues anteriores de metas\n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas:\n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try: \n",
    "        conn = conexion_BD()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        query = \"UPDATE fuentes_cruda.tb_datos_crudos_metas SET id_estado_registro = 4 WHERE id_estado_registro = 1\"\n",
    "        cursor.execute(query)\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        fuentes.append(par.nombre_archivo_metas)\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cambioDeEstado.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiezaCamposString(df):\n",
    "    for campo in df.select_dtypes(include=['object']).columns:\n",
    "        df[campo] = df[campo].astype(str) \\\n",
    "                             .str.upper() \\\n",
    "                             .str.strip() \\\n",
    "                             .str.replace('\\n', '', regex=True) \\\n",
    "                             .str.replace('\\r', '', regex=True) \\\n",
    "                             .str.replace('\\t', '', regex=True) \\\n",
    "                             .str.replace('  ', '', regex=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crucePlantaComercial(df_base):\n",
    "    \"\"\"\n",
    "    Función que se encarga de cruzar la identificacion del comercial con su id_cargo\n",
    "\n",
    "    Argumentos:\n",
    "        df_base: Contiene el dataframe que se requiere cruzar estos campos\n",
    "    Retorna: \n",
    "        df_final: Retorna el dataframe con los campos faltantes\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    conn = conexion_BD()\n",
    "\n",
    "    if conn:\n",
    "        \n",
    "        try:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"SELECT identificacion, id_cargo FROM fuentes_cruda.tb_datos_crudos_planta_comercial WHERE version = ( SELECT MAX(version) FROM fuentes_cruda.tb_datos_crudos_planta_comercial ) and identificacion != 0 and id_cargo != 0\")\n",
    "                rows = cur.fetchall()\n",
    "\n",
    "                df_resultados = pd.DataFrame(rows, columns=['identificacion', 'id_cargo'])\n",
    "                \n",
    "                # Perform the cross-reference and add id_cargo to the base dataframe\n",
    "                df_final = pd.merge(df_base, df_resultados, on='identificacion', how='left')\n",
    "                df_final['id_cargo'] = df_final['id_cargo'].fillna(0)\n",
    "                df_final['id_cargo'] = df_final['id_cargo'].astype(int)\n",
    "                \n",
    "                df_final = df_final[['id', 'id_ejecucion', 'identificacion', 'nombre','id_cargo', 'anio', 'mes', 'hoja_fuente', 'tipo', 'couta_mes', 'fecha_procesamiento', 'fuente', 'id_estado_registro', 'detalle']]\n",
    "                return df_final\n",
    "            \n",
    "        except SQLAlchemyError as e:\n",
    "            fuentes.append(par.nombre_archivo_metas)\n",
    "            cantidad_registros.append(0)\n",
    "            estado.append(2)\n",
    "            funcion_error.append(crucePlantaComercial.__name__)\n",
    "            descripcion_error.append(str(e)[:100])\n",
    "            salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurarLogging():\n",
    "    \"\"\"\n",
    "    Configura el logging para escribir en un archivo y en la salida estándar\n",
    "    Utiliza la ruta definida en par.ruta_log para el directorio de logs.\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        None\n",
    "    \"\"\"\n",
    "    # Configuración del logging\n",
    "    log_directory = par.ruta_log  # Usa la ruta definida en config.py\n",
    "    log_file = os.path.join(log_directory, \"cargue_datos_crudos_metas.log\")\n",
    "\n",
    "    # Crear el directorio si no existe\n",
    "    if not os.path.exists(log_directory):\n",
    "        os.makedirs(log_directory)\n",
    "\n",
    "    # Configurar el logger\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file, mode='a'),  # 'a' para modo append\n",
    "            #logging.StreamHandler()  # Para imprimir en pantalla\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ya hay registros en el histórico. No se almacenará nada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INTCOM\\AppData\\Local\\Temp\\2\\ipykernel_14464\\3888982384.py:23: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_tabla_bd = pd.read_sql(sql_consulta, engine)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    Programa principal que se encarga de controlar el orden en que se debe ejecutar el procesamiento \n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        configurarLogging()\n",
    "        id_ejecucion = generate_uuid().upper()\n",
    "        id_ejecucion_en_curso = id_ejecucion\n",
    "        \n",
    "        fecha_inicio = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_inicio_date = datetime.strptime(fecha_inicio, \"%Y-%m-%d %H:%M:%S\")\n",
    "        archivos = [f for f in os.listdir(par.ruta_fuente_metas) if archivo_modificado_hoy(os.path.join(par.ruta_fuente_metas, f))]\n",
    "        archivos_actualizados = archivos\n",
    "        \n",
    "        # Consultar histórico\n",
    "        df_historico = consultarTablaHistorico('tb_datos_crudos_metas')\n",
    "\n",
    "        # Asegurar que la columna 'fecha_procesamiento' sea de tipo datetime\n",
    "        df_historico['fecha_procesamiento'] = pd.to_datetime(df_historico['fecha_procesamiento'], errors='coerce')\n",
    "\n",
    "        # Verificar si la fecha de hoy ya está en el histórico\n",
    "        if fecha_inicio_date.date() in df_historico['fecha_procesamiento'].dt.date.unique():\n",
    "            print(\"Ya hay registros en el histórico. No se almacenará nada.\")\n",
    "        else:\n",
    "            if par.nombre_archivo_metas in archivos_actualizados:  \n",
    "                \n",
    "                # Importar metas\n",
    "                df_canal_fijo, df_canal_movil, df_red, df_bajas_fijo = importarMetas(par.ruta_fuente_metas, par.nombre_hoja_canal_fijo, par.nombre_hoja_canal_movil, par.nombre_hoja_red, par.nombre_hoja_bajas_fijo)\n",
    "\n",
    "                # Combinar metas\n",
    "                df_combinado = combinarMetas(df_canal_fijo, df_canal_movil, df_red, df_bajas_fijo)\n",
    "\n",
    "                # Limpieza de campos\n",
    "                df_limpiado = limpiezaCamposString(df_combinado)\n",
    "                \n",
    "                # Seleccionar campos\n",
    "                df_base = seleccionCamposMetas(df_limpiado, fecha_actual, id_ejecucion_en_curso)\n",
    "             \n",
    "                df_metas_nuevo = crucePlantaComercial(df_base)\n",
    "                \n",
    "                registros = len(df_metas_nuevo)\n",
    "                cantidad_registros.append(registros)\n",
    "\n",
    "                if registros > 0:\n",
    "                    df_resumen = cargueResumen(id_ejecucion, fecha_inicio_date, par.nombre_archivo_metas, registros, par.destino_metas, 1)\n",
    "                    cambioDeEstado()\n",
    "                    \n",
    "                    cargueDatosBD(df_metas_nuevo)\n",
    "\n",
    "                fecha_fin = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                fecha_fin_date = datetime.strptime(fecha_fin, \"%Y-%m-%d %H:%M:%S\")\n",
    "                duracion_proceso = fecha_fin_date - fecha_inicio_date\n",
    "                duracion_proceso_seg = int(duracion_proceso.total_seconds())\n",
    "                actualizarFechaFinProcesamiento(id_ejecucion, fecha_fin_date, duracion_proceso_seg)\n",
    "\n",
    "            else:#INCLUSION DE CODIGO ELSE MARIO PUELLO 16/01/2024\n",
    "                duracion_proceso =None\n",
    "                \n",
    "            duracion.append(str(duracion_proceso))\n",
    "            estado.append(1)\n",
    "            salidaLogMonitoreo()\n",
    "    except Exception as e:\n",
    "        fuentes.append(par.nombre_archivo_metas)\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(\"__main__\")\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
