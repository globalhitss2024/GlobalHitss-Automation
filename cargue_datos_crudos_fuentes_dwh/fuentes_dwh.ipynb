{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl as opn\n",
    "import psycopg2\n",
    "from psycopg2 import sql, Error, OperationalError\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil  # Para mover el archivo descargado\n",
    "import logging\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import sys\n",
    "sys.path.append('C:\\\\ambiente_desarrollo\\\\dev-empresas-negocios-env\\\\desarrollo_produccion')\n",
    "import parametros_produccion as par\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES GLOBALES\n",
    "fecha_actual = datetime.today().date()\n",
    "duracion = []\n",
    "fuentes = []\n",
    "cantidad_registros = []\n",
    "estado = []\n",
    "fecha_fin_procesamiento =[]\n",
    "funcion_error = []\n",
    "descripcion_error = []\n",
    "id_ejecucion = str(uuid.uuid4())  # Generar UUID de ejecución\n",
    "destino = 'Fuentes DWH'\n",
    "id_estado = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def salidaLogMonitoreo():\n",
    "    \n",
    "    Este metodo captura la informacion que se desea imprimir en el Log\n",
    "    para monitoreo y funcionamiento del desarrollo\n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        None\n",
    "    \n",
    "    Fecha_fin = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    print(f\"Fecha_inicio: {fecha_inicio}\")\n",
    "    print(f\"Fecha_fin: {Fecha_fin}\")\n",
    "    print(f\"Duracion: {duracion}\")\n",
    "    print(f\"Fuentes: {fuentes}\")\n",
    "    print(f\"Cantidad_registros: {cantidad_registros}\")\n",
    "    print(f\"Destino: {destino}\")\n",
    "    print(f\"Estado: {estado}\")\n",
    "    print(\"Lugar errores: \", ' | '.join(map(str, funcion_error)))\n",
    "    print(\"Descripción errores: \", ' | '.join(map(str, descripcion_error)))\n",
    "    if estado[0] == 1 :\n",
    "        print(\"Ejecución exitosa\")\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "\"\"\"\n",
    "def salidaLogMonitoreo():\n",
    "    \"\"\"\n",
    "    Este método captura la información que se desea imprimir en el Log\n",
    "    para monitoreo y funcionamiento del desarrollo.\n",
    "    \"\"\"\n",
    "    Fecha_fin = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    logging.info(f\"Fecha_inicio: {fecha_inicio}\")\n",
    "    logging.info(f\"Fecha_fin: {Fecha_fin}\")\n",
    "    logging.info(f\"Duracion: {duracion}\")\n",
    "    logging.info(f\"Fuentes: {fuentes}\")\n",
    "    logging.info(f\"Cantidad_registros: {cantidad_registros}\")\n",
    "    logging.info(f\"Destino: {destino}\")\n",
    "    logging.info(f\"Estado: {estado}\")\n",
    "    logging.info(\"Lugar errores: \" + ' | '.join(map(str, funcion_error)))\n",
    "    logging.info(\"Descripción errores: \" + ' | '.join(map(str, descripcion_error)))\n",
    "    if estado[0] == 1:\n",
    "        logging.info(\"Ejecución exitosa\")\n",
    "    logging.info(\"------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar resumen de datos en la BD\n",
    "def cargueResumen(id_ejecucion, fecha_inicio_date,fecha_fin_procesamiento, duracion,fuentes, cantidad_registros, destino, id_estado):\n",
    "    try:\n",
    "        df_resumen_cargue = pd.DataFrame({\n",
    "        'id_ejecucion': [id_ejecucion],  # Envolver en una lista\n",
    "        'fecha_inicio_procesamiento': [fecha_inicio_date],\n",
    "        'fecha_fin_procesamiento': [fecha_fin_procesamiento], \n",
    "        'duracion_segundos': [duracion],\n",
    "        'fuentes': [fuentes],\n",
    "        'cantidad_registros': [cantidad_registros],\n",
    "        'destino': [destino],\n",
    "        'id_estado': [id_estado],\n",
    "    })\n",
    "        \n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'control_procesamiento'\n",
    "        nombre_tabla = 'tb_resumen_cargue'\n",
    "        \n",
    "        df_resumen_cargue.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Fuentes DWH')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueResumen.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurarLogging():\n",
    "    \"\"\"\n",
    "    Configura el logging para escribir en un archivo y en la salida estándar\n",
    "    Utiliza la ruta definida en par.ruta_log para el directorio de logs.\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        None\n",
    "    \"\"\"\n",
    "    # Configuración del logging\n",
    "    log_directory = par.ruta_log_produccion  # Usa la ruta definida en config.py\n",
    "    log_file = os.path.join(log_directory, \"Fuentes DWH.log\")\n",
    "\n",
    "    # Crear el directorio si no existe\n",
    "    if not os.path.exists(log_directory):\n",
    "        os.makedirs(log_directory)\n",
    "\n",
    "    # Configurar el logger\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file, mode='a'),  # 'a' para modo append\n",
    "            #logging.StreamHandler()  # Para imprimir en pantalla\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertarErroresDB():\n",
    "    \"\"\"\n",
    "    Metodo para insertar a POSTGRESQL los errores capturados durante la ejecución\n",
    "    Argumentos Globales:\n",
    "        fecha_inicio: Captura la fecha en que inicio la ejecución\n",
    "        fecha_fin: Captura la fecha en que finalizo la ejecución\n",
    "        duracion: Duración del procesamiento\n",
    "        fuente: Indica la fuente de donde provienen los datos\n",
    "        cantidad_registros: Cantidad de registros por fuente\n",
    "        destino: Indica la tabla a donde se estan ingestando los datos\n",
    "        id_estado: Indica el estado del proceso definidos en la base de datos \n",
    "        funcion_error: Indica la función donde se esta presentando una falla\n",
    "        descripcion_error: Descripción del error generado\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convertir las cadenas de texto a objetos datetime\n",
    "        fecha_inicio_tr = datetime.strptime(fecha_inicio, \"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin_tr = datetime.strptime(fecha_fin, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        duracion_proceso_timedelta = fecha_fin_tr - fecha_inicio_tr\n",
    "        duracion_proceso_seconds = duracion_proceso_timedelta.total_seconds()\n",
    "        \n",
    "        errores = pd.DataFrame({\n",
    "            'fecha_inicio': fecha_inicio,\n",
    "            'fecha_fin': fecha_fin,\n",
    "            'duracion': duracion_proceso_seconds,\n",
    "            'fuente': fuentes,\n",
    "            'cantidad_registros': cantidad_registros,\n",
    "            'destino': destino,\n",
    "            'id_estado': estado,\n",
    "            'funcion_error': funcion_error,\n",
    "            'descripcion_error': descripcion_error\n",
    "        })\n",
    "        \n",
    "        conexion_errores = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'control_procesamiento'\n",
    "        nombre_tabla = 'tb_errores_cargue'\n",
    "        errores.to_sql(nombre_tabla, con=conexion_errores, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        cargueResumen(id_ejecucion_en_curso, fecha_inicio_tr,2) \n",
    "        salidaLogMonitoreo()\n",
    "\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Fuente DWH')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(insertarErroresDB.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultarHistoricoBD_Dim_agentes():\n",
    "    \"\"\"\n",
    "    Función que consulta los datos historicos existentes en la base de datos de la tabla de tb_datos_crudos_legalizadas\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        df_historico_mb : Retorna el historico de los datos cargados en la BD\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "\n",
    "        #engine = conexion_BD()\n",
    "        sql_consulta = \"Select * \\\n",
    "                    from fuentes_cruda.tb_datos_crudos_dwh_dim_agentes\"\n",
    "        df_historico_dim_agentes = pd.read_sql(sql_consulta, engine)\n",
    "    \n",
    "    \n",
    "        return df_historico_dim_agentes\n",
    "        \n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DWH')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consultarHistoricoBD_Dim_agentes.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultarHistoricoBD_Plan_negocios():\n",
    "    \"\"\"\n",
    "    Función que consulta los datos historicos existentes en la base de datos de la tabla de tb_datos_crudos_legalizadas\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        df_historico_mb : Retorna el historico de los datos cargados en la BD\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "\n",
    "        #engine = conexion_BD()\n",
    "        sql_consulta = \"Select * \\\n",
    "                    from fuentes_cruda.tb_datos_crudos_dwh_plan_negocios\"\n",
    "        df_historico_Plan_negocios = pd.read_sql(sql_consulta, engine)\n",
    "    \n",
    "    \n",
    "        return df_historico_Plan_negocios\n",
    "        \n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DWH')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consultarHistoricoBD_Plan_negocios.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultarHistoricoBD_Ventas():\n",
    "    \"\"\"\n",
    "    Función que consulta los datos historicos existentes en la base de datos de la tabla de tb_datos_crudos_legalizadas\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        df_historico_mb : Retorna el historico de los datos cargados en la BD\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "\n",
    "        #engine = conexion_BD()\n",
    "        sql_consulta = \"Select * \\\n",
    "                    from fuentes_cruda.tb_datos_crudos_dwh_ventas\"\n",
    "        df_historico_Ventas = pd.read_sql(sql_consulta, engine)\n",
    "    \n",
    "    \n",
    "        return df_historico_Ventas\n",
    "        \n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DWH')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consultarHistoricoBD_Ventas.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultarHistoricoBD_Cp():\n",
    "    \"\"\"\n",
    "    Función que consulta los datos historicos existentes en la base de datos de la tabla de tb_datos_crudos_legalizadas\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        df_historico_mb : Retorna el historico de los datos cargados en la BD\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "\n",
    "        #engine = conexion_BD()\n",
    "        sql_consulta = \"Select * \\\n",
    "                    from fuentes_cruda.tb_datos_crudos_dwh_cambio_plan\"\n",
    "        df_historico_cp = pd.read_sql(sql_consulta, engine)\n",
    "    \n",
    "    \n",
    "        return df_historico_cp\n",
    "        \n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DWH')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consultarHistoricoBD_Cp.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultarHistoricoBD_Base_seg():\n",
    "    \"\"\"\n",
    "    Función que consulta los datos historicos existentes en la base de datos de la tabla de tb_datos_crudos_legalizadas\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        df_historico_mb : Retorna el historico de los datos cargados en la BD\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "\n",
    "        #engine = conexion_BD()\n",
    "        sql_consulta = \"Select * \\\n",
    "                    from fuentes_cruda.tb_datos_crudos_dwh_base_seg\"\n",
    "        df_historico_base_seg = pd.read_sql(sql_consulta, engine)\n",
    "    \n",
    "    \n",
    "        return df_historico_base_seg\n",
    "        \n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DWH')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consultarHistoricoBD_Base_seg.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultarHistoricoBD_Codigo_Ciudad():\n",
    "    \"\"\"\n",
    "    Función que consulta los datos historicos existentes en la base de datos de la tabla de tb_datos_crudos_legalizadas\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        df_historico_mb : Retorna el historico de los datos cargados en la BD\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        engine = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "\n",
    "        #engine = conexion_BD()\n",
    "        sql_consulta = \"Select * \\\n",
    "                    from fuentes_cruda.tb_datos_crudos_dwh_codigo_ciudad\"\n",
    "        df_historico_codigo_ciudad = pd.read_sql(sql_consulta, engine)\n",
    "    \n",
    "    \n",
    "        return df_historico_codigo_ciudad\n",
    "        \n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DWH')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consultarHistoricoBD_Codigo_Ciudad.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargueDatosBD_Dim_agentes(df_final):\n",
    "    \"\"\"\n",
    "    Función que se encarga de cargar los dataframes procesados hacia la base de datos\n",
    "    \n",
    "    Argumentos:\n",
    "        df_final: Contiene el dataframe que se requiere cargar a la BD\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'fuentes_cruda'\n",
    "        nombre_tabla = 'tb_datos_crudos_dwh_dim_agentes'\n",
    "        \n",
    "        df_final.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Dhw dim agentes')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueDatosBD_Dim_agentes.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargueDatosBD_Plan_negocios(df_final):\n",
    "    \"\"\"\n",
    "    Función que se encarga de cargar los dataframes procesados hacia la base de datos\n",
    "    \n",
    "    Argumentos:\n",
    "        df_final: Contiene el dataframe que se requiere cargar a la BD\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'fuentes_cruda'\n",
    "        nombre_tabla = 'tb_datos_crudos_dwh_plan_negocios'\n",
    "        \n",
    "        df_final.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Dwh Plan negocio')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueDatosBD_Plan_negocios.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargueDatosBD_Ventas(df_final):\n",
    "    \"\"\"\n",
    "    Función que se encarga de cargar los dataframes procesados hacia la base de datos\n",
    "    \n",
    "    Argumentos:\n",
    "        df_final: Contiene el dataframe que se requiere cargar a la BD\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'fuentes_cruda'\n",
    "        nombre_tabla = 'tb_datos_crudos_dwh_ventas'\n",
    "        \n",
    "        df_final.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Dwh Ventas')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueDatosBD_Ventas.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargueDatosBD_Cp(df_final):\n",
    "    \"\"\"\n",
    "    Función que se encarga de cargar los dataframes procesados hacia la base de datos\n",
    "    \n",
    "    Argumentos:\n",
    "        df_final: Contiene el dataframe que se requiere cargar a la BD\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'fuentes_cruda'\n",
    "        nombre_tabla = 'tb_datos_crudos_dwh_cambio_plan'\n",
    "        \n",
    "        df_final.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Dwh Ventas')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueDatosBD_Cp.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargueDatos_Base_seg(df_final):\n",
    "    \"\"\"\n",
    "    Función que se encarga de cargar los dataframes procesados hacia la base de datos\n",
    "    \n",
    "    Argumentos:\n",
    "        df_final: Contiene el dataframe que se requiere cargar a la BD\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'fuentes_cruda'\n",
    "        nombre_tabla = 'tb_datos_crudos_dwh_base_seg'\n",
    "        \n",
    "        df_final.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Dwh Ventas')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueDatos_Base_seg.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargueDatos_Codigo_Ciudad(df_final):\n",
    "    \"\"\"\n",
    "    Función que se encarga de cargar los dataframes procesados hacia la base de datos\n",
    "    \n",
    "    Argumentos:\n",
    "        df_final: Contiene el dataframe que se requiere cargar a la BD\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'fuentes_cruda'\n",
    "        nombre_tabla = 'tb_datos_crudos_dwh_codigo_ciudad'\n",
    "        \n",
    "        df_final.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Dwh Ventas')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueDatos_Codigo_Ciudad.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conexion_BD():\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            dbname=\"dwh_db\",    \n",
    "            user=\"45110947\",            \n",
    "            password=\"Mmilu28()*\",    \n",
    "            host=\"100.123.59.140\",          \n",
    "            port=\"5432\"                \n",
    "        )\n",
    "        print(\"Conexión a la base de datos Yellowbrick exitosa.\")\n",
    "        return connection\n",
    "    except Error as e:\n",
    "        print(f\"Error al conectar a la base de datos: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_Dim_agentes(engine):\n",
    "    try:\n",
    "        # Consulta tabla DWH_DB.SEGMENTACION.INH_DIM_AGENTES\n",
    "        consulta = \"\"\"\n",
    "         SELECT\n",
    "        COD_AGENTE,COST_ID_AGENTE,CUSTCODE_AGENTE,NAME_AGENTE,ESTADO_AGENTE,\n",
    "        CANAL as CANAL_COMERCIAL,SUBCANAL,CATEGORIA,REGION_COMERCIAL AS DIRECCION,GERENCIA,\n",
    "        CIUDAD_AGENTE,DEPARTAMENTO,REGION_CONCESION,\n",
    "        UNIDAD\n",
    "        FROM DWH_DB.SEGMENTACION.inh_dim_agentes\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Ejecutando consulta: {consulta}\")  # Imprime la consulta para depuración\n",
    "        cursor = engine.cursor()\n",
    "        cursor.execute(consulta)\n",
    "        resultados = cursor.fetchall()\n",
    "\n",
    "        # Obtener los nombres de las columnas\n",
    "        columnas = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        # Crear el DataFrame con los resultados\n",
    "        df_dim_agentes = pd.DataFrame(resultados, columns=columnas)\n",
    "\n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if df_dim_agentes.empty:\n",
    "            print(\"No se encontraron resultados.\")\n",
    "        else:\n",
    "            df_dim_agentes.head(9)  # Mostrar las primeras 5 filas\n",
    "\n",
    "        return df_dim_agentes\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DHW')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consulta_Dim_agentes.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_Plan_negocios(engine):\n",
    "    try:\n",
    "        # Consulta tabla DWH_DB.SEGMENTACION.INH_DIM_AGENTES\n",
    "        consulta = \"\"\"\n",
    "         select * from DWH_DB.SEGMENTACION.INH_DIM_PLANES t\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Ejecutando consulta: {consulta}\")  # Imprime la consulta para depuración\n",
    "        cursor = engine.cursor()\n",
    "        cursor.execute(consulta)\n",
    "        resultados = cursor.fetchall()\n",
    "\n",
    "        # Obtener los nombres de las columnas\n",
    "        columnas = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        # Crear el DataFrame con los resultados\n",
    "        df_plan_negocios = pd.DataFrame(resultados, columns=columnas)\n",
    "\n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if df_plan_negocios.empty:\n",
    "            print(\"No se encontraron resultados.\")\n",
    "        else:\n",
    "            df_plan_negocios.head(9)  # Mostrar las primeras 5 filas\n",
    "\n",
    "        return df_plan_negocios\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DHW')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consulta_Plan_negocios.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_Cp(engine):\n",
    "    try:\n",
    "        # Consulta tabla DWH_DB.SEGMENTACION.INH_DIM_AGENTES\n",
    "        consulta = \"\"\"\n",
    "         select A.CO_ID\n",
    ",('NEG_PLANES') as FLAG\n",
    ",A.CUSTOMER_ID\n",
    ",A.TELE_NUMB\n",
    ",I.PRODUCTO AS TIPO_LINEA\n",
    ",H.CD_SEGMENTO_CLIENTE AS CODIGO_SEGMENTO_CLIENTE\n",
    ",D.CUSTCODE_AGENTE\n",
    ",H.DS_SEGMENTO_CLIENTE AS SEGMENTACION\n",
    ",H.DS_UNIDAD_NEGOCIO\n",
    ",E.REGION_DESC AS REGION_GEOGRAFICA\n",
    ",D.REGION_COMERCIAL\n",
    ",D.GERENCIA\n",
    ",D.CANAL\n",
    ",D.COD_AGENTE\n",
    ",TO_CHAR(cast(A.FE_ACTIVACION as DATE),'DD/MM/YYYY') FE_ACTIVACION\n",
    ",G.COD_CICLO\n",
    ",A.NO_DIA_CORTE_FACTURACION\n",
    ",A.NO_MESES_ANTIGUEDAD\n",
    ",M.DS_RANGO_ANTIGUEDAD\n",
    ",I.COD_PLAN AS COD_PLAN_ORIGEN\n",
    ",I.PLAN_DESC AS DS_PLAN_ORIGEN\n",
    ",cast (I.VLR_CFM as numeric) AS CFM_PLAN_ORIGEN\n",
    ",I.TIPO_PLAN AS TIPO_PLAN_ORIGEN\n",
    ",N.DS_GAMA_PLAN AS DS_GAMA_PLAN_ORIGEN\n",
    ",J.COD_PLAN AS COD_PLAN_DESTINO\n",
    ",J.PLAN_DESC AS DS_PLAN_DESTINO\n",
    ",pn.ban_pyme as PLAN_NEGOCIOS\n",
    ",cast (J.VLR_CFM as numeric) AS CFM_PLAN_DESTINO\n",
    ",J.TIPO_PLAN AS TIPO_PLAN_DESTINO\n",
    ",O.DS_GAMA_PLAN AS DS_GAMA_PLAN_DESTINO\n",
    ",cast (A.VL_CFM_NETO as numeric)\n",
    ",TO_CHAR(cast(A.FE_CREACION as DATE),'DD/MM/YYYY') FE_CREACION\n",
    ",A.NO_TICKLER\n",
    ",TO_CHAR(cast(A.FE_CAMBIO as DATE),'DD/MM/YYYY') FE_CAMBIO\n",
    ",L.DS_TIPO_GRADE\n",
    ",P.CD_USUARIO_RED AS CD_USUARIO_RED_USUARIO_CRECION\n",
    ",P.DS_NOMBRE_USUARIO AS DS_NOMBRE_USUARIO_CREACION\n",
    ",P.DS_UBICACION_USUARIO AS DS_UBICACION_USUARIO_CREACION\n",
    ",P.DS_CANAL_USUARIO AS DS_CANAL_USUARIO_CREACION\n",
    ",CASE WHEN A.NO_EJECUTADO=1 then 'Cerrado' else 'Abierto' END ESTADO_TICKLER\n",
    ",TO_CHAR(cast(A.FE_CIERRE as DATE),'DD/MM/YYYY') FE_CIERRE\n",
    ",Q.CD_USUARIO_RED AS CD_USUARIO_RED_USUARIO_CIERRE\n",
    ",Q.DS_NOMBRE_USUARIO AS DS_NOMBRE_USUARIO_CIERRE\n",
    ",Q.DS_UBICACION_USUARIO AS DS_UBICACION_USUARIO_CIERRE\n",
    ",Q.DS_CANAL_USUARIO AS DS_CANAL_USUARIO_CIERRE\n",
    ",A.DS_TICKLER\n",
    ",A.DS_NOMBRE_CLIENTE\n",
    ",F.TIPO_IDENTIFICADOR_DESC\n",
    ",A.DS_IDENTIFICACION\n",
    ",T.IDENTIFICACION\n",
    ",T.IDENTIFICACION_MTR\n",
    ",A.DS_TELEFONO_1\n",
    ",A.DS_TELEFONO_2\n",
    ",A.IMEI\n",
    ",A.IMSI\n",
    ",cast (A.VL_CFM_NETO_CONTABLE_VOZ as numeric)\n",
    ",cast (A.VL_CFM_NETO_CONTABLE_DATOS as numeric)\n",
    "FROM DWH_DB.CLIENTES.TBL_FACT_CLIEN_CAMBIOS_PLAN_EJECUTADOS A\n",
    "LEFT JOIN (SELECT CUSTCODE_AGENTE,REGION_COMERCIAL,GERENCIA,CANAL,COD_AGENTE FROM DWH_DB.SEGMENTACION.INH_DIM_AGENTES) D ON A.COD_AGENTE=D.COD_AGENTE\n",
    "LEFT JOIN (SELECT REGION_DESC,COD_MERCADO FROM DWH_DB.SEGMENTACION.INH_DIM_REGIONES) E ON A.COD_MERCADO=E.COD_MERCADO\n",
    "LEFT JOIN (SELECT TIPO_IDENTIFICADOR_DESC,TIPO_IDENTIFICACION FROM DWH_DB.SEGMENTACION.INH_DIM_TIPO_IDENTIFICADOR) F ON A.TIPO_IDENTIFICACION=F.TIPO_IDENTIFICACION\n",
    "LEFT JOIN (SELECT COD_CICLO FROM DWH_DB.SEGMENTACION.INH_DIM_CICLOS) G ON A.COD_CICLO=G.COD_CICLO\n",
    "LEFT JOIN (SELECT CD_SEGMENTO_CLIENTE, DS_SEGMENTO_CLIENTE, DS_UNIDAD_NEGOCIO,SK_SEGMENTO_CLIENTE FROM DWH_DB.CLIENTES.TBL_DIM_SEGMENTO_CLIENTE_T1) H ON A.SK_SEGMENTO_CLIENTE=H.SK_SEGMENTO_CLIENTE\n",
    "LEFT JOIN (SELECT COD_PLAN,PLAN_DESC,VLR_CFM,TIPO_PLAN,PRODUCTO FROM DWH_DB.SEGMENTACION.INH_DIM_PLANES) I ON A.COD_PLAN_ORIGEN=I.COD_PLAN\n",
    "LEFT JOIN (SELECT COD_PLAN,PLAN_DESC,VLR_CFM,TIPO_PLAN,PRODUCTO FROM DWH_DB.SEGMENTACION.INH_DIM_PLANES) J ON A.COD_PLAN_DESTINO=J.COD_PLAN\n",
    "LEFT JOIN (SELECT DS_TIPO_GRADE,SK_TIPO_GRADE FROM DWH_DB.CLIENTES.TBL_DIM_TIPO_GRADE_T0) L ON A.SK_TIPO_GRADE=L.SK_TIPO_GRADE\n",
    "LEFT JOIN (SELECT DS_RANGO_ANTIGUEDAD,SK_RANGO_ANTIGUEDAD FROM DWH_DB.CLIENTES.TBL_DIM_RANGO_ANTIGUEDAD_T1) M ON A.SK_RANGO_ANTIGUEDAD=M.SK_RANGO_ANTIGUEDAD\n",
    "LEFT JOIN (SELECT DS_GAMA_PLAN,SK_GAMA_PLAN FROM DWH_DB.CLIENTES.TBL_DIM_GAMA_PLAN_T1) N ON A.SK_GAMA_PLAN_ORIGEN=N.SK_GAMA_PLAN\n",
    "LEFT JOIN (SELECT DS_GAMA_PLAN,SK_GAMA_PLAN FROM DWH_DB.CLIENTES.TBL_DIM_GAMA_PLAN_T1) O ON A.SK_GAMA_PLAN_DESTINO=O.SK_GAMA_PLAN\n",
    "LEFT JOIN (SELECT CD_USUARIO_RED,DS_NOMBRE_USUARIO,DS_UBICACION_USUARIO,DS_CANAL_USUARIO,SK_USUARIO_UBICACION FROM DWH_DB.CLIENTES.TBL_DIM_USUARIO_UBICACION_T2) P ON A.SK_USUARIO_CREACION=P.SK_USUARIO_UBICACION\n",
    "LEFT JOIN (SELECT CD_USUARIO_RED,DS_NOMBRE_USUARIO,DS_UBICACION_USUARIO,DS_CANAL_USUARIO,SK_USUARIO_UBICACION FROM DWH_DB.CLIENTES.TBL_DIM_USUARIO_UBICACION_T2) Q ON A.SK_USUARIO_CIERRE=Q.SK_USUARIO_UBICACION\n",
    "LEFT JOIN (SELECT CO_ID, IDENTIFICACION, IDENTIFICACION_MTR FROM SEGMENTACION.INH_SEG_BSCS_CLIENTES ) T ON A.CO_ID =T.CO_ID\n",
    "LEFT JOIN (select BAN_PYME, COD_PLAN from DWH_DB.SEGMENTACION.INH_DIM_PLANES) PN on J.COD_PLAN = PN.cod_plan\n",
    "WHERE cast(A.FE_CREACION as DATE) >= '2024-04-15'\n",
    "and pn.ban_pyme = 'S'\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Ejecutando consulta: {consulta}\")  # Imprime la consulta para depuración\n",
    "        cursor = engine.cursor()\n",
    "        cursor.execute(consulta)\n",
    "        resultados = cursor.fetchall()\n",
    "\n",
    "        # Obtener los nombres de las columnas\n",
    "        columnas = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        # Crear el DataFrame con los resultados\n",
    "        df_cp = pd.DataFrame(resultados, columns=columnas)\n",
    "\n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if df_cp.empty:\n",
    "            print(\"No se encontraron resultados.\")\n",
    "        else:\n",
    "            df_cp.head(9)  # Mostrar las primeras 5 filas\n",
    "\n",
    "        return df_cp\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DHW')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consulta_Cp.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_Ventas(engine):\n",
    "    try:\n",
    "        # Consulta tabla DWH_DB.SEGMENTACION.INH_DIM_AGENTES\n",
    "        consulta = \"\"\"\n",
    "         SELECT\n",
    "            ('NEGOCIOS') AS Segmento,\n",
    "            R5.segmento AS FLAG,\n",
    "            M.CLASECAUSALINIC,\n",
    "            M.CO_ID,\n",
    "            M.TELE_NUMB,\n",
    "            M.PLAN,\n",
    "            M.PAQUETE,\n",
    "            M.TIPO,\n",
    "            M.AGENT_CODE,\n",
    "            translate(M.RAZON_INICIAL, 'áéíóú', 'aeiou') AS RAZON_INICIAL,\n",
    "            translate(M.RAZON_FINAL, 'áéíóú', 'aeiou') AS RAZON_FINAL,\n",
    "            M.FECHA_ULTEST,\n",
    "            M.ACTIVACION,\n",
    "            M.SSN,\n",
    "            M.TMCYSPC,\n",
    "            M.CIUDAD,\n",
    "            CAST(M.CFM AS INTEGER),\n",
    "            M.AGENT_NAME,\n",
    "            M.DOCUMENTO,\n",
    "            SUBSTRING(ciclo, POSITION(' - ' IN ciclo) + 2, 3) AS ciclo,\n",
    "            M.identificacion_venderdor,\n",
    "            M.CUSTCODE\n",
    "        FROM \n",
    "            DWH.BSCS_ACTIVACIONES M\n",
    "        JOIN \n",
    "            DWH_DB.VENTAS.TBL_VENT_SEG_INF_CONFIDENCIALES R5 ON M.SSN = R5.identificacion\n",
    "        WHERE \n",
    "            (M.RAZON_INICIAL LIKE '%32%' OR \n",
    "             M.RAZON_INICIAL LIKE '%57%' OR \n",
    "             M.RAZON_INICIAL LIKE '%267%' OR \n",
    "             M.RAZON_INICIAL LIKE '%345%' OR \n",
    "             M.RAZON_INICIAL LIKE '%362%' OR \n",
    "             M.RAZON_INICIAL LIKE '%118%' OR \n",
    "             M.RAZON_INICIAL LIKE '%285%' OR \n",
    "             M.RAZON_INICIAL LIKE '%500%' OR \n",
    "             M.RAZON_INICIAL LIKE '%202%')\n",
    "        \n",
    "        UNION\n",
    "\n",
    "        SELECT\n",
    "            ('NEGOCIOS') AS Segmento,\n",
    "            R5.segmento AS FLAG,\n",
    "            M.CLASECAUSALINIC,\n",
    "            M.CO_ID,\n",
    "            M.TELE_NUMB,\n",
    "            M.PLAN,\n",
    "            M.PAQUETE,\n",
    "            M.TIPO,\n",
    "            M.AGENT_CODE,\n",
    "            translate(M.RAZON_INICIAL, 'áéíóú', 'aeiou') AS RAZON_INICIAL,\n",
    "            translate(M.RAZON_FINAL, 'áéíóú', 'aeiou') AS RAZON_FINAL,\n",
    "            M.FECHA_ULTEST,\n",
    "            M.ACTIVACION,\n",
    "            M.SSN,\n",
    "            M.TMCYSPC,\n",
    "            M.CIUDAD,\n",
    "            CAST(M.CFM AS INTEGER),\n",
    "            M.AGENT_NAME,\n",
    "            M.DOCUMENTO,\n",
    "            SUBSTRING(ciclo, POSITION(' - ' IN ciclo) + 2, 3) AS ciclo,\n",
    "            M.identificacion_venderdor,\n",
    "            M.CUSTCODE\n",
    "        FROM \n",
    "            DWH.BSCS_ACTIVACIONES M\n",
    "        JOIN \n",
    "            DWH_DB.VENTAS.TBL_VENT_SEG_INF_CONFIDENCIALES R5 ON M.SSN = R5.identificacion\n",
    "        WHERE \n",
    "            (M.RAZON_INICIAL LIKE '%333%' OR \n",
    "             M.RAZON_INICIAL LIKE '%444%' OR \n",
    "             M.RAZON_INICIAL LIKE '%555%' OR \n",
    "             M.RAZON_INICIAL LIKE '%666%' OR \n",
    "             M.RAZON_INICIAL LIKE '%777%')\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Ejecutando consulta: {consulta}\")  # Imprime la consulta para depuración\n",
    "        cursor = engine.cursor()\n",
    "        cursor.execute(consulta)\n",
    "        resultados = cursor.fetchall()\n",
    "\n",
    "        # Obtener los nombres de las columnas\n",
    "        columnas = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        # Crear el DataFrame con los resultados\n",
    "        df_ventas = pd.DataFrame(resultados, columns=columnas)\n",
    "\n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if df_ventas.empty:\n",
    "            print(\"No se encontraron resultados.\")\n",
    "        else:\n",
    "            df_ventas.head(9)  # Mostrar las primeras 5 filas\n",
    "\n",
    "        return df_ventas\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DHW')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consulta_Ventas.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_Base_seg(engine):\n",
    "    try:\n",
    "        # Consulta tabla DWH_DB.SEGMENTACION.INH_DIM_AGENTES\n",
    "        consulta = \"\"\"\n",
    "         SELECT \n",
    "         x.identificacion  as CUST_ID,\n",
    "        ('') as Cod_plan,\n",
    "        x.segmento as Flag,\n",
    "        ('') as Usuario\n",
    "        FROM DWH_DB.VENTAS.TBL_VENT_SEG_INF_CONFIDENCIALES x\n",
    "        WHERE CB_ESACTUAL='1' \n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Ejecutando consulta: {consulta}\")  # Imprime la consulta para depuración\n",
    "        cursor = engine.cursor()\n",
    "        cursor.execute(consulta)\n",
    "        resultados = cursor.fetchall()\n",
    "\n",
    "        # Obtener los nombres de las columnas\n",
    "        columnas = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        # Crear el DataFrame con los resultados\n",
    "        df_base_seg = pd.DataFrame(resultados, columns=columnas)\n",
    "\n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if df_base_seg.empty:\n",
    "            print(\"No se encontraron resultados.\")\n",
    "        else:\n",
    "            df_base_seg.head(9)  # Mostrar las primeras 5 filas\n",
    "\n",
    "        return df_base_seg\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DHW')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consulta_Base_seg.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_Codigo_Ciudad(engine):\n",
    "    try:\n",
    "        # Consulta tabla DWH_DB.SEGMENTACION.INH_DIM_AGENTES\n",
    "        consulta = \"\"\"\n",
    "          SELECT x.* FROM clientes.tbl_dim_distribuidor_bscs_t1 x\n",
    "          where x.tipo_distribuidor in ('D','T','MAY','C')\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Ejecutando consulta: {consulta}\")  # Imprime la consulta para depuración\n",
    "        cursor = engine.cursor()\n",
    "        cursor.execute(consulta)\n",
    "        resultados = cursor.fetchall()\n",
    "\n",
    "        # Obtener los nombres de las columnas\n",
    "        columnas = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        # Crear el DataFrame con los resultados\n",
    "        df_codigo_ciudad = pd.DataFrame(resultados, columns=columnas)\n",
    "\n",
    "        # Verificar si el DataFrame está vacío\n",
    "        if df_codigo_ciudad.empty:\n",
    "            print(\"No se encontraron resultados.\")\n",
    "        else:\n",
    "            df_codigo_ciudad.head(9)  # Mostrar las primeras 5 filas\n",
    "\n",
    "        return df_codigo_ciudad\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DHW')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consulta_Codigo_Ciudad.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión a la base de datos Yellowbrick exitosa.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        configurarLogging()\n",
    "        #Variables constantes dentro del codigo para funciones\n",
    "        \n",
    "\n",
    "\n",
    "        id_ejecucion_dim_agente = str(uuid.uuid4()).upper()\n",
    "        id_ejecucion_plan_negocios = str(uuid.uuid4()).upper()\n",
    "        id_ejecucion_base_seg = str(uuid.uuid4()).upper()\n",
    "        id_ejecucion_ventas = str(uuid.uuid4()).upper()\n",
    "        id_ejecucion_codigo_ciudad = str(uuid.uuid4()).upper()\n",
    "        id_ejecucion_cambioplan = str(uuid.uuid4()).upper()\n",
    "        \n",
    "        fecha_inicio = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_inicio_tr = datetime.strptime(fecha_inicio, \"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin_tr = datetime.strptime(fecha_fin, \"%Y-%m-%d %H:%M:%S\")\n",
    "        id_estado = 1\n",
    "        estado = 1  # O el valor adecuado para el estado\n",
    "        duracion_proceso_timedelta = fecha_fin_tr - fecha_inicio_tr\n",
    "        duracion_proceso_seconds = duracion_proceso_timedelta.total_seconds()\n",
    "        \n",
    "        # Historicos de las tablas con la llave creada \n",
    "        engine = conexion_BD()\n",
    "        df_his_dim_agente = consultarHistoricoBD_Dim_agentes()\n",
    "        df_his_dim_agente['llaveDuplihis'] = df_his_dim_agente['cod_agente'].astype(str) + \\\n",
    "            df_his_dim_agente['cost_id_agente'].astype(str) + \\\n",
    "            df_his_dim_agente['custcode_agente'].astype(str) + \\\n",
    "            df_his_dim_agente['name_agente'].astype(str) + \\\n",
    "            df_his_dim_agente['categoria'].astype(str)\n",
    "\n",
    "\n",
    "        df_his_plan_negocios = consultarHistoricoBD_Plan_negocios()\n",
    "        df_his_plan_negocios['llaveDuplihis'] = df_his_plan_negocios['cod_plan'].astype(str) + \\\n",
    "            df_his_plan_negocios['vlr_cfm'].astype(str) + \\\n",
    "            df_his_plan_negocios['segm_cfm'].astype(str) + \\\n",
    "            df_his_plan_negocios['categoria'].astype(str)\n",
    "        \n",
    "        df_his_ventas = consultarHistoricoBD_Ventas()\n",
    "        df_his_ventas['llaveDuplihis'] = df_his_ventas['segmento'].astype(str) + \\\n",
    "            df_his_ventas['co_id'].astype(str) + \\\n",
    "            df_his_ventas['tele_numb'].astype(str) + \\\n",
    "            df_his_ventas['paquete'].astype(str) + \\\n",
    "            df_his_ventas['fecha_ultest'].astype(str) + \\\n",
    "            df_his_ventas['custcode'].astype(str)  \n",
    "        \n",
    "        df_his_base_seg = consultarHistoricoBD_Base_seg()\n",
    "\n",
    "        df_his_codigo_ciudad = consultarHistoricoBD_Codigo_Ciudad()\n",
    "        df_his_codigo_ciudad['llaveDuplihis'] = df_his_codigo_ciudad['sk_distribuidor'].astype(str) + \\\n",
    "            df_his_codigo_ciudad['cod_distribuidor'].astype(str) + \\\n",
    "            df_his_codigo_ciudad['fecha_creacion'].astype(str)\n",
    "        \n",
    "        df_his_cambioplan = consultarHistoricoBD_Cp()\n",
    "        df_his_cambioplan['llaveDuplihis'] = df_his_cambioplan['co_id'].astype(str) + \\\n",
    "            df_his_cambioplan['customer_id'].astype(str) + \\\n",
    "            df_his_cambioplan['tele_numb'].astype(str) + \\\n",
    "            df_his_cambioplan['cod_plan_origen'].astype(str) + \\\n",
    "            df_his_cambioplan['cod_plan_destino'].astype(str) + \\\n",
    "            df_his_cambioplan['fe_creacion'].astype(str) \n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        df_dim_agente = consulta_Dim_agentes(engine) # ya esta tb_datos_crudos_dwh_dim_agentes\n",
    "        df_dim_agente['id_ejecucion'] = id_ejecucion_dim_agente  # Agregar la columna id_ejecucion con el mismo UUID en todas las fila\n",
    "        df_dim_agente['id'] = [str(uuid.uuid4()) for _ in range(len(df_dim_agente))]  # Agregar la columna id con un UUID único por cada fila\n",
    "        df_dim_agente['fecha_procesamiento'] = fecha_inicio  # Agregar la columna fecha_procesamiento con la fecha y hora actual\n",
    "        df_dim_agente['id_estado'] = 1  # Crear el estado del registro como entero\n",
    "        df_dim_agente['id_estado_registro'] = 1\n",
    "\n",
    "        df_dim_agente['llaveDuplihis'] = df_dim_agente['cod_agente'].astype(str) + \\\n",
    "            df_dim_agente['cost_id_agente'].astype(str) + \\\n",
    "            df_dim_agente['custcode_agente'].astype(str) + \\\n",
    "            df_dim_agente['name_agente'].astype(str) + \\\n",
    "            df_dim_agente['categoria'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "        merge_df_dim_agente = pd.merge(df_dim_agente, df_his_dim_agente[['llaveDuplihis']], \n",
    "                    on=['llaveDuplihis'], \n",
    "                    how='left', \n",
    "                    indicator=True)\n",
    "        df_dim_agente_fin = merge_df_dim_agente[merge_df_dim_agente['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "        registros_dim_agent = len(df_dim_agente_fin)\n",
    "        df_dim_agente_fin.drop(columns=['llaveDuplihis'], inplace=True)\n",
    "\n",
    "        if registros_dim_agent > 0:\n",
    "           df_resumen = cargueResumen(\n",
    "        id_ejecucion_dim_agente, fecha_inicio_tr, fecha_fin_tr, duracion_proceso_seconds,\n",
    "        'Fuente DWH', registros_dim_agent, 'tb_datos_crudos_dwh_dim_agentes', id_estado\n",
    "        )\n",
    "        cargueDatosBD_Dim_agentes(df_dim_agente_fin)\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        df_plan_negocios = consulta_Plan_negocios(engine) # ya esta tb_datos_crudos_dwh_plan_negocios\n",
    "        df_plan_negocios['id_ejecucion'] = id_ejecucion_plan_negocios  # Agregar la columna id_ejecucion con el mismo UUID en todas las fila\n",
    "        df_plan_negocios['id'] = [str(uuid.uuid4()) for _ in range(len(df_plan_negocios))]  # Agregar la columna id con un UUID único por cada fila\n",
    "        df_plan_negocios['fecha_procesamiento'] = fecha_inicio  # Agregar la columna fecha_procesamiento con la fecha y hora actual\n",
    "        df_plan_negocios['id_estado'] = 1  # Crear el estado del registro como entero\n",
    "        df_plan_negocios['id_estado_registro'] = 1\n",
    "     \n",
    "        df_plan_negocios['llaveDuplihis'] = df_plan_negocios['cod_plan'].astype(str) + \\\n",
    "            df_plan_negocios['vlr_cfm'].astype(str) + \\\n",
    "            df_plan_negocios['segm_cfm'].astype(str) + \\\n",
    "            df_plan_negocios['categoria'].astype(str)\n",
    "        merge_df_plan_negocios = pd.merge(df_plan_negocios, df_his_plan_negocios[['llaveDuplihis']], \n",
    "                    on=['llaveDuplihis'], \n",
    "                    how='left', \n",
    "                    indicator=True)\n",
    "        df_plan_negocios_fin = merge_df_plan_negocios[merge_df_plan_negocios['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        \n",
    "        registros_plan_negocios = len(df_plan_negocios_fin)\n",
    "        df_plan_negocios_fin.drop(columns=['llaveDuplihis'], inplace=True)\n",
    "        \n",
    "        if registros_plan_negocios > 0:\n",
    "           df_resumen = cargueResumen(\n",
    "        id_ejecucion_plan_negocios, fecha_inicio_tr, fecha_fin_tr, duracion_proceso_seconds,\n",
    "        'Fuente DWH', registros_plan_negocios, 'tb_datos_crudos_dwh_plan_negocios', id_estado\n",
    "        )\n",
    "        cargueDatosBD_Plan_negocios(df_plan_negocios_fin)\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        df_ventas = consulta_Ventas(engine) # ya esta tb_datos_crudos_dwh_ventas\n",
    "        df_ventas['id_ejecucion'] = id_ejecucion_ventas  # Agregar la columna id_ejecucion con el mismo UUID en todas las fila\n",
    "        df_ventas['id'] = [str(uuid.uuid4()) for _ in range(len(df_ventas))]  # Agregar la columna id con un UUID único por cada fila\n",
    "        df_ventas['fecha_procesamiento'] = fecha_inicio  # Agregar la columna fecha_procesamiento con la fecha y hora actual\n",
    "        df_ventas['id_estado'] = 1  # Crear el estado del registro como entero\n",
    "        df_ventas['id_estado_registro'] = 1\n",
    "        df_ventas['llaveDuplihis'] = df_ventas['segmento'].astype(str) + \\\n",
    "            df_ventas['co_id'].astype(str) + \\\n",
    "            df_ventas['tele_numb'].astype(str) + \\\n",
    "            df_ventas['paquete'].astype(str) + \\\n",
    "            df_ventas['fecha_ultest'].astype(str) + \\\n",
    "            df_ventas['custcode'].astype(str) \n",
    "        \n",
    "        merge_df_ventas = pd.merge(df_ventas, df_his_ventas[['llaveDuplihis']], \n",
    "                    on=['llaveDuplihis'], \n",
    "                    how='left', \n",
    "                    indicator=True)\n",
    "        df_ventas_fin = merge_df_ventas[merge_df_ventas['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "        registros_ventas = len(df_ventas_fin)\n",
    "        df_ventas_fin.drop(columns=['llaveDuplihis'], inplace=True)\n",
    "        \n",
    "        if registros_ventas > 0:\n",
    "           df_resumen = cargueResumen(\n",
    "        id_ejecucion_ventas, fecha_inicio_tr, fecha_fin_tr, duracion_proceso_seconds,\n",
    "        'Fuente DWH', registros_ventas, 'tb_datos_crudos_dwh_ventas', id_estado\n",
    "        )\n",
    "        cargueDatosBD_Ventas(df_ventas_fin)\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        df_base_seg = consulta_Base_seg(engine) # ya esta tb_datos_crudos_dwh_base_seg\n",
    "        df_base_seg['id_ejecucion'] = id_ejecucion_base_seg  # Agregar la columna id_ejecucion con el mismo UUID en todas las fila\n",
    "        df_base_seg['id'] = [str(uuid.uuid4()) for _ in range(len(df_base_seg))]  # Agregar la columna id con un UUID único por cada fila\n",
    "        df_base_seg['fecha_procesamiento'] = fecha_inicio  # Agregar la columna fecha_procesamiento con la fecha y hora actual\n",
    "        df_base_seg['id_estado'] = 1  # Crear el estado del registro como entero\n",
    "        df_base_seg['id_estado_registro'] = 1\n",
    "        merge_df_base_seg = pd.merge(df_base_seg, df_his_base_seg[['cust_id']], \n",
    "                    on=['cust_id'], \n",
    "                    how='left', \n",
    "                    indicator=True)\n",
    "        df_base_seg_fin = merge_df_base_seg[merge_df_base_seg['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "        registros_base_seg = len(df_base_seg_fin)\n",
    "\n",
    "\n",
    "        if registros_base_seg > 0:\n",
    "           df_resumen = cargueResumen(\n",
    "        id_ejecucion_base_seg, fecha_inicio_tr, fecha_fin_tr, duracion_proceso_seconds,\n",
    "        'Fuente DWH', registros_base_seg, 'tb_datos_crudos_dwh_base_seg', id_estado\n",
    "        )\n",
    "        cargueDatos_Base_seg(df_base_seg_fin)\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        df_codigo_ciudad = consulta_Codigo_Ciudad(engine)# ya esta tb_datos_crudos_dwh_codigo_ciudad\n",
    "        df_codigo_ciudad['id_ejecucion'] = id_ejecucion_codigo_ciudad  # Agregar la columna id_ejecucion con el mismo UUID en todas las fila\n",
    "        df_codigo_ciudad['id'] = [str(uuid.uuid4()) for _ in range(len(df_codigo_ciudad))]  # Agregar la columna id con un UUID único por cada fila\n",
    "        df_codigo_ciudad['fecha_procesamiento'] = fecha_inicio  # Agregar la columna fecha_procesamiento con la fecha y hora actual\n",
    "        df_codigo_ciudad['id_estado'] = 1  # Crear el estado del registro como entero\n",
    "        df_codigo_ciudad['id_estado_registro'] = 1\n",
    "        df_codigo_ciudad['llaveDuplihis'] = df_codigo_ciudad['sk_distribuidor'].astype(str) + \\\n",
    "            df_codigo_ciudad['cod_distribuidor'].astype(str) + \\\n",
    "            df_codigo_ciudad['fecha_creacion'].astype(str)\n",
    "        \n",
    "        merge_df_codigo_ciudad = pd.merge(df_codigo_ciudad, df_his_codigo_ciudad[['llaveDuplihis']], \n",
    "                    on=['llaveDuplihis'], \n",
    "                    how='left', \n",
    "                    indicator=True)\n",
    "        df_codigo_ciudad_fin = merge_df_codigo_ciudad[merge_df_codigo_ciudad['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "        registros_codigo_ciudad = len(df_codigo_ciudad_fin)\n",
    "        df_codigo_ciudad_fin.drop(columns=['llaveDuplihis'], inplace=True)\n",
    "\n",
    "        if registros_codigo_ciudad > 0:\n",
    "           df_resumen = cargueResumen(\n",
    "        id_ejecucion_codigo_ciudad, fecha_inicio_tr, fecha_fin_tr, duracion_proceso_seconds,\n",
    "        'Fuente DWH', registros_codigo_ciudad, 'tb_datos_crudos_dwh_codigo_ciudad', id_estado\n",
    "        )\n",
    "        cargueDatos_Codigo_Ciudad(df_codigo_ciudad_fin)\n",
    "        \n",
    "\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        df_cambioplan = consulta_Cp(engine)# ya esta tb_datos_crudos_dwh_cambio_plan\n",
    "        df_cambioplan['id_ejecucion'] = id_ejecucion_cambioplan  # Agregar la columna id_ejecucion con el mismo UUID en todas las fila\n",
    "        df_cambioplan['id'] = [str(uuid.uuid4()) for _ in range(len(df_cambioplan))]  # Agregar la columna id con un UUID único por cada fila\n",
    "        df_cambioplan['fecha_procesamiento'] = fecha_inicio  # Agregar la columna fecha_procesamiento con la fecha y hora actual\n",
    "        df_cambioplan['id_estado'] = 1  # Crear el estado del registro como entero\n",
    "        df_cambioplan['id_estado_registro'] = 1\n",
    "        df_cambioplan['llaveDuplihis'] = df_cambioplan['co_id'].astype(str) + \\\n",
    "            df_cambioplan['customer_id'].astype(str) + \\\n",
    "            df_cambioplan['tele_numb'].astype(str) + \\\n",
    "            df_cambioplan['cod_plan_origen'].astype(str) + \\\n",
    "            df_cambioplan['cod_plan_destino'].astype(str) + \\\n",
    "            df_cambioplan['fe_creacion'].astype(str) \n",
    "        \n",
    "        merge_df_cambioplan = pd.merge(df_cambioplan, df_his_cambioplan[['llaveDuplihis']], \n",
    "                    on=['llaveDuplihis'], \n",
    "                    how='left', \n",
    "                    indicator=True)\n",
    "        df_cambioplan_fin = merge_df_cambioplan[merge_df_cambioplan['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        \n",
    "        registros_cambioplan = len(df_cambioplan_fin)\n",
    "\n",
    "        df_cambioplan_fin.drop(columns=['llaveDuplihis'], inplace=True)\n",
    "        \n",
    "        if registros_cambioplan > 0:\n",
    "           df_resumen = cargueResumen(\n",
    "        id_ejecucion_cambioplan, fecha_inicio_tr, fecha_fin_tr, duracion_proceso_seconds,\n",
    "        'Fuente DWH', registros_cambioplan, 'tb_datos_crudos_dwh_cambio_plan', id_estado\n",
    "        )\n",
    "        cargueDatosBD_Cp(df_cambioplan_fin)\n",
    "  \n",
    "\n",
    "\n",
    "        cantidad_registros.append(registros_base_seg+registros_cambioplan+registros_codigo_ciudad+registros_dim_agent+registros_ventas+registros_plan_negocios)\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Fuentes DWH')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(\"__main__\")\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#pd.set_option('display.max_columns', None) \n",
    "#pd.set_option('display.max_rows', None)  \n",
    "#pd.set_option('display.width', None)  \n",
    "#pd.set_option('display.max_colwidth', None) \n",
    "\n",
    "#print(df_cambioplan.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-empresas-negocios-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
