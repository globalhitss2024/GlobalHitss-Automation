{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "        driver_path = \"C:\\\\ambiente_desarrollo\\\\dev-empresas-negocios-env-web-scraping\\\\edgedriver_win64\\\\msedgedriver.exe\"\n",
    "        url_login = \"https://avanza.claro.com.co/#/signin\"\n",
    "        usuario = \"38501867\"\n",
    "        contrasena = \"D4vidp_25*\"\n",
    "        download_dir = \"C:\\\\Users\\\\AMD_INTCOM\\\\Downloads\"\n",
    "        final_dir = r\"C:\\\\ambiente_desarrollo\\\\dev-empresas-negocios-env-web-scraping\\\\fuentes\"\n",
    "        final_file_name = \"Base_Avanza.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import sys\n",
    "sys.path.append('C:\\\\ambiente_desarrollo\\\\dev-empresas-negocios-env-web-scraping\\\\desarrollo_produccion')\n",
    "import parametros_produccion as par\n",
    "import time\n",
    "import os\n",
    "import shutil  # Para mover el archivo descargado\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import psycopg2\n",
    "import logging\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIABLES GLOBALES\n",
    "fecha_actual = datetime.today().date()\n",
    "duracion = []\n",
    "fuentes = []\n",
    "cantidad_registros = []\n",
    "estado = []\n",
    "fecha_fin_procesamiento =[]\n",
    "funcion_error = []\n",
    "descripcion_error = []\n",
    "id_ejecucion = str(uuid.uuid4())  # Generar UUID de ejecución\n",
    "destino = 'Web Scraping Avanza'\n",
    "id_estado = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def salidaLogMonitoreo():\n",
    "    \n",
    "    Este metodo captura la informacion que se desea imprimir en el Log\n",
    "    para monitoreo y funcionamiento del desarrollo\n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        None\n",
    "    \n",
    "    Fecha_fin = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    print(f\"Fecha_inicio: {fecha_inicio}\")\n",
    "    print(f\"Fecha_fin: {Fecha_fin}\")\n",
    "    print(f\"Duracion: {duracion}\")\n",
    "    print(f\"Fuentes: {fuentes}\")\n",
    "    print(f\"Cantidad_registros: {cantidad_registros}\")\n",
    "    print(f\"Destino: {destino}\")\n",
    "    print(f\"Estado: {estado}\")\n",
    "    print(\"Lugar errores: \", ' | '.join(map(str, funcion_error)))\n",
    "    print(\"Descripción errores: \", ' | '.join(map(str, descripcion_error)))\n",
    "    if estado[0] == 1 :\n",
    "        print(\"Ejecución exitosa\")\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "\n",
    "\"\"\"\n",
    "def salidaLogMonitoreo():\n",
    "    \"\"\"\n",
    "    Este método captura la información que se desea imprimir en el Log\n",
    "    para monitoreo y funcionamiento del desarrollo.\n",
    "    \"\"\"\n",
    "    Fecha_fin = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    logging.info(f\"Fecha_inicio: {fecha_inicio}\")\n",
    "    logging.info(f\"Fecha_fin: {Fecha_fin}\")\n",
    "    logging.info(f\"Duracion: {duracion}\")\n",
    "    logging.info(f\"Fuentes: {fuentes}\")\n",
    "    logging.info(f\"Cantidad_registros: {cantidad_registros}\")\n",
    "    logging.info(f\"Destino: {destino}\")\n",
    "    logging.info(f\"Estado: {estado}\")\n",
    "    logging.info(\"Lugar errores: \" + ' | '.join(map(str, funcion_error)))\n",
    "    logging.info(\"Descripción errores: \" + ' | '.join(map(str, descripcion_error)))\n",
    "    if estado[0] == 1:\n",
    "        logging.info(\"Ejecución exitosa\")\n",
    "    logging.info(\"------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion de cargue de resumen de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar resumen de datos en la BD\n",
    "def cargueResumen(id_ejecucion, fecha_inicio_date,fecha_fin_procesamiento, duracion,fuentes, cantidad_registros, destino, id_estado):\n",
    "    try:\n",
    "        df_resumen_cargue = pd.DataFrame({\n",
    "        'id_ejecucion': [id_ejecucion],  # Envolver en una lista\n",
    "        'fecha_inicio_procesamiento': [fecha_inicio_date],\n",
    "        'fecha_fin_procesamiento': [fecha_fin_procesamiento], \n",
    "        'duracion_segundos': [duracion],\n",
    "        'fuentes': [fuentes],\n",
    "        'cantidad_registros': [cantidad_registros],\n",
    "        'destino': [destino],\n",
    "        'id_estado': [id_estado],\n",
    "    })\n",
    "        Usuario_pro = 'postgres'\n",
    "        contraseña_pro = '1Nt3l163nC14_C0m3rc14L'\n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'control_procesamiento'\n",
    "        nombre_tabla = 'tb_resumen_cargue'\n",
    "        \n",
    "        df_resumen_cargue.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Web Scraping Avanza')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueResumen.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion que realiza el cargue a BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargueDatosBD(df_final):\n",
    "    \"\"\"\n",
    "    Función que se encarga de cargar los dataframes procesados hacia la base de datos\n",
    "    \n",
    "    Argumentos:\n",
    "        df_final: Contiene el dataframe que se requiere cargar a la BD\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        conexion = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'fuentes_cruda'\n",
    "        nombre_tabla = 'tb_datos_crudos_avanza_webscraping'\n",
    "        \n",
    "        df_final.to_sql(nombre_tabla, con=conexion, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Web Scraping Avanza')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(cargueDatosBD.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "    finally:\n",
    "        conexion.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertarErroresDB():\n",
    "    \"\"\"\n",
    "    Metodo para insertar a POSTGRESQL los errores capturados durante la ejecución\n",
    "    Argumentos Globales:\n",
    "        fecha_inicio: Captura la fecha en que inicio la ejecución\n",
    "        fecha_fin: Captura la fecha en que finalizo la ejecución\n",
    "        duracion: Duración del procesamiento\n",
    "        fuente: Indica la fuente de donde provienen los datos\n",
    "        cantidad_registros: Cantidad de registros por fuente\n",
    "        destino: Indica la tabla a donde se estan ingestando los datos\n",
    "        id_estado: Indica el estado del proceso definidos en la base de datos \n",
    "        funcion_error: Indica la función donde se esta presentando una falla\n",
    "        descripcion_error: Descripción del error generado\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        SQLAlchemyError as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convertir las cadenas de texto a objetos datetime\n",
    "        fecha_inicio_tr = datetime.strptime(fecha_inicio, \"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin_tr = datetime.strptime(fecha_fin, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        duracion_proceso_timedelta = fecha_fin_tr - fecha_inicio_tr\n",
    "        duracion_proceso_seconds = duracion_proceso_timedelta.total_seconds()\n",
    "        \n",
    "        errores = pd.DataFrame({\n",
    "            'fecha_inicio': fecha_inicio,\n",
    "            'fecha_fin': fecha_fin,\n",
    "            'duracion': duracion_proceso_seconds,\n",
    "            'fuente': fuentes,\n",
    "            'cantidad_registros': cantidad_registros,\n",
    "            'destino': destino,\n",
    "            'id_estado': estado,\n",
    "            'funcion_error': funcion_error,\n",
    "            'descripcion_error': descripcion_error\n",
    "        })\n",
    "        \n",
    "        conexion_errores = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        # Especificar el esquema y la tabla en la que deseas insertar los datos\n",
    "        nombre_esquema = 'control_procesamiento'\n",
    "        nombre_tabla = 'tb_errores_cargue'\n",
    "        errores.to_sql(nombre_tabla, con=conexion_errores, schema=nombre_esquema, if_exists='append', index=False)\n",
    "        cargueResumen(id_ejecucion_en_curso, fecha_inicio_tr,2) \n",
    "        salidaLogMonitoreo()\n",
    "\n",
    "    \n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Web Scraping Avanza')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(insertarErroresDB.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexion a BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conexion_BD():\n",
    "    \"\"\"\n",
    "    Función que genera la conexión hacia la base de datos por medio de la libreria psycopg2\n",
    "    \n",
    "    Argumentos:\n",
    "        id_ejecucion: id del proceso ejecutado\n",
    "        fecha_fin_date: Fecha fin de procesamiento\n",
    "        duracion_proceso_seg: Duración en segundos del procesamiento\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        engine = create_engine(f'postgresql://{par.usuario}:{par.contrasena}@{par.host}:{par.port}/{par.bd_inteligencia_comercial_produccion}')\n",
    "        return engine\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        fuentes.append('Web Scraping Avanza')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(conexion_BD.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultarDatosParametrosDesarrollo():\n",
    "   \n",
    "    \"\"\"\n",
    "    Metodo que se encarga de consultar cada una de las tablas referente a los pesos de las vacantes\n",
    "    asignados desde el Micrositio\n",
    " \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna:\n",
    "        Tupla de Dataframes consultados\n",
    " \n",
    "    Excepciones manejadas:\n",
    "        Exception: Registro de control de errores que se presenten para cargue en log de stderr y tabla SQL ErroresProceso\n",
    "    \"\"\"\n",
    "    try:\n",
    "       \n",
    "        ConsultarParametrosDesarrollo = \"SELECT pd.ParametroId, pd.Nombre, pd.Descripcion, pd.Tipo, pd.Tabla, pd.Valor, \\\n",
    "                                        pd.RangoInicial, pd.RangoFinal, pd.Proceso, pd.EstadoRegistroId, pd.Unidad, pd.Agrupacion, \\\n",
    "                                        FORMAT(pd.FechaModificacion, 'yyyy-MM-dd HH:mm:ss.fff K') AS FechaModificacion \\\n",
    "                                        FROM db_motor_ia.ParametrosDesarrollo pd\" #0\n",
    "        dfParametrosDesarrollo = lecturaDatosAzureSQLMotor(ConsultarParametrosDesarrollo)\n",
    " \n",
    "        return dfParametrosDesarrollo\n",
    " \n",
    "    except Exception as error:\n",
    "        error_message = str(error)[:300]\n",
    "        nombre_metodo_error = inspect.currentframe().f_code.co_name\n",
    "        logging.error(f\"Se capturó una excepción: Nombre metodo: {nombre_metodo_error}, Proceso {proceso} Error capturado: {error_message}\")\n",
    "        logging.error(\"Mensaje de error: \",error_message)\n",
    "        salidaLogMonitoreo(proceso, nombre_metodo_error, error_message)\n",
    " \n",
    " \n",
    "#dfParametrosDesarrollo = consultarDatosParametrosDesarrollo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta de historico de datos Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultarHistoricoWebScrapingAvanza():\n",
    "    \"\"\"\n",
    "    Función que consulta los datos historicos existentes en la base de datos de la tabla de tb_datos_crudos_legalizadas\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        df_avanza_historico : Retorna el historico de los datos cargados en la BD\n",
    "    Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        engine = conexion_BD()\n",
    "        sql_consulta = \"Select * \\\n",
    "                    from fuentes_cruda.tb_datos_crudos_avanza_webscraping\"\n",
    "        df_avanza_historico = pd.read_sql(sql_consulta, engine)\n",
    "        df_avanza_historico = df_avanza_historico.drop_duplicates(subset=['id','fecha'])\n",
    "    \n",
    "        return df_avanza_historico\n",
    "        \n",
    "    except Exception as e:\n",
    "        fuentes.append('Web Scraping Avanza')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(consultarHistoricoWebScrapingAvanza.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurarLogging():\n",
    "    \"\"\"\n",
    "    Configura el logging para escribir en un archivo y en la salida estándar\n",
    "    Utiliza la ruta definida en par.ruta_log para el directorio de logs.\n",
    "    \n",
    "    Argumentos:\n",
    "        None\n",
    "    Retorna: \n",
    "        None\n",
    "    Excepciones manejadas: \n",
    "        None\n",
    "    \"\"\"\n",
    "    # Configuración del logging\n",
    "    log_directory = par.ruta_log_produccion\n",
    "    log_file = os.path.join(log_directory, \"cargue_webscraping_avanza.log\")\n",
    "\n",
    "    # Crear el directorio si no existe\n",
    "    if not os.path.exists(log_directory):\n",
    "        os.makedirs(log_directory)\n",
    "\n",
    "    # Configurar el logger\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file, mode='a'),  # 'a' para modo append\n",
    "            #logging.StreamHandler()  # Para imprimir en pantalla\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creacion de direccionamiento del driver y objeto para uso de driver, Creacion de espera para ir a Pagina web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navegar_Rep_Avanza(driver_path, url_login, usuario, contrasena):\n",
    "    \"\"\"\n",
    "    Función que realiza el inicio de sesión y la navegación hacia el reporte en la plataforma Avanza.\n",
    "\n",
    "    Argumentos:\n",
    "        driver_path: Ruta completa al Microsoft Edge WebDriver.\n",
    "        url_login: URL de la página de inicio de sesión.\n",
    "        usuario: Nombre de usuario para iniciar sesión.\n",
    "        contrasena: Contraseña para iniciar sesión.\n",
    "    \n",
    "    Retorna:\n",
    "        driver: Objeto del navegador después de haber navegado al reporte.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Inicializa el WebDriver con el servicio\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Edge(service=service)\n",
    "\n",
    "        # Crea un objeto WebDriverWait\n",
    "        wait = WebDriverWait(driver, 30)\n",
    "\n",
    "        # Navega a la página de inicio de sesión\n",
    "        driver.get(url_login)\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Realiza el inicio de sesiónº\n",
    "        usuario_input = driver.find_element(By.NAME, \"username\")\n",
    "        contrasena_input = driver.find_element(By.NAME, \"password\")\n",
    "        usuario_input.send_keys(usuario)\n",
    "        contrasena_input.send_keys(contrasena)\n",
    "\n",
    "        # Encuentra y haz clic en el botón de inicio de sesión\n",
    "        boton_logeo = driver.find_element(By.XPATH, \"//button[@type='submit']\")\n",
    "        boton_logeo.click()\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Selecciona el elemento de la lista desplegable\n",
    "        lista = wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@data-itemvalue='AVANZA']\")))\n",
    "        lista.click()\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Haz clic en el enlace \"Ver todo\"\n",
    "        ver_todo_link = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[@data-tb-test-id='channel-see-all-link']\")))\n",
    "        ver_todo_link.click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Haz clic en el enlace del dashboard\n",
    "        link_dash = wait.until(EC.element_to_be_clickable((By.XPATH, \"//a[@href='/#/site/AVANZA/redirect_to_view/21434']\")))\n",
    "        link_dash.click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Navega directamente al reporte detallado\n",
    "        driver.get(\"https://avanza.claro.com.co/t/AVANZA/views/DetallereporteCuotas/Comportamiento_Diario?%3Aembed=y&%3AshowVizHome=n&%3Atoolbar=top&%3AopenAuthoringInTopWindow=true&%3AbrowserBackButtonUndo=true&%3AcommentingEnabled=true&%3AreloadOnCustomViewSave=true&%3AshowAppBanner=false&%3AisVizPortal=true&%3AapiID=host0#navType=0&navSrc=Opt&1\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        return driver  # Retorna el driver para seguir interactuando si es necesario\n",
    "    \n",
    "    except Exception as e:\n",
    "            fuentes.append('Web Scraping Avanza')\n",
    "            cantidad_registros.append(0)\n",
    "            estado.append(2)\n",
    "            funcion_error.append(navegar_Rep_Avanza.__name__)\n",
    "            descripcion_error.append(str(e)[:100])\n",
    "            insertarErroresDB()\n",
    "            salidaLogMonitoreo()\n",
    "            if 'driver' in locals():\n",
    "              driver.quit()\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descargar_Rep_Avanza(driver, download_dir, final_dir, final_file_name, timeout=300):\n",
    "    \"\"\"\n",
    "      Función que realizara los filtros finales y la descarga de los datos para el uso de DF de Avanza\n",
    "        \n",
    "      Argumentos:\n",
    "        driver: objeto de la anterior funcion para ejecucion.\n",
    "        download_dir: lugar de descarga del archivo.\n",
    "        final_dir: lugar final de la descarga del archivo paso a otra carpeta.\n",
    "        final_file_name: Nombre final que se asignara al archivo.\n",
    "\n",
    "      Retorna: \n",
    "        new_file_path: objeto que se usara para la extraccion del DF del Web Scraping\n",
    "      Excepciones manejadas: \n",
    "        Exception as e: Captura el error en caso de que no se puedan insertar los datos en BD y genera un log localmente\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        wait = WebDriverWait(driver, 30)\n",
    "\n",
    "        # Reliza los filtros en la visual para la descarga\n",
    "        filtra_visual = wait.until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \".tab-vizAxisWrapper\"))\n",
    "        )\n",
    "        filtra_visual.click()\n",
    "\n",
    "        boton_descarga = wait.until(\n",
    "            EC.element_to_be_clickable((By.ID, \"download\"))\n",
    "        )\n",
    "        boton_descarga.click()\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Seleccionar la opción \"Datos\"\n",
    "        seleccion = wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//span[text()='Datos']\"))\n",
    "        )\n",
    "        seleccion.click()\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Cambiar a la nueva ventana\n",
    "        ventana_original = driver.current_window_handle\n",
    "        ventana_nueva = None\n",
    "\n",
    "        for handle in driver.window_handles:\n",
    "            if handle != ventana_original:\n",
    "                ventana_nueva = handle\n",
    "                break\n",
    "\n",
    "        if ventana_nueva:\n",
    "            driver.switch_to.window(ventana_nueva)\n",
    "\n",
    "            # Selección de datos completos y filtros para descarga\n",
    "            datos_completos = wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//div[@aria-label='Datos completos tabla']\"))\n",
    "            )\n",
    "            datos_completos.click()\n",
    "\n",
    "            mostrar_campos_btn = wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[@class='f10m8oqj icon' and @aria-label='Mostrar campos']\"))\n",
    "            )\n",
    "            mostrar_campos_btn.click()\n",
    "            time.sleep(15)\n",
    "\n",
    "            campo_todo = wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//div[@title='(Todo)']\"))\n",
    "            )\n",
    "            campo_todo.click()\n",
    "            time.sleep(10)\n",
    "\n",
    "            descargar_btn = wait.until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[span[text()='Descargar']]\"))\n",
    "            )\n",
    "            descargar_btn.click()\n",
    "            time.sleep(15)\n",
    "\n",
    "            # Volver a la ventana original\n",
    "            driver.switch_to.window(ventana_original)\n",
    "\n",
    "        # Configurar la descarga del archivo\n",
    "        time.sleep(180)  # Esperar 3 minutos para la descarga\n",
    "        initial_files = {f: os.path.getmtime(os.path.join(download_dir, f)) for f in os.listdir(download_dir)}\n",
    "\n",
    "        elapsed_time = 0\n",
    "        downloaded_file = None\n",
    "\n",
    "        while elapsed_time < timeout:\n",
    "            current_files = os.listdir(download_dir)\n",
    "            new_files = [f for f in current_files if f.startswith(\"C_Diario\") and \"_Datos completos_data\" in f and f.endswith(\".csv\")]\n",
    "\n",
    "            if new_files:\n",
    "                downloaded_file = max(new_files, key=lambda x: os.path.getmtime(os.path.join(download_dir, x)))\n",
    "                break\n",
    "\n",
    "            time.sleep(2)\n",
    "            elapsed_time += 2\n",
    "\n",
    "        if downloaded_file:\n",
    "            downloaded_file_path = os.path.join(download_dir, downloaded_file)\n",
    "            new_file_path = os.path.join(final_dir, final_file_name)\n",
    "\n",
    "            if os.path.exists(new_file_path):\n",
    "                os.remove(new_file_path)  # Reemplazar archivo existente\n",
    "\n",
    "            shutil.move(downloaded_file_path, new_file_path)\n",
    "            print(f\"Archivo movido y renombrado a: {new_file_path}\")\n",
    "            return new_file_path  # Retornar el path del archivo final\n",
    "        else:\n",
    "            print(f\"No se encontró ningún archivo 'C_Diario' en el tiempo establecido.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "            fuentes.append('Web Scraping Avanza')\n",
    "            cantidad_registros.append(0)\n",
    "            estado.append(2)\n",
    "            funcion_error.append(descargar_Rep_Avanza.__name__)\n",
    "            descripcion_error.append(str(e)[:100])\n",
    "            insertarErroresDB()\n",
    "            salidaLogMonitoreo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificacion de Data Frame, cambio de tipo de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrearDf(id_ejecucion, new_file_path, fecha_inicio): \n",
    "\n",
    "    \"\"\"\n",
    "    Función que crea el DataFrame principal y realiza la limpieza de los datos para el cargue.\n",
    "\n",
    "    Argumentos:\n",
    "        id_ejecucion: ID de ejecución del proceso.\n",
    "        new_file_path: Ruta del archivo CSV generado por la función de scraping.\n",
    "        fecha_inicio: Fecha de inicio del procesamiento.\n",
    "\n",
    "    Retorna: \n",
    "        Df_Filtrado: DataFrame con los datos organizados para el cargue.\n",
    "    \"\"\"\n",
    "            \n",
    "     # Borrar el DataFrame df si existe\n",
    "     #if 'df' in locals():\n",
    "     #   print(\"Eliminando el DataFrame anterior.\")\n",
    "     #   del df  # Eliminar el DataFrame df si existe\n",
    "    \n",
    "    # Inicialización de variables\n",
    "    df = None\n",
    "    \n",
    "    try:\n",
    "        # Cargar el archivo CSV en un DataFrame\n",
    "        df = pd.read_csv(new_file_path, delimiter=';', on_bad_lines='skip')\n",
    "\n",
    "        # Agregar columnas adicionales\n",
    "        df['id_ejecucion'] = id_ejecucion\n",
    "        df['id'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "        df['fecha_procesamiento'] = fecha_inicio\n",
    "        df['id_estado'] = 1\n",
    "        df['id_estado_registro'] = 1\n",
    "        df.rename(columns={'Mes de FECHA': 'mes_de_fecha'}, inplace=True)\n",
    "        df.columns = [col.lower() for col in df.columns]  # Convertir nombres de columnas a minúsculas\n",
    "\n",
    "        # Definir las columnas y tipos deseados\n",
    "        columnas_ordenadas = [\n",
    "            'id', 'id_ejecucion', 'tipo', 'mes_de_fecha', 'fechag', 'segmento', 'td', 'vr_neto', 'agent_code', 'canal',\n",
    "            'cant', 'cfm_actual', 'cfm_anterior', 'ciclo', 'co_id', 'cod_plan_actual', 'cod_plan_anterior', 'custcode_mtr',\n",
    "            'dia_ev', 'documento2', 'fecha', 'fecha2', 'id_vendedor', 'identif_mtr', 'misisdn', 'plan_actual',\n",
    "            'plan_anterior', 'rango_cfm', 'razon', 'region', 'region_ant', 'segmento_b', 'ssn', 'subcanal', 'tipob',\n",
    "            'user_creacion', 'fecha_procesamiento', 'id_estado', 'id_estado_registro'\n",
    "        ]\n",
    "\n",
    "        tipos_datos = {\n",
    "            'id': 'string',\n",
    "            'id_ejecucion': 'string',\n",
    "            'tipo': 'string',\n",
    "            'mes_de_fecha': 'string',\n",
    "            'fechag': 'Int64',\n",
    "            'segmento': 'string',\n",
    "            'td': 'string',\n",
    "            'vr_neto': 'string',\n",
    "            'agent_code': 'string',\n",
    "            'canal': 'string',\n",
    "            'cant': 'Int64',\n",
    "            'cfm_actual': 'string',\n",
    "            'cfm_anterior': 'string',\n",
    "            'ciclo': 'Int64',\n",
    "            'co_id': 'Int64',\n",
    "            'cod_plan_actual': 'string',\n",
    "            'cod_plan_anterior': 'string',\n",
    "            'custcode_mtr': 'float',\n",
    "            'dia_ev': 'Int64',\n",
    "            'documento2': 'string',\n",
    "            'fecha': 'string',\n",
    "            'fecha2': 'Int64',\n",
    "            'id_vendedor': 'string',\n",
    "            'identif_mtr': 'Int64',\n",
    "            'misisdn': 'Int64',\n",
    "            'plan_actual': 'string',\n",
    "            'plan_anterior': 'string',\n",
    "            'rango_cfm': 'string',\n",
    "            'razon': 'string',\n",
    "            'region': 'string',\n",
    "            'region_ant': 'float',\n",
    "            'segmento_b': 'string',\n",
    "            'ssn': 'Int64',\n",
    "            'subcanal': 'string',\n",
    "            'tipob': 'string',\n",
    "            'user_creacion': 'string',\n",
    "            'fecha_procesamiento': 'datetime64[ns]',\n",
    "            'id_estado': 'Int64',\n",
    "            'id_estado_registro': 'Int64'\n",
    "        }\n",
    "\n",
    "        # Limpiar valores no numéricos o NaN antes de convertir\n",
    "        for col, dtype in tipos_datos.items():\n",
    "            if dtype in ['Int64', 'float'] and col in df.columns:\n",
    "                # Intentar convertir a numérico, reemplazar errores por NaN\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "                # Reemplazar NaN con un valor predeterminado\n",
    "                if dtype == 'Int64':\n",
    "                    df[col] = df[col].fillna(0).astype('int64')  # Convertir a int64 directamente\n",
    "\n",
    "        # Aplicar tipos a columnas presentes en el DataFrame\n",
    "        tipos_datos_presentes = {col: dtype for col, dtype in tipos_datos.items() if col in df.columns}\n",
    "        df = df.astype(tipos_datos_presentes)\n",
    "\n",
    "        # Reorganizar columnas según el orden deseado\n",
    "        df = df[[col for col in columnas_ordenadas if col in df.columns]]\n",
    "\n",
    "        # Limitar el número de filas, si es necesario\n",
    "        df_filtrado = df\n",
    "\n",
    "        return df_filtrado\n",
    "\n",
    "   \n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Web Scraping Avanza')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(CrearDf.__name__)\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()\n",
    "        salidaLogMonitoreo()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecucion de funciones Cargue a BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo movido y renombrado a: C:\\\\ambiente_desarrollo\\\\dev-empresas-negocios-env-web-scraping\\\\fuentes\\Base_Avanza.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMD_INTCOM\\AppData\\Local\\Temp\\ipykernel_11032\\2106886455.py:25: DtypeWarning: Columns (4,10,21,22,31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(new_file_path, delimiter=';', on_bad_lines='skip')\n",
      "C:\\Users\\AMD_INTCOM\\AppData\\Local\\Temp\\ipykernel_11032\\3753904483.py:29: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df_dataframe_avanza['fecha'] = pd.to_datetime(df_dataframe_avanza['fecha'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        configurarLogging()\n",
    "        #Variables constantes dentro del codigo para funciones\n",
    "        \n",
    "\n",
    "        id_ejecucion = str(uuid.uuid4()).upper()  # Generar ID de ejecución\n",
    "        fecha_inicio = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_inicio_tr = datetime.strptime(fecha_inicio, \"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        fecha_fin_tr = datetime.strptime(fecha_fin, \"%Y-%m-%d %H:%M:%S\")\n",
    "        id_estado = 1\n",
    "        estado = 1  # O el valor adecuado para el estado\n",
    "        duracion_proceso_timedelta = fecha_fin_tr - fecha_inicio_tr\n",
    "        duracion_proceso_seconds = duracion_proceso_timedelta.total_seconds()\n",
    "\n",
    "\n",
    "\n",
    "        ## Ejecucion primer funcion de Web Scraping logeo y busqueda de datos\n",
    "        driver = navegar_Rep_Avanza(driver_path, url_login, usuario, contrasena)\n",
    "\n",
    "        # Ejecucion segunda parte funcion de web scraping, filtro y descarga de los datos\n",
    "        new_file_path = descargar_Rep_Avanza(driver, download_dir, final_dir, final_file_name)\n",
    "        \n",
    "        #Ejecucuion tercera parte del web scraping e inicio ETL, organizacion de la informacion y limpieza\n",
    "        df_dataframe_avanza = CrearDf(id_ejecucion, new_file_path, fecha_inicio)\n",
    "        df_avanza_historico = consultarHistoricoWebScrapingAvanza()\n",
    "         # Convierte la columna 'fecha' a tipo datetime en ambos DataFrames\n",
    "        df_dataframe_avanza['fecha'] = pd.to_datetime(df_dataframe_avanza['fecha'], errors='coerce')\n",
    "        df_avanza_historico['fecha'] = pd.to_datetime(df_avanza_historico['fecha'], errors='coerce')\n",
    "        # Realizar un merge para conservar solo los registros de df_dataframe_avanza que no están en df_avanza_historico\n",
    "        #df_merge = df_dataframe_avanza.merge(df_avanza_historico[['fecha']], on='fecha', how='left', indicator=True)\n",
    "        #df_resultante = df_merge[df_merge['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "        # Crear una columna que indique si la fecha existe en el DataFrame histórico\n",
    "        df_dataframe_avanza['existe_en_historico'] = df_dataframe_avanza['fecha'].isin(df_avanza_historico['fecha']).astype(int)\n",
    "\n",
    "        # Filtrar solo las filas donde la fecha no está en el histórico\n",
    "        df_resultante = df_dataframe_avanza[df_dataframe_avanza['existe_en_historico'] == 0].copy()\n",
    "\n",
    "        # Elimina la columna auxiliar si ya no es necesaria\n",
    "        df_resultante.drop(columns=['existe_en_historico'], inplace=True)\n",
    "\n",
    "        registros = len(df_resultante)\n",
    "        cantidad_registros.append(registros)\n",
    "\n",
    "        # Ejecucion cargue de datos ETL, se carga la funcion de CargueDatosBD, insercion a BD\n",
    "        if registros > 0:\n",
    "           df_resumen = cargueResumen(\n",
    "        id_ejecucion, fecha_inicio_tr, fecha_fin_tr, duracion_proceso_seconds,\n",
    "        'Web Scraping Avanza', registros, 'tb_datos_crudos_avanza_webscraping', id_estado\n",
    "        )\n",
    "        cargueDatosBD(df_resultante)\n",
    "\n",
    "    except Exception as e:\n",
    "        fuentes.append('Web Scraping Avanza')\n",
    "        cantidad_registros.append(0)\n",
    "        estado.append(2)\n",
    "        funcion_error.append(\"__main__\")\n",
    "        descripcion_error.append(str(e)[:100])\n",
    "        insertarErroresDB()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-empresas-negocios-env-web-scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
